
@book{fishman1996MonteCarloConcepts,
	address = {New York},
	series = {Springer {Series} in {Operations} {Research} and {Financial} {Engineering}},
	title = {Monte {Carlo}: {Concepts}, {Algorithms}, and {Applications}},
	isbn = {978-0-387-94527-9},
	shorttitle = {Monte {Carlo}},
	url = {https://www.springer.com/gp/book/9780387945279},
	abstract = {This book provides an introduction to the Monte Carlo method suitable for a one-or two-semester course for graduate and advanced undergraduate students in the mathematical and engineering sciences. It also can serve as a reference for the professional analyst. In the past, my inability to provide students with a single­ source book on this topic for class and for later professional reference had left me repeatedly frustrated, and eventually motivated me to write this book. In addition to focused accounts of major topics, the book has two unifying themes: One concerns the effective use of information and the other concerns error control and reduction. The book describes how to incorporate information about a problem into a sampling plan in a way that reduces the cost of estimating its solution to within a specified error bound. Although exploiting special structures to reduce cost long has been a hallmark of the Monte Carlo method, the propen­ sity of users of the method to discard useful information because it does not fit traditional textbook models repeatedly has impressed me. The present account aims at reducing the impediments to integrating this information. Errors, both statistical and computational, abound in every Monte Carlo sam­ pling experiment, and a considerable methodology exists for controlling them.},
	language = {en},
	urldate = {2021-09-24},
	publisher = {Springer-Verlag},
	author = {Fishman, George},
	year = {1996},
	doi = {10.1007/978-1-4757-2553-7},
	file = {Snapshot:/Users/yulei/Zotero/storage/Q3YAC3BP/9780387945279.html:text/html;全文:/Users/yulei/Zotero/storage/F3PBSQQ2/Fishman - 1996 - Monte Carlo Concepts, Algorithms, and Application.pdf:application/pdf},
}

@article{loh1996LatinHypercubeSampling,
	title = {On {Latin} {Hypercube} {Sampling}},
	volume = {24},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/2242641},
	abstract = {This paper contains a collection of results on Latin hypercube sampling. The first result is a Berry-Esseen-type bound for the multivariate central limit theorem of the sample mean μ̂n based on a Latin hypercube sample. The second establishes sufficient conditions on the convergence rate in the strong law for μ̂n. Finally motivated by the concept of empirical likelihood, a way of constructing nonparametric confidence regions based on Latin hypercube samples is proposed for vector means.},
	number = {5},
	urldate = {2021-09-24},
	journal = {The Annals of Statistics},
	author = {Loh, Wei-Liem},
	year = {1996},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {2058--2080},
}

@book{niederreiter1992RandomNumberGeneration,
	address = {USA},
	title = {Random number generation and quasi-{Monte} {Carlo} methods},
	isbn = {978-0-89871-295-7},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Niederreiter, Harald},
	year = {1992},
}


@article{bellman1966DynamicProgramming,
	title = {Dynamic {Programming}},
	volume = {153},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.153.3731.34},
	doi = {10.1126/science.153.3731.34},
	language = {en},
	number = {3731},
	urldate = {2021-09-23},
	journal = {Science},
	author = {Bellman, R.},
	month = jul,
	year = {1966},
	pages = {34--37},
	file = {全文:/Users/yulei/Zotero/storage/Z285JYNK/Bellman - 1966 - Dynamic Programming.pdf:application/pdf},
}

@article{bou-zeid2004ParametricSensitivityAnalysis,
	title = {Parametric sensitivity analysis of leachate transport simulations at landfills},
	volume = {24},
	issn = {0956-053X},
	url = {https://www.sciencedirect.com/science/article/pii/S0956053X04000571},
	doi = {10.1016/j.wasman.2004.03.004},
	abstract = {This paper presents a case study in simulating leachate generation and transport at a 2000 ton/day landfill facility and assesses leachate migration away from the landfill in order to control associated environmental impacts, particularly on groundwater wells down gradient of the site. The site offers unique characteristics in that it is a former quarry converted to a landfill and is planned to have refuse depths that could reach 100 m, making it one of the deepest in the world. Leachate quantity and potential percolation into the subsurface are estimated using the Hydrologic Evaluation of Landfill Performance (HELP) model. A three-dimensional subsurface model (PORFLOW) was adopted to simulate ground water flow and contaminant transport away from the site. A comprehensive sensitivity analysis to leachate transport control parameters was also conducted. Sensitivity analysis suggests that changes in partition coefficient, source strength, aquifer hydraulic conductivity, and dispersivity have the most significant impact on model output indicating that these parameters should be carefully selected when similar modeling studies are performed.},
	language = {en},
	number = {7},
	urldate = {2021-02-25},
	journal = {Waste Management},
	author = {Bou-Zeid, E and El-Fadel, M},
	month = jan,
	year = {2004},
	pages = {681--689},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/NWSVPGAT/S0956053X04000571.html:text/html;Bou-Zeid_El-Fadel_2004_Parametric sensitivity analysis of leachate transport simulations at landfills.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Bou-Zeid_El-Fadel_2004_Parametric sensitivity analysis of leachate transport simulations at landfills.pdf:application/pdf},
}

@article{leeComplexitiesAgentBasedModeling2015,
	title = {The {Complexities} of {Agent}-{Based} {Modeling} {Output} {Analysis}},
	volume = {18},
	issn = {1460-7425},
	doi = {10.18564/jasss.2897},
	number = {4},
	journal = {Journal of Artificial Societies and Social Simulation},
	author = {Lee, Ju-Sung and Filatova, Tatiana and Ligmann-Zielinska, Arika and Hassani-Mahmooei, Behrooz and Stonedahl, Forrest and Lorscheid, Iris and Voinov, Alexey and Polhill, J. Gary and Sun, Zhanli and Parker, Dawn C.},
	year = {2015},
	pages = {4},
	file = {Lee 等。 - 2015 - The Complexities of Agent-Based Modeling Output An.pdf:/Users/yulei/Zotero/storage/J97B8JI8/Lee 等。 - 2015 - The Complexities of Agent-Based Modeling Output An.pdf:application/pdf;The Complexities of Agent-Based Modeling Output Analysis:/Users/yulei/Zotero/storage/ZBNTKFKL/4.html:text/html},
}

@article{campolongo1997SensitivityAnalysisEnvironmental,
	series = {The {Role} of {Sensitivity} {Analysis} in the {Corroboration} of {Models} and its {Links} to {Model} {Structural} and {Parametric} {Uncertainty}},
	title = {Sensitivity analysis of an environmental model: an application of different analysis methods},
	volume = {57},
	issn = {0951-8320},
	shorttitle = {Sensitivity analysis of an environmental model},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832097000215},
	doi = {10.1016/S0951-8320(97)00021-5},
	abstract = {A parametric sensitivity analysis (SA) was conducted on a well known model for the production of a key sulphur bearing compound from algal biota. The model is of interest because of the climatic relevance of the gas (dimethylsulphide, DMS), an initiator of cloud particles. A screening test at low sample size is applied first (Morris method) followed by a computationally intensive variance based measure. Standardised regression coefficients are also computed. The various SA measures are compared with each other, and the use of bootstrap is suggested to extract empirical confidence bounds on the SA estimators. For some of the input factors, investigators guess about the parameters relevance was confirmed; for some others, the results shed new light on the system mechanism and on the data parametrisation.},
	language = {en},
	number = {1},
	urldate = {2021-02-24},
	journal = {Reliability Engineering \& System Safety},
	author = {Campolongo, Francesca and Saltelli, Andrea},
	month = jul,
	year = {1997},
	pages = {49--69},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/CTIQE7WD/S0951832097000215.html:text/html;Campolongo_Saltelli_1997_Sensitivity analysis of an environmental model.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Campolongo_Saltelli_1997_Sensitivity analysis of an environmental model.pdf:application/pdf},
}

@article{geComprehensiveApproachSensitivity2014,
	title = {Comprehensive {Approach} for the {Sensitivity} {Analysis} of {High}-{Dimensional} and {Computationally} {Expensive} {Traffic} {Simulation} {Models}},
	volume = {2422},
	issn = {0361-1981},
	url = {https://doi.org/10.3141/2422-14},
	doi = {10.3141/2422-14},
	abstract = {The reliability of traffic model results is strictly connected to the quality of its calibration. A challenge arising in this context concerns the selection of the most influential input parameters. A model sensitivity analysis should be used with this aim. However, because of the limitations of time and computational resources, a proper sensitivity analysis is rarely performed in common practice. A recent study introduced a methodology based on Gaussian process metamodels for the sensitivity analysis of computationally expensive traffic simulation models. The main limitation was a dependence on model dimensionality. When the model has more than about 15 to 20 parameters, estimation of a Gaussian process metamodel (also known as a Kriging metamodel) may become problematic. In this paper, the Kriging-based approach is coupled with a recently developed approach, quasi-optimized trajectory-based elementary effects (quasi-OTEE), for the sensitivity analysis of computationally expensive models. The quasi-OTEE sensitivity analysis can be used to identify the whole subset of sensitive parameters of a high-dimensional model, and the Kriging-based sensitivity analysis can then be used to refine the analysis and to rank the different parameters of the subset in a more reliable way. Application of this new sequential sensitivity analysis method is illustrated with the Wiedemann-74 car-following model. Results show that the new method requires 40 times fewer model evaluations than a standard variance-based sensitivity analysis to identify the influential parameters and their ranks.},
	language = {en},
	number = {1},
	urldate = {2021-02-24},
	journal = {Transportation Research Record},
	author = {Ge, Qiao and Ciuffo, Biagio and Menendez, Monica},
	month = jan,
	year = {2014},
	note = {Publisher: SAGE Publications Inc},
	pages = {121--130},
	file = {Ge 等。 - 2014 - Comprehensive Approach for the Sensitivity Analysi.pdf:/Users/yulei/Zotero/storage/NWPBKWE9/Ge 等。 - 2014 - Comprehensive Approach for the Sensitivity Analysi.pdf:application/pdf},
}

@article{ge2014EfficientSensitivityAnalysis,
	title = {An {Efficient} {Sensitivity} {Analysis} {Approach} for {Computationally} {Expensive} {Microscopic} {Traffic} {Simulation} {Models}},
	volume = {2},
	issn = {2287-7940},
	url = {https://trid.trb.org/view/1407674},
	number = {2},
	urldate = {2021-02-24},
	journal = {International Journal of Transportation},
	author = {Ge, Qiao and Menendez, Monica},
	year = {2014},
	file = {Snapshot:/Users/yulei/Zotero/storage/UZCU2E5W/1407674.html:text/html;Ge_Menendez_2014_An Efficient Sensitivity Analysis Approach for Computationally Expensive.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Ge_Menendez_2014_An Efficient Sensitivity Analysis Approach for Computationally Expensive.pdf:application/pdf},
}

@misc{SensitivityAnalysisPractice,
	title = {Sensitivity {Analysis} in {Practice}: {A} {Guide} to {Assessing} {Scientific} {Models} {\textbar} {Wiley}},
	shorttitle = {Sensitivity {Analysis} in {Practice}},
	url = {https://www.wiley.com/en-us/Sensitivity+Analysis+in+Practice%3A+A+Guide+to+Assessing+Scientific+Models-p-9780470870938},
	abstract = {Sensitivity analysis should be considered a pre-requisite for statistical model building in any scientific discipline where modelling takes place. For a non-expert, choosing the method of analysis for their model is complex, and depends on a number of factors. This book guides the non-expert through their problem in order to enable them to choose and apply the most appropriate method. It offers a review of the state-of-the-art in sensitivity analysis, and is suitable for a wide range of practitioners. It is focussed on the use of SIMLAB – a widely distributed freely-available sensitivity analysis software package developed by the authors – for solving problems in sensitivity analysis of statistical models. Other key features: Provides an accessible overview of the current most widely used methods for sensitivity analysis. Opens with a detailed worked example to explain the motivation behind the book. Includes a range of examples to help illustrate the concepts discussed. Focuses on implementation of the methods in the software SIMLAB - a freely-available sensitivity analysis software package developed by the authors. Contains a large number of references to sources for further reading. Authored by the leading authorities on sensitivity analysis.},
	language = {en-us},
	urldate = {2021-02-22},
	journal = {Wiley.com},
	file = {Snapshot:/Users/yulei/Zotero/storage/LEJ35SXN/Sensitivity+Analysis+in+Practice+A+Guide+to+Assessing+Scientific+Models-p-9780470870938.html:text/html;Sensitivity Analysis in Practice.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Sensitivity Analysis in Practice.pdf:application/pdf},
}

@misc{2010GlobalSensitivityAnalysis,
	title = {Global {Sensitivity} {Analysis}: {The} {Primer}},
	shorttitle = {Global {Sensitivity} {Analysis}},
	url = {https://www.wiley.com/en-us/Global+Sensitivity+Analysis%3A+The+Primer-p-9780470059975},
	abstract = {Complex mathematical and computational models are used in all areas of society and technology and yet model based science is increasingly contested or refuted, especially when models are applied to controversial themes in domains such as health, the environment or the economy. More stringent standards of proofs are demanded from model-based numbers, especially when these numbers represent potential financial losses, threats to human health or the state of the environment. Quantitative sensitivity analysis is generally agreed to be one such standard. Mathematical models are good at mapping assumptions into inferences. A modeller makes assumptions about laws pertaining to the system, about its status and a plethora of other, often arcane, system variables and internal model settings. To what extent can we rely on the model-based inference when most of these assumptions are fraught with uncertainties? Global Sensitivity Analysis offers an accessible treatment of such problems via quantitative sensitivity analysis, beginning with the first principles and guiding the reader through the full range of recommended practices with a rich set of solved exercises. The text explains the motivation for sensitivity analysis, reviews the required statistical concepts, and provides a guide to potential applications. The book: Provides a self-contained treatment of the subject, allowing readers to learn and practice global sensitivity analysis without further materials. Presents ways to frame the analysis, interpret its results, and avoid potential pitfalls. Features numerous exercises and solved problems to help illustrate the applications. Is authored by leading sensitivity analysis practitioners, combining a range of disciplinary backgrounds. Postgraduate students and practitioners in a wide range of subjects, including statistics, mathematics, engineering, physics, chemistry, environmental sciences, biology, toxicology, actuarial sciences, and econometrics will find much of use here. This book will prove equally valuable to engineers working on risk analysis and to financial analysts concerned with pricing and hedging.},
	language = {en-us},
	urldate = {2021-02-15},
	journal = {Wiley.com},
	year = {2010},
	file = {Global Sensitivity Analysis The Primer  Wiley.pdf:/Users/yulei/Zotero/storage/AXXP6CEI/Global Sensitivity Analysis The Primer  Wiley.pdf:application/pdf;Snapshot:/Users/yulei/Zotero/storage/U9Q39CX6/Global+Sensitivity+Analysis+The+Primer-p-9780470059975.html:text/html},
}

@article{liebermanGoalOrientedInferenceApproach2013,
	title = {Goal-{Oriented} {Inference}: {Approach}, {Linear} {Theory}, and {Application} to {Advection} {Diffusion}},
	shorttitle = {Goal-{Oriented} {Inference}},
	doi = {10.1137/130913110},
	abstract = {Inference of model parameters is one step in an engineering process often ending in predictions that support decision in the form of design or control. Incorporation of end goals into the inference process leads to more efficient goal-oriented algorithms that automatically target the most relevant parameters for prediction. In the linear setting the control-theoretic concepts underlying balanced truncation model reduction can be exploited in inference through a dimensionally optimal subspace regularizer. The inference-for-prediction method exactly replicates the prediction results of either truncated singular value decomposition, Tikhonov-regularized, or Gaussian statistical inverse problem formulations independent of data; it sacrifices accuracy in parameter estimate for online efficiency. The new method leads to low-dimensional parameterization of the inverse problem enabling solution on smartphones or laptops in the field.},
	journal = {SIAM Rev.},
	author = {Lieberman, Chad and Willcox, K.},
	year = {2013},
	file = {Full Text PDF:/Users/yulei/Zotero/storage/WV9WHDA8/Lieberman 和 Willcox - 2013 - Goal-Oriented Inference Approach, Linear Theory, .pdf:application/pdf},
}

@article{liebermanNonlinearGoalOrientedBayesian2014,
	title = {Nonlinear {Goal}-{Oriented} {Bayesian} {Inference}: {Application} to {Carbon} {Capture} and {Storage}},
	volume = {36},
	issn = {1064-8275},
	shorttitle = {Nonlinear {Goal}-{Oriented} {Bayesian} {Inference}},
	url = {https://epubs.siam.org/doi/10.1137/130928315},
	doi = {10.1137/130928315},
	abstract = {In many engineering problems, unknown parameters of a model are inferred in order to make predictions, to design controllers, or to  optimize the model.  When parameters are distributed (continuous) or very high-dimensional (discrete) and quantities of interest are low-dimensional, parameters need not be fully resolved to make accurate estimates of quantities of interest. In this work, we extend goal-oriented inference---the process of estimating predictions from observed data without resolving the parameter, previously justified theoretically in the linear setting---to Bayesian statistical inference problem formulations with nonlinear experimental and prediction processes. We propose to learn the joint density of data and predictions offline using Gaussian mixture models. When data are observed online, we condition the representation to arrive at a probabilistic description of predictions given observed data.  Our approach enables real-time estimation of uncertainty in quantities of interest and renders tractable high-dimensional PDE-constrained Bayesian inference when there exist low-dimensional output quantities of interest.  We demonstrate the method on a realistic problem in carbon capture and storage for which existing methods of Bayesian parameter estimation are intractable.},
	number = {3},
	urldate = {2021-02-10},
	journal = {SIAM Journal on Scientific Computing},
	author = {Lieberman, Chad and Willcox, Karen},
	month = jan,
	year = {2014},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {B427--B449},
	file = {全文:/Users/yulei/Zotero/storage/PZ7LT2YY/Lieberman 和 Willcox - 2014 - Nonlinear Goal-Oriented Bayesian Inference Applic.pdf:application/pdf;Snapshot:/Users/yulei/Zotero/storage/22SH2PTM/130928315.html:text/html},
}

@article{borgonovo2016CommonRationaleGlobal,
	title = {A {Common} {Rationale} for {Global} {Sensitivity} {Measures} and {Their} {Estimation}},
	volume = {36},
	issn = {1539-6924},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/risa.12555},
	doi = {10.1111/risa.12555},
	abstract = {Measures of sensitivity and uncertainty have become an integral part of risk analysis. Many such measures have a conditional probabilistic structure, for which a straightforward Monte Carlo estimation procedure has a double-loop form. Recently, a more efficient single-loop procedure has been introduced, and consistency of this procedure has been demonstrated separately for particular measures, such as those based on variance, density, and information value. In this work, we give a unified proof of single-loop consistency that applies to any measure satisfying a common rationale. This proof is not only more general but invokes less restrictive assumptions than heretofore in the literature, allowing for the presence of correlations among model inputs and of categorical variables. We examine numerical convergence of such an estimator under a variety of sensitivity measures. We also examine its application to a published medical case study.},
	language = {en},
	number = {10},
	urldate = {2021-02-10},
	journal = {Risk Analysis},
	author = {Borgonovo, Emanuele and Hazen, Gordon B. and Plischke, Elmar},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/risa.12555},
	keywords = {Global sensitivity measures, Monte Carlo simulation, probabilistic sensitivity analysis, risk analysis, uncertainty analysis},
	pages = {1871--1895},
	file = {Borgonovo 等。 - 2016 - A Common Rationale for Global Sensitivity Measures.pdf:/Users/yulei/Zotero/storage/6RBVWID9/Borgonovo 等。 - 2016 - A Common Rationale for Global Sensitivity Measures.pdf:application/pdf;Snapshot:/Users/yulei/Zotero/storage/C8QA5N32/risa.html:text/html;Borgonovo et al_2016_A Common Rationale for Global Sensitivity Measures and Their Estimation.txt:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Borgonovo et al_2016_A Common Rationale for Global Sensitivity Measures and Their Estimation.txt:text/plain},
}

@article{razavi2021FutureSensitivityAnalysis,
	title = {The {Future} of {Sensitivity} {Analysis}: {An} essential discipline for systems modeling and policy support},
	volume = {137},
	issn = {1364-8152},
	shorttitle = {The {Future} of {Sensitivity} {Analysis}},
	url = {http://www.sciencedirect.com/science/article/pii/S1364815220310112},
	doi = {10.1016/j.envsoft.2020.104954},
	abstract = {Sensitivity analysis (SA) is en route to becoming an integral part of mathematical modeling. The tremendous potential benefits of SA are, however, yet to be fully realized, both for advancing mechanistic and data-driven modeling of human and natural systems, and in support of decision making. In this perspective paper, a multidisciplinary group of researchers and practitioners revisit the current status of SA, and outline research challenges in regard to both theoretical frameworks and their applications to solve real-world problems. Six areas are discussed that warrant further attention, including (1) structuring and standardizing SA as a discipline, (2) realizing the untapped potential of SA for systems modeling, (3) addressing the computational burden of SA, (4) progressing SA in the context of machine learning, (5) clarifying the relationship and role of SA to uncertainty quantification, and (6) evolving the use of SA in support of decision making. An outlook for the future of SA is provided that underlines how SA must underpin a wide variety of activities to better serve science and society.},
	language = {en},
	urldate = {2021-02-03},
	journal = {Environmental Modelling \& Software},
	author = {Razavi, Saman and Jakeman, Anthony and Saltelli, Andrea and Prieur, Clémentine and Iooss, Bertrand and Borgonovo, Emanuele and Plischke, Elmar and Lo Piano, Samuele and Iwanaga, Takuya and Becker, William and Tarantola, Stefano and Guillaume, Joseph H. A. and Jakeman, John and Gupta, Hoshin and Melillo, Nicola and Rabitti, Giovanni and Chabridon, Vincent and Duan, Qingyun and Sun, Xifu and Smith, Stefán and Sheikholeslami, Razi and Hosseini, Nasim and Asadzadeh, Masoud and Puy, Arnald and Kucherenko, Sergei and Maier, Holger R.},
	month = mar,
	year = {2021},
	keywords = {Decision making, Machine learning, Mathematical modeling, Model robustness, Model validation and verification, Policy support, Sensitivity analysis, Uncertainty quantification},
	pages = {104954},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/CJ34XNZ3/S1364815220310112.html:text/html;Razavi et al_2021_The Future of Sensitivity Analysis.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Razavi et al_2021_The Future of Sensitivity Analysis.pdf:application/pdf},
}

@article{krinsky1986ApproximatingStatisticalProperties,
	title = {On {Approximating} the {Statistical} {Properties} of {Elasticities}},
	volume = {68},
	issn = {0034-6535},
	url = {https://www.jstor.org/stable/1924536},
	doi = {10.2307/1924536},
	abstract = {This paper argues for the use of a simulation methodology to examine the distributions of elasticities that, in turn, are complex, non-linear functions of estimated parameters. Linear approximations are often used in this context but are shown to be poor substitutes for the suggested procedure.},
	number = {4},
	urldate = {2021-01-15},
	journal = {The Review of Economics and Statistics},
	author = {Krinsky, Itzhak and Robb, A. Leslie},
	year = {1986},
	note = {Publisher: The MIT Press},
	pages = {715--719},
	file = {Krinsky_Robb_1986_On Approximating the Statistical Properties of Elasticities.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Krinsky_Robb_1986_On Approximating the Statistical Properties of Elasticities.pdf:application/pdf},
}

@article{keane1997CareerDecisionsYoung,
	title = {The {Career} {Decisions} of {Young} {Men}},
	volume = {105},
	issn = {0022-3808},
	url = {https://www.jstor.org/stable/10.1086/262080},
	doi = {10.1086/262080},
	abstract = {This paper provides structural estimates of a dynamic model of schooling, work, and occupational choice decisions based on 11 years of observations on a sample of young men from the 1979 youth cohort of the National Longitudinal Surveys of Labor Market Experience (NLSY). The structural estimation framework that we adopt fully imposes the restrictions of the theory and permits an investigation of whether such a theoretically restricted model can succeed in quantitatively fitting the observed data patterns. We find that a suitably extended human capital investment model can in fact do an excellent job of fitting observed data on school attendance, work, occupational choices, and wages in the NLSY data on young men and also produces reasonable forecasts of future work decisions and wage patterns.},
	number = {3},
	urldate = {2020-12-17},
	journal = {Journal of Political Economy},
	author = {Keane, Michael P. and Wolpin, Kenneth I.},
	year = {1997},
	note = {Publisher: The University of Chicago Press},
	pages = {473--522},
	file = {Keane_Wolpin_1997_The Career Decisions of Young Men.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Keane_Wolpin_1997_The Career Decisions of Young Men.pdf:application/pdf},
}

@article{keane1994SolutionEstimationDiscrete,
	title = {The {Solution} and {Estimation} of {Discrete} {Choice} {Dynamic} {Programming} {Models} by {Simulation} and {Interpolation}: {Monte} {Carlo} {Evidence}},
	volume = {76},
	issn = {0034-6535},
	shorttitle = {The {Solution} and {Estimation} of {Discrete} {Choice} {Dynamic} {Programming} {Models} by {Simulation} and {Interpolation}},
	url = {https://www.jstor.org/stable/2109768},
	doi = {10.2307/2109768},
	abstract = {Over the past decade, a substantial literature on methods for the estimation of discrete choice dynamic programming (DDP) models of behavior has developed. However, the implementation of these methods can impose major computational burdens because solving for agents' decision rules often involves high dimensional integrations that must be performed at each point in the state space. In this paper we develop an approximate solution method that consists of: 1) using Monte Carlo integration to stimulate the required multiple integrals at a subset of the state points, and 2) interpolating the non-simulated values using a regression function. The overall performance of this approximation method appears to be excellent.},
	number = {4},
	urldate = {2020-12-04},
	journal = {The Review of Economics and Statistics},
	author = {Keane, Michael P. and Wolpin, Kenneth I.},
	year = {1994},
	note = {Publisher: The MIT Press},
	pages = {648--672},
	file = {Keane_Wolpin_1994_The Solution and Estimation of Discrete Choice Dynamic Programming Models by.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Keane_Wolpin_1994_The Solution and Estimation of Discrete Choice Dynamic Programming Models by.pdf:application/pdf},
}

@article{kala2019QuantileorientedGlobalSensitivity,
	title = {Quantile-oriented global sensitivity analysis of design resistance},
	volume = {25},
	copyright = {Copyright (c) 2019 The Author(s). Published by VGTU Press.},
	issn = {1822-3605},
	url = {https://journals.vgtu.lt/index.php/JCEM/article/view/9627},
	doi = {10.3846/jcem.2019.9627},
	abstract = {The article investigates the application of a new type of global quantile-oriented sensitivity analysis (called QSA in the article) and contrasts it with established Sobol’ sensitivity analysis (SSA). Comparison of QSA of the resistance design value (0.1 percentile) with SSA is performed on an example of the analysis of the resistance of a steel IPN 200 beam, which is subjected to lateral-torsional buckling. The resistance is approximated using higher order polynomial metamodels created from advanced non-linear FE models. The main, higher order and total effects are calculated using the Latin Hypercube Sampling method. Noticeable differences between the two methods are found, with QSA apparently revealing higher sensitivity of the resistance design value to random input second and higher order interactions (compared to SSA). SSA cannot identify certain reliability aspects of structural design as comprehensively as QSA, particularly in relation to higher order interactions effects of input imperfections. In order to better understand the reasons for the differences between QSA and SSA, two simple examples are presented, where QSA (median) and SSA show a general agreement in the calculation of certain sensitivity indices.
													Keyword :
																											sensitivity analysis,
																			quantile,
																			resistance,
																			lateral-torsional buckling,
																			imperfections,
																			steel,
																			random sampling



								How to Cite



  Kala, Z. (2019). Quantile-oriented global sensitivity analysis of design resistance. Journal of Civil Engineering and Management, 25(4), 297-305. https://doi.org/10.3846/jcem.2019.9627




											More Citation Formats





														ACM




														ACS




														APA




														ABNT




														Chicago




														Harvard




														IEEE




														MLA




														Turabian




														Vancouver},
	language = {en},
	number = {4},
	urldate = {2020-09-10},
	journal = {Journal of Civil Engineering and Management},
	author = {Kala, Zdeněk},
	month = apr,
	year = {2019},
	note = {Number: 4},
	keywords = {imperfections, lateral-torsional buckling, quantile, random sampling, resistance, sensitivity analysis, steel},
	pages = {297--305},
	file = {Snapshot:/Users/yulei/Zotero/storage/RSDL6K8G/9627.html:text/html;Kala_2019_Quantile-oriented global sensitivity analysis of design resistance.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kala_2019_Quantile-oriented global sensitivity analysis of design resistance.pdf:application/pdf},
}

@article{kucherenko2017DifferentNumericalEstimators,
	title = {Different numerical estimators for main effect global sensitivity indices},
	volume = {165},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832016301065},
	doi = {10.1016/j.ress.2017.04.003},
	abstract = {The variance-based method of global sensitivity indices based on Sobol' sensitivity indices became very popular among practitioners due to its easiness of interpretation. For complex practical problems computation of Sobol' indices generally requires a large number of function evaluations to achieve reasonable convergence. Four different direct formulas for computing Sobol’ main effect sensitivity indices are compared on a set of test models for which there are analytical results. Considered test functions represent various types of models that are found in practice. Formulas are based on high-dimensional integrals which are evaluated using Monte Carlo (MC) and Quasi Monte Carlo (QMC) techniques. Direct formulas are also compared with a different approach based on the so-called “double loop reordering” formula. It is found that the “double loop reordering” (DLR) approach shows a superior performance among all methods both for models with independent and dependent variables.},
	language = {en},
	urldate = {2020-08-30},
	journal = {Reliability Engineering \& System Safety},
	author = {Kucherenko, S. and Song, S.},
	month = sep,
	year = {2017},
	keywords = {Double loop reordering, Global sensitivity analysis, Quasi Monte Carlo, Sobol’ sensitivity indices},
	pages = {222--238},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/P2N985EY/S0951832016301065.html:text/html;Kucherenko_Song_2017_Different numerical estimators for main effect global sensitivity indices.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kucherenko_Song_2017_Different numerical estimators for main effect global sensitivity indices.pdf:application/pdf;Kucherenko_Song_2017_Different numerical estimators for main effect global sensitivity indices.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kucherenko_Song_2017_Different numerical estimators for main effect global sensitivity indices2.pdf:application/pdf},
}

@article{kucherenko2012EstimationGlobalSensitivity,
	title = {Estimation of global sensitivity indices for models with dependent variables},
	volume = {183},
	issn = {0010-4655},
	url = {http://www.sciencedirect.com/science/article/pii/S0010465511004085},
	doi = {10.1016/j.cpc.2011.12.020},
	abstract = {A novel approach for estimation variance-based sensitivity indices for models with dependent variables is presented. Both the first order and total sensitivity indices are derived as generalizations of Sobolʼ sensitivity indices. Formulas and Monte Carlo numerical estimates similar to Sobolʼ formulas are derived. A copula-based approach is proposed for sampling from arbitrary multivariate probability distributions. A good agreement between analytical and numerical values of the first order and total indices for considered test cases is obtained. The behavior of sensitivity indices depends on the relative predominance of interactions and correlations. The method is shown to be efficient and general.},
	language = {en},
	number = {4},
	urldate = {2020-08-20},
	journal = {Computer Physics Communications},
	author = {Kucherenko, S. and Tarantola, S. and Annoni, P.},
	month = apr,
	year = {2012},
	keywords = {Global sensitivity analysis, Correlated inputs, Gaussian copula, Quasi Monte Carlo methods, Sobolʼ sensitivity indices, Sobolʼ sequences},
	pages = {937--946},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/HCNZVZ8G/S0010465511004085.html:text/html;Kucherenko et al_2012_Estimation of global sensitivity indices for models with dependent variables.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kucherenko et al_2012_Estimation of global sensitivity indices for models with dependent variables.pdf:application/pdf},
}

@article{borgonovo2016SensitivityAnalysisReview,
	title = {Sensitivity analysis: {A} review of recent advances},
	volume = {248},
	issn = {0377-2217},
	shorttitle = {Sensitivity analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0377221715005469},
	doi = {10.1016/j.ejor.2015.06.032},
	abstract = {The solution of several operations research problems requires the creation of a quantitative model. Sensitivity analysis is a crucial step in the model building and result communication process. Through sensitivity analysis we gain essential insights on model behavior, on its structure and on its response to changes in the model inputs. Several interrogations are possible and several sensitivity analysis methods have been developed, giving rise to a vast and growing literature. We present an overview of available methods, structuring them into local and global methods. For local methods, we discuss Tornado diagrams, one way sensitivity functions, differentiation-based methods and scenario decomposition through finite change sensitivity indices, providing a unified view of the associated sensitivity measures. We then analyze global sensitivity methods, first discussing screening methods such as sequential bifurcation and the Morris method. We then address variance-based, moment-independent and value of information-based sensitivity methods. We discuss their formalization in a common rationale and present recent results that permit the estimation of global sensitivity measures by post-processing the sample generated by a traditional Monte Carlo simulation. We then investigate in detail the methodological issues concerning the crucial step of correctly interpreting the results of a sensitivity analysis. A classical example is worked out to illustrate some of the approaches.},
	language = {en},
	number = {3},
	urldate = {2020-08-17},
	journal = {European Journal of Operational Research},
	author = {Borgonovo, Emanuele and Plischke, Elmar},
	month = feb,
	year = {2016},
	keywords = {Sensitivity analysis, Computer experiments, Simulation},
	pages = {869--887},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/AD9XFENC/S0377221715005469.html:text/html;Borgonovo_Plischke_2016_Sensitivity analysis.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Borgonovo_Plischke_2016_Sensitivity analysis.pdf:application/pdf},
}

@article{luyi2012MomentindependentImportanceMeasure,
	title = {Moment-independent importance measure of basic variable and its state dependent parameter solution},
	volume = {38},
	issn = {0167-4730},
	url = {http://www.sciencedirect.com/science/article/pii/S0167473012000276},
	doi = {10.1016/j.strusafe.2012.04.001},
	abstract = {To analyze the effect of basic variable on output of the structure or system in reliability engineering, two moment-independent importance measures of the basic variable are proposed respectively on the failure probability and distribution function of the output. The importance measures proposed not only inherit the advantages of the traditional moment-independent importance measures, but also reflect the intrinsic relationship of the moment-independent measures and the corresponding variance-based importance measures. For the problem that the computational effort of the moment-independent importance measure is usually too high, the computation of the proposed moment-independent importance measures is transformed into that of the variance-based importance measures on their intrinsic relationship. And then combining the high efficient state dependent parameter (SDP) method for the calculation of the conditional moments of the model output, a SDP solution is established to solve two moment-independent importance measures. Several examples are used to demonstrate that the proposed importance measures can effectively describe the effect of the basic variable on the reliability of the structure system, and the established solution can obtain the two importance measures simultaneously with only a single set of model runs, which allows for a strong reduction of the computational cost.},
	language = {en},
	urldate = {2020-07-21},
	journal = {Structural Safety},
	author = {Luyi, Li and Zhenzhou, Lu and Jun, Feng and Bintuan, Wang},
	month = sep,
	year = {2012},
	keywords = {Basic variable, Importance measure, Moment-independent, State dependent parameter (SDP)},
	pages = {40--47},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/RI7D5IJ7/S0167473012000276.html:text/html;Luyi et al_2012_Moment-independent importance measure of basic variable and its state dependent.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Luyi et al_2012_Moment-independent importance measure of basic variable and its state dependent.pdf:application/pdf},
}

@article{yun2018EfficientGlobalReliability,
	title = {An efficient global reliability sensitivity analysis algorithm based on classification of model output and subset simulation},
	volume = {74},
	issn = {0167-4730},
	url = {http://www.sciencedirect.com/science/article/pii/S016747301730437X},
	doi = {10.1016/j.strusafe.2018.04.003},
	abstract = {The global reliability sensitivity analysis measures the effect of each model input variable on the failure probability, which is very useful for reliability-based optimization design. The aim of this paper is to propose an alternative method to estimate the global reliability sensitivity indices by one group of model input–output samples. Firstly, Bayes formula is used to convert the original expression of global reliability sensitivity index into an equivalent form where only the unconditional failure probability and the failure-conditional probability density function (PDF) of each model input variable are required. All global reliability sensitivity indices can be simultaneously estimated by this new equivalent form, and the computational cost of the process is independent of the dimensionality of model input variables. Secondly, to improve the efficiency of sampling which aims at calculating the unconditional failure probability and estimating the failure-conditional PDF of every model input simultaneously, subset simulation method is extended to achieve these two aims. In the proposed procedure, subset simulation is used to estimate the unconditional failure probability, and Metropolis-Hastings algorithm is employed to convert the samples in failure domain from the current PDF in subset simulation to the PDF corresponding to the original PDF of model inputs for estimating the failure-conditional PDF of each model input variable. Thirdly, Edgeworth expansion is employed to approximate the failure-conditional PDF of each model input variable. Finally, the global reliability sensitivity index can be easily computed as byproducts using the unconditional failure probability and the failure-conditional PDF of each model input in failure probability analysis, and this process does not need any extra model evaluations after the unconditional failure probability analysis is completed by subset simulation. A headless rivet model, a roof truss structure and a composite cantilever beam structure are analyzed, and the results demonstrate the effectiveness of the proposed method in global reliability sensitivity analysis.},
	language = {en},
	urldate = {2020-07-21},
	journal = {Structural Safety},
	author = {Yun, Wanying and Lu, Zhenzhou and Zhang, Yu and Jiang, Xian},
	month = sep,
	year = {2018},
	keywords = {Bayes formula, Dimensional-independency, Edgeworth expansion, First four-order conditional moments, Global reliability sensitivity analysis, Metropolis-Hastings algorithm},
	pages = {49--57},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/QVA5V87B/S016747301730437X.html:text/html;Yun et al_2018_An efficient global reliability sensitivity analysis algorithm based on.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Yun et al_2018_An efficient global reliability sensitivity analysis algorithm based on.pdf:application/pdf},
}

@article{borgonovo2007NewUncertaintyImportance,
	title = {A new uncertainty importance measure},
	volume = {92},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832006000883},
	doi = {10.1016/j.ress.2006.04.015},
	abstract = {Uncertainty in parameters is present in many risk assessment problems and leads to uncertainty in model predictions. In this work, we introduce a global sensitivity indicator which looks at the influence of input uncertainty on the entire output distribution without reference to a specific moment of the output (moment independence) and which can be defined also in the presence of correlations among the parameters. We discuss its mathematical properties and highlight the differences between the present indicator, variance-based uncertainty importance measures and a moment independent sensitivity indicator previously introduced in the literature. Numerical results are discussed with application to the probabilistic risk assessment model on which Iman [A matrix-based approach to uncertainty and sensitivity analysis for fault trees. Risk Anal 1987;7(1):22–33] first introduced uncertainty importance measures.},
	language = {en},
	number = {6},
	urldate = {2020-07-21},
	journal = {Reliability Engineering \& System Safety},
	author = {Borgonovo, E.},
	month = jun,
	year = {2007},
	keywords = {Global sensitivity analysis, Importance measures, Probabilistic risk assessment, Uncertainty analysis, Uncertainty importance measures},
	pages = {771--784},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/B5LQ9LRA/S0951832006000883.html:text/html;Borgonovo_2007_A new uncertainty importance measure.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Borgonovo_2007_A new uncertainty importance measure.pdf:application/pdf},
}

@article{chun2000UncertaintyImportanceMeasure,
	title = {An uncertainty importance measure using a distance metric for the change in a cumulative distribution function},
	volume = {70},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832000000685},
	doi = {10.1016/S0951-8320(00)00068-5},
	abstract = {A simple measure of uncertainty importance using the entire change of cumulative distribution functions (CDFs) has been developed for use in probability safety assessments (PSAs). The entire change of CDFs is quantified in terms of the metric distance between two CDFs. The metric distance measure developed in this study reflects the relative impact of distributional changes of inputs on the change of an output distribution, while most of the existing uncertainty importance measures reflect the magnitude of relative contribution of input uncertainties to the output uncertainty. The present measure has been evaluated analytically for various analytical distributions to examine its characteristics. To illustrate the applicability and strength of the present measure, two examples are provided. The first example is an application of the present measure to a typical problem of a system fault tree analysis and the second one is for a hypothetical non-linear model. Comparisons of the present result with those obtained by existing uncertainty importance measures show that the metric distance measure is a useful tool to express the measure of uncertainty importance in terms of the relative impact of distributional changes of inputs on the change of an output distribution.},
	language = {en},
	number = {3},
	urldate = {2020-07-21},
	journal = {Reliability Engineering \& System Safety},
	author = {Chun, Moon-Hyun and Han, Seok-Jung and Tak, Nam-IL},
	month = dec,
	year = {2000},
	keywords = {Importance measure, Uncertainty analysis, Metric distance, Uncertainty importance},
	pages = {313--321},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/FVHUVGSD/S0951832000000685.html:text/html;Chun et al_2000_An uncertainty importance measure using a distance metric for the change in a.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Chun et al_2000_An uncertainty importance measure using a distance metric for the change in a.pdf:application/pdf},
}

@article{hommaImportanceMeasuresGlobal1996,
	title = {Importance measures in global sensitivity analysis of nonlinear models},
	volume = {52},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/0951832096000026},
	doi = {10.1016/0951-8320(96)00002-6},
	abstract = {The present paper deals with a new method of global sensitivity analysis of nonlinear models. This is based on a measure of importance to calculate the fractional contribution of the input parameters to the variance of the model prediction. Measures of importance in sensitivity analysis have been suggested by several authors, whose work is reviewed in this article. More emphasis is given to the developments of sensitivity indices by the Russian mathematician I.M. Sobol'. Given that Sobol' treatment of the measure of importance is the most general, his formalism is employed throughout this paper where conceptual and computational improvements of the method are presented. The computational novelty of this study is the introduction of the ‘total effect’ parameter index. This index provides a measure of the total effect of a given parameter, including all the possible synergetic terms between that parameter and all the others. Rank transformation of the data is also introduced in order to increase the reproducibility of the method. These methods are tested on a few analytical and computer models. The main conclusion of this work is the identification of a sensitivity analysis methodology which is both flexible, accurate and informative, and which can be achieved at reasonable computational cost.},
	language = {en},
	number = {1},
	urldate = {2020-07-18},
	journal = {Reliability Engineering \& System Safety},
	author = {Homma, Toshimitsu and Saltelli, Andrea},
	month = apr,
	year = {1996},
	pages = {1--17},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/AASUMP77/0951832096000026.html:text/html},
}

@article{kucherenko2019QuantileBasedGlobal,
	title = {Quantile based global sensitivity measures},
	volume = {185},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832016304574},
	doi = {10.1016/j.ress.2018.12.001},
	abstract = {New global sensitivity measures based on quantiles of the output are introduced. Such measures can be used for global sensitivity analysis of problems in which αth quantiles are explicitly the functions of interest and for identification of variables which are the most important in achieving extreme values of the model output. It is proven that there is a link between introduced measures and Sobol’ main effect sensitivity indices. Two different Monte Carlo estimators are considered. It is shown that the double loop reordering approach is much more efficient than the brute force estimator. Several test cases and practical case studies related to structural safety are used to illustrate the developed method. Results of numerical calculations show the efficiency of the presented technique.},
	language = {en},
	urldate = {2020-07-17},
	journal = {Reliability Engineering \& System Safety},
	author = {Kucherenko, Sergei and Song, Shufang and Wang, Lu},
	month = may,
	year = {2019},
	keywords = {Global sensitivity analysis, Sobol’ sensitivity indices, Quantile based global sensitivity measure, Structural safety},
	pages = {35--48},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/VFLQI2N5/S0951832016304574.html:text/html;Kucherenko et al_2019_Quantile based global sensitivity measures.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kucherenko et al_2019_Quantile based global sensitivity measures.pdf:application/pdf},
}

@article{eisenhauer2015EstimationDynamicDiscrete,
	title = {Estimation of {Dynamic} {Discrete} {Choice} {Models} by {Maximum} {Likelihood} and the {Simulated} {Method} of {Moments}},
	volume = {56},
	copyright = {© (2015) by the Economics Department of the University of Pennsylvania and the Osaka University Institute of Social and Economic Research Association},
	issn = {1468-2354},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/iere.12107},
	doi = {https://doi.org/10.1111/iere.12107},
	abstract = {We compare the performance of maximum likelihood (ML) and simulated method of moments (SMM) estimators for dynamic discrete choice models. We construct and estimate a simplified dynamic structural model of education that captures some basic features of educational choices in the United States in the 1980s and early 1990s. We use estimates from our model to simulate a synthetic data set and assess the ability of ML and SMM to recover the model parameters on this sample. We investigate the performance of alternative tuning parameters for SMM.},
	language = {en},
	number = {2},
	urldate = {2021-03-14},
	journal = {International Economic Review},
	author = {Eisenhauer, Philipp and Heckman, James J. and Mosso, Stefano},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/iere.12107},
	pages = {331--357},
	file = {Snapshot:/Users/yulei/Zotero/storage/ND2LCDZD/iere.html:text/html;Eisenhauer et al_2015_Estimation of Dynamic Discrete Choice Models by Maximum Likelihood and the.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Eisenhauer et al_2015_Estimation of Dynamic Discrete Choice Models by Maximum Likelihood and the.pdf:application/pdf},
}

@article{mcfadden1989MethodSimulatedMoments,
	title = {A {Method} of {Simulated} {Moments} for {Estimation} of {Discrete} {Response} {Models} {Without} {Numerical} {Integration}},
	volume = {57},
	issn = {00129682},
	url = {https://www.jstor.org/stable/1913621?origin=crossref},
	doi = {10.2307/1913621},
	abstract = {This paper proposes a simple modification of a conventional method of moments estimator for a discrete response model, replacing response probabilities that require numerical integration with estimators obtained by Monte Carlo simulation. This methodof simulated moments (MSM) does not require precise estimates of these probabilities for consistency and asymptotic normality, relying instead on the law of large numbers operating across observations to control simulation error, and hence can use simulations of practical size. The method is useful for models such as high-dimensional multinomial probit (MNP), where computation has restricted applications.},
	language = {en},
	number = {5},
	urldate = {2021-04-06},
	journal = {Econometrica},
	author = {McFadden, Daniel},
	month = sep,
	year = {1989},
	pages = {995},
	file = {McFadden_1989_A Method of Simulated Moments for Estimation of Discrete Response Models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/McFadden_1989_A Method of Simulated Moments for Estimation of Discrete Response Models.pdf:application/pdf},
}

@article{kala2020QuantilebasedSobolSensitivity,
	title = {Quantile-based versus {Sobol} sensitivity analysis in limit state design},
	volume = {28},
	issn = {2352-0124},
	url = {https://www.sciencedirect.com/science/article/pii/S2352012420305993},
	doi = {10.1016/j.istruc.2020.10.037},
	abstract = {In limit state design, the reliability of building constructions is generally verified using design quantiles. The design resistance of a structure is explicitly expressed as a low quantile of the cumulative distribution function of resistance. The aim of this article is to show the connections and differences between quantile-oriented sensitivity analysis subordinated to a contrast and classic Sobol sensitivity analysis. Changing the fixed input variable causes synchronous change in the quantile and mean value, but how do the results of these two sensitivity analyses differ? The question is whether or not the changes around the design quantile (measured by contrast indices) are similar to the changes around the mean value, which are measured using Sobol’s indices. Comparison is performed on a case study, where the resistance of the structure is expressed by a non-linear function, the inputs of which are random material and geometric characteristics of the structure. The non-dimensional slenderness is a deterministic parameter, which changes the influence of input variables on the resistance as the model output. It was concluded upon comparing the results of both sensitivity analyses that the rank of the most important variables is the same for both low and high slenderness and is similar for intermediate slenderness. However, the interaction effects are very different. The identification of insignificant variables is the same. Other significant similarities and differences between both types of sensitivity analyses are presented in the article.},
	language = {en},
	urldate = {2021-04-07},
	journal = {Structures},
	author = {Kala, Zdeněk},
	month = dec,
	year = {2020},
	keywords = {Sensitivity analysis, Buckling, Design quantiles, Eurocodes, Stability, Steel, Structural reliability},
	pages = {2424--2430},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/JIRQ7HWG/S2352012420305993.html:text/html;Kala_2020_Quantile-based versus Sobol sensitivity analysis in limit state design.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kala_2020_Quantile-based versus Sobol sensitivity analysis in limit state design.pdf:application/pdf},
}

@article{kala2021GlobalSensitivityAnalysis,
	title = {Global {Sensitivity} {Analysis} of {Quantiles}: {New} {Importance} {Measure} {Based} on {Superquantiles} and {Subquantiles}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Global {Sensitivity} {Analysis} of {Quantiles}},
	url = {https://www.mdpi.com/2073-8994/13/2/263},
	doi = {10.3390/sym13020263},
	abstract = {The article introduces quantile deviation l as a new sensitivity measure based on the difference between superquantile and subquantile. New global sensitivity indices based on the square of l are presented. The proposed sensitivity indices are compared with quantile-oriented sensitivity indices subordinated to contrasts and classical Sobol sensitivity indices. The comparison is performed in a case study using a non-linear mathematical function, the output of which represents the elastic resistance of a slender steel member under compression. The steel member has random imperfections that reduce its load-carrying capacity. The member length is a deterministic parameter that significantly changes the sensitivity of the output resistance to the random effects of input imperfections. The comparison of the results of three types of global sensitivity analyses shows the rationality of the new quantile-oriented sensitivity indices, which have good properties similar to classical Sobol indices. Sensitivity indices subordinated to contrasts are the least comprehensible because they exhibit the strongest interaction effects between inputs. However, using total indices, all three types of sensitivity analyses lead to approximately the same conclusions. The similarity of the results of two quantile-oriented and Sobol sensitivity analysis confirms that Sobol sensitivity analysis is empathetic to the structural reliability and that the variance is one of the important characteristics significantly influencing the low quantile of resistance.},
	language = {en},
	number = {2},
	urldate = {2021-04-07},
	journal = {Symmetry},
	author = {Kala, Zdeněk},
	month = feb,
	year = {2021},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {quantile, sensitivity analysis, buckling, civil engineering, limit states, reliability, safety, subquantile, superquantile},
	pages = {263},
	file = {Snapshot:/Users/yulei/Zotero/storage/LI5HPQQ3/263.html:text/html;Kala_2021_Global Sensitivity Analysis of Quantiles.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kala_2021_Global Sensitivity Analysis of Quantiles.pdf:application/pdf},
}

@article{eisenhauer2019ApproximateSolutionFinite,
	title = {The approximate solution of finite‐horizon discrete‐choice dynamic programming models},
	volume = {34},
	issn = {0883-7252, 1099-1255},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2648},
	doi = {10.1002/jae.2648},
	abstract = {The estimation of finite-horizon discrete-choice dynamic programming (DCDP) models is computationally expensive. This limits their realism and impedes verification and validation efforts. Keane and Wolpin (Review of Economics and Statistics, 1994, 76(4), 648–672) propose an interpolation method that ameliorates the computational burden but introduces approximation error. I describe their approach in detail, successfully recompute their original quality diagnostics, and provide some additional insights that underscore the trade-off between computation time and the accuracy of estimation results.},
	language = {en},
	number = {1},
	urldate = {2021-04-11},
	journal = {Journal of Applied Econometrics},
	author = {Eisenhauer, Philipp},
	month = jan,
	year = {2019},
	pages = {149--154},
	file = {Eisenhauer_2019_The approximate solution of finite‐horizon discrete‐choice dynamic programming.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Eisenhauer_2019_The approximate solution of finite‐horizon discrete‐choice dynamic programming.pdf:application/pdf},
}

@article{song2021QuantileSensitivityMeasures,
	title = {Quantile sensitivity measures based on subset simulation importance sampling},
	volume = {208},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832020308917},
	doi = {10.1016/j.ress.2020.107405},
	abstract = {Global sensitivity measures based on quantiles of the output are an efficient tool in measuring the effect of input variables for problems in which α−th quantiles are the functions of interest and for identification of inputs which are the most important in achieving the specific values of the model output. Previously proposed methods for numerical estimation of such measures are costly and not practically feasible in cases in which the quantile level α is very small or high. It is shown that the subset simulation importance sampling (SS-IS) method previously applied for solving small failure probability problems can be efficiently used for estimating quantile global sensitivity measures (QGSM). Considered test cases and engineering examples show that the proposed SS-IS method is more efficient than the previously proposed Monte Carlo method.},
	language = {en},
	urldate = {2021-05-19},
	journal = {Reliability Engineering \& System Safety},
	author = {Song, Shufang and Bai, Zhiwei and Kucherenko, Sergei and Wang, Lu and Yang, Caiqiong},
	month = apr,
	year = {2021},
	keywords = {Importance sampling, Quantile sensitivity measures, Subset simulation, Variance-based global sensitivity indices},
	pages = {107405},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/D7LRAGAA/S0951832020308917.html:text/html;Song et al_2021_Quantile sensitivity measures based on subset simulation importance sampling.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Song et al_2021_Quantile sensitivity measures based on subset simulation importance sampling.pdf:application/pdf},
}

@article{song2009SubsetSimulationStructural,
	title = {Subset simulation for structural reliability sensitivity analysis},
	volume = {94},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832008001944},
	doi = {10.1016/j.ress.2008.07.006},
	abstract = {Based on two procedures for efficiently generating conditional samples, i.e. Markov chain Monte Carlo (MCMC) simulation and importance sampling (IS), two reliability sensitivity (RS) algorithms are presented. On the basis of reliability analysis of Subset simulation (Subsim), the RS of the failure probability with respect to the distribution parameter of the basic variable is transformed as a set of RS of conditional failure probabilities with respect to the distribution parameter of the basic variable. By use of the conditional samples generated by MCMC simulation and IS, procedures are established to estimate the RS of the conditional failure probabilities. The formulae of the RS estimator, its variance and its coefficient of variation are derived in detail. The results of the illustrations show high efficiency and high precision of the presented algorithms, and it is suitable for highly nonlinear limit state equation and structural system with single and multiple failure modes.},
	language = {en},
	number = {2},
	urldate = {2021-05-20},
	journal = {Reliability Engineering \& System Safety},
	author = {Song, Shufang and Lu, Zhenzhou and Qiao, Hongwei},
	month = feb,
	year = {2009},
	keywords = {Conditional failure probability, Importance sampling (IS), Markov chain Monte Carlo (MCMC) simulation, Reliability sensitivity (RS), Subset simulation (Subsim)},
	pages = {658--665},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/MYAJVUSV/S0951832008001944.html:text/html;Song et al_2009_Subset simulation for structural reliability sensitivity analysis.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Song et al_2009_Subset simulation for structural reliability sensitivity analysis.pdf:application/pdf},
}

@report{eisenhauer2021StructuralModelsPolicymaking,
	title = {Structural models for policy-making: {Coping} with parametric uncertainty},
	shorttitle = {Structural models for policy-making},
	url = {https://arxiv.org/abs/2103.01115v2},
	abstract = {The ex-ante evaluation of policies using structural econometric models is based on estimated parameters as a stand-in for the truth. This practice ignores uncertainty in the counterfactual policy predictions of the model. We develop a generic approach that deals with parametric uncertainty using uncertainty sets and frames model-informed policymaking as a decision problem under uncertainty. The seminal human capital investment model by Keane and Wolpin (1997) provides us with a well-known, influential, and empirically-grounded test case. We document considerable uncertainty in their policy predictions and highlight the resulting policy recommendations from using different formal rules on decision-making under uncertainty.},
	language = {en},
	urldate = {2021-08-05},
	author = {Eisenhauer, Philipp and Gabler, Janoś and Janys, Lena},
	month = mar,
	year = {2021},
	keywords = {⛔ No DOI found},
	file = {Snapshot:/Users/yulei/Zotero/storage/VGBD3MHC/2103.html:text/html;Eisenhauer et al_2021_Structural models for policy-making.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Eisenhauer et al_2021_Structural models for policy-making.pdf:application/pdf},
}

@article{harenberg2019UncertaintyQuantificationGlobal,
	title = {Uncertainty quantification and global sensitivity analysis for economic models},
	volume = {10},
	issn = {1759-7331},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/QE866},
	doi = {10.3982/QE866},
	abstract = {We present a global sensitivity analysis that quantifies the impact of parameter uncertainty on model outcomes. Specifically, we propose variance-decomposition-based Sobol' indices to establish an importance ranking of parameters and univariate effects to determine the direction of their impact. We employ the state-of-the-art approach of constructing a polynomial chaos expansion of the model, from which Sobol' indices and univariate effects are then obtained analytically, using only a limited number of model evaluations. We apply this analysis to several quantities of interest of a standard real-business-cycle model and compare it to traditional local sensitivity analysis approaches. The results show that local sensitivity analysis can be very misleading, whereas the proposed method accurately and efficiently ranks all parameters according to importance, identifying interactions and nonlinearities.},
	language = {en},
	number = {1},
	urldate = {2021-08-12},
	journal = {Quantitative Economics},
	author = {Harenberg, Daniel and Marelli, Stefano and Sudret, Bruno and Winschel, Viktor},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/QE866},
	keywords = {sensitivity analysis, C60, C63, Computational techniques, polynomial chaos expansion, uncertainty quantification},
	pages = {1--41},
	file = {Snapshot:/Users/yulei/Zotero/storage/KRPWAPGC/QE866.html:text/html;Harenberg et al_2019_Uncertainty quantification and global sensitivity analysis for economic models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Harenberg et al_2019_Uncertainty quantification and global sensitivity analysis for economic models.pdf:application/pdf},
}

@article{tennoe2018UncertainpyPythonToolbox,
	title = {Uncertainpy: {A} {Python} {Toolbox} for {Uncertainty} {Quantification} and {Sensitivity} {Analysis} in {Computational} {Neuroscience}},
	volume = {12},
	issn = {1662-5196},
	shorttitle = {Uncertainpy},
	url = {https://www.frontiersin.org/article/10.3389/fninf.2018.00049/full},
	doi = {10.3389/fninf.2018.00049},
	language = {en},
	urldate = {2021-08-18},
	journal = {Frontiers in Neuroinformatics},
	author = {Tennøe, Simen and Halnes, Geir and Einevoll, Gaute T.},
	month = aug,
	year = {2018},
	pages = {49},
	file = {Tennøe et al_2018_Uncertainpy.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Tennøe et al_2018_Uncertainpy.pdf:application/pdf},
}

@techreport{blesch2021RobustDecisionMakingRisk,
	title = {Robust {Decision}-{Making} {Under} {Risk} and {Ambiguity}},
	url = {https://ideas.repec.org/p/ajk/ajkdps/104.html},
	abstract = {Economists often estimate a subset of their model parameters outside the model and let the decision-makers inside the model treat these point estimates as-if they are correct. This practice ignores model ambiguity, opens the door for misspecification of the decision problem, and leads to post-decision disappointment. We develop a framework to explore, evaluate, and optimize decision rules that explicitly account for the uncertainty in the first step estimation using statistical decision theory. We show how to operationalize our analysis by studying a stochastic dynamic investment model where the decision-makers take ambiguity about the model's transition dynamics directly into account.},
	language = {en},
	number = {104},
	urldate = {2021-08-20},
	institution = {University of Bonn and University of Cologne, Germany},
	author = {Blesch, Maximilian and Eisenhauer, Philipp},
	month = jul,
	year = {2021},
	note = {Publication Title: ECONtribute Discussion Papers Series},
	keywords = {decision-making under uncertainty, robust Markov decision process},
	file = {Snapshot:/Users/yulei/Zotero/storage/V3BQCUSY/104.html:text/html;Blesch_Eisenhauer_2021_Robust Decision-Making Under Risk and Ambiguity.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Blesch_Eisenhauer_2021_Robust Decision-Making Under Risk and Ambiguity.pdf:application/pdf},
}

@article{lopiano2019NutritionPublicHealth,
	title = {Nutrition and public health economic evaluations under the lenses of post normal science},
	volume = {112},
	issn = {0016-3287},
	url = {https://www.sciencedirect.com/science/article/pii/S0016328718304087},
	doi = {10.1016/j.futures.2019.06.008},
	abstract = {The emerging scientific field of public health economics, considering health-related behaviours such as physical activity and smoking, is establishing itself as an important component in assessing the impact of policy interventions on preventing disease. Epidemiological evidence points to links between diet, lifestyle and non-communicable diseases (NCDs). Policy decisions aimed at preventing or reducing the burden of NCDs need credible, reliable and evidence-based scientific information. In this context, facts are uncertain and values are contested, while decisions are often urgent and the stakes are high, a typical setting for post-normal science (PNS). Therefore, this work applies quality tools developed in the context of PNS to models used in nutrition and public health economics, using as a guide the seven-point checklist of sensitivity auditing. This analysis has identified scope for improvement in a number of areas, such as the definition of the modelling exercise scope, the justification of the choice of family of models adopted, comprehensively framing the issue by including the perspective of relevant stakeholders and the exertion of more care in justifying assumptions. Addressing these dimensions may even result in refraining from producing a quantitative assessment when the circumstances do not hold. This would conflict with the common imperative to quantify in regulatory policies - often requested by policy guidelines - and with the dynamics of the policy cycle. For this reason, we discuss the implied trade-offs, contradictions and way forward with an eye to achieving progress in the practice.},
	language = {en},
	urldate = {2021-08-24},
	journal = {Futures},
	author = {Lo Piano, Samuele and Robinson, Marguerite},
	month = sep,
	year = {2019},
	keywords = {Sensitivity analysis, Cost-benefit analysis, Ethics of quantification, Health economics, Post-normal science, Sensitivity auditing},
	pages = {102436},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/YTT89VAF/S0016328718304087.html:text/html;Lo Piano_Robinson_2019_Nutrition and public health economic evaluations under the lenses of post.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Lo Piano_Robinson_2019_Nutrition and public health economic evaluations under the lenses of post.pdf:application/pdf},
}

@article{saltelli2015ClimateModelsEconomic,
	title = {Climate {Models} as {Economic} {Guides} {Scientific} {Challenge} or {Quixotic} {Quest}?},
	volume = {31},
	issn = {0748-5492},
	url = {https://www.jstor.org/stable/43314858},
	number = {3},
	urldate = {2021-08-24},
	journal = {Issues in Science and Technology},
	author = {SALTELLI, ANDREA and STARK, PHILIP B. and BECKER, WILLIAM and STANO, PAWEL},
	year = {2015},
	note = {Publisher: University of Texas at Dallas},
	keywords = {⛔ No DOI found},
	pages = {79--84},
	file = {SALTELLI et al_2015_Climate Models as Economic Guides Scientific Challenge or Quixotic Quest.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/SALTELLI et al_2015_Climate Models as Economic Guides Scientific Challenge or Quixotic Quest.pdf:application/pdf},
}

@misc{JRCPublicationsRepository,
	title = {{JRC} {Publications} {Repository} - {Climate} {Models} as {Economic} {Guides}: {Scientific} {Challenge} or {Quixotic} {Quest}?},
	url = {https://publications.jrc.ec.europa.eu/repository/handle/JRC94904},
	urldate = {2021-08-26},
	file = {JRC Publications Repository - Climate Models as Economic Guides\: Scientific Challenge or Quixotic Quest?:/Users/yulei/Zotero/storage/TCIDEYHG/JRC94904.html:text/html},
}

@article{saltelli2019WhyManyPublished,
	title = {Why so many published sensitivity analyses are false: {A} systematic review of sensitivity analysis practices},
	volume = {114},
	issn = {1364-8152},
	shorttitle = {Why so many published sensitivity analyses are false},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815218302822},
	doi = {10.1016/j.envsoft.2019.01.012},
	abstract = {Sensitivity analysis provides information on the relative importance of model input parameters and assumptions. It is distinct from uncertainty analysis, which addresses the question ‘How uncertain is the prediction?’ Uncertainty analysis needs to map what a model does when selected input assumptions and parameters are left free to vary over their range of existence, and this is equally true of a sensitivity analysis. Despite this, many uncertainty and sensitivity analyses still explore the input space moving along one-dimensional corridors leaving space of the input factors mostly unexplored. Our extensive systematic literature review shows that many highly cited papers (42\% in the present analysis) fail the elementary requirement to properly explore the space of the input factors. The results, while discipline-dependent, point to a worrying lack of standards and recognized good practices. We end by exploring possible reasons for this problem, and suggest some guidelines for proper use of the methods.},
	language = {en},
	urldate = {2021-09-01},
	journal = {Environmental Modelling \& Software},
	author = {Saltelli, Andrea and Aleksankina, Ksenia and Becker, William and Fennell, Pamela and Ferretti, Federico and Holst, Niels and Li, Sushan and Wu, Qiongli},
	month = apr,
	year = {2019},
	pages = {29--39},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/U75ISW8F/S1364815218302822.html:text/html;Saltelli et al_2019_Why so many published sensitivity analyses are false.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Saltelli et al_2019_Why so many published sensitivity analyses are false.pdf:application/pdf},
}

@book{ghanem2017HandbookUncertaintyQuantification,
	address = {Cham},
	title = {Handbook of {Uncertainty} {Quantification}},
	isbn = {978-3-319-12384-4 978-3-319-12385-1},
	url = {http://link.springer.com/10.1007/978-3-319-12385-1},
	language = {en},
	urldate = {2021-09-01},
	publisher = {Springer International Publishing},
	editor = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
	year = {2017},
	doi = {10.1007/978-3-319-12385-1},
	file = {Ghanem et al_2017_Handbook of Uncertainty Quantification.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Ghanem et al_2017_Handbook of Uncertainty Quantification.pdf:application/pdf},
}

@article{sobol2001GlobalSensitivityIndices,
	series = {The {Second} {IMACS} {Seminar} on {Monte} {Carlo} {Methods}},
	title = {Global sensitivity indices for nonlinear mathematical models and their {Monte} {Carlo} estimates},
	volume = {55},
	issn = {0378-4754},
	url = {https://www.sciencedirect.com/science/article/pii/S0378475400002706},
	doi = {10.1016/S0378-4754(00)00270-6},
	abstract = {Global sensitivity indices for rather complex mathematical models can be efficiently computed by Monte Carlo (or quasi-Monte Carlo) methods. These indices are used for estimating the influence of individual variables or groups of variables on the model output.},
	language = {en},
	number = {1},
	urldate = {2021-09-05},
	journal = {Mathematics and Computers in Simulation},
	author = {Sobol′, I. M},
	month = feb,
	year = {2001},
	keywords = {Sensitivity analysis, Mathematical modelling, Monte Carlo method, Quasi-Monte Carlo method},
	pages = {271--280},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/JGQHDIZL/S0378475400002706.html:text/html;Sobol′_2001_Global sensitivity indices for nonlinear mathematical models and their Monte.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Sobol′_2001_Global sensitivity indices for nonlinear mathematical models and their Monte.pdf:application/pdf},
}

@article{fort2016NewSensitivityAnalysis,
	title = {New sensitivity analysis subordinated to a contrast},
	volume = {45},
	issn = {0361-0926},
	url = {https://doi.org/10.1080/03610926.2014.901369},
	doi = {10.1080/03610926.2014.901369},
	abstract = {In a model of the form Y = h(X1, …, Xd) where the goal is to estimate a parameter of the probability distribution of Y, we define new sensitivity indices which quantify the importance of each variable Xi with respect to this parameter of interest. The aim of this paper is to define goal oriented sensitivity indices and we will show that Sobol indices are sensitivity indices associated to a particular characteristic of the distribution Y. We name the framework we present as Goal Oriented Sensitivity Analysis (GOSA).},
	number = {15},
	urldate = {2021-09-05},
	journal = {Communications in Statistics - Theory and Methods},
	author = {Fort, Jean-Claude and Klein, Thierry and Rachdi, Nabil},
	month = aug,
	year = {2016},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/03610926.2014.901369},
	keywords = {Sensitivity analysis, 62K99, 62P30, 65C60, 97M50, Sobol indices},
	pages = {4349--4364},
	file = {Snapshot:/Users/yulei/Zotero/storage/5UIZ9R4H/03610926.2014.html:text/html;Fort et al_2016_New sensitivity analysis subordinated to a contrast.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Fort et al_2016_New sensitivity analysis subordinated to a contrast.pdf:application/pdf},
}

@unpublished{browne2017EstimateQuantileorientedSensitivity,
	title = {Estimate of quantile-oriented sensitivity indices},
	url = {https://hal.archives-ouvertes.fr/hal-01450891},
	abstract = {In the context of black-box numerical codes, it is relevant to use sensitivity analysis in order to assess the influence of each random input X over the output Y. Goal-oriented sensitivity analysis states that one must first focus on a certain probability feature θ(Y) from the distribution of Y (such as its mean, quantile, or a probability of failure etc...), which would be chosen regarding a relevant strategy. The wish is to evaluate the impact of each input over θ(Y). In order to get supplementary information about sensitivity, we set that θ(Y) is the α-level quantile of Y , where α ∈]0, 1[. Throughout some examples, it has been pointed out that in some cases quantile-oriented sensitivity indices can detect some influence that Sobol indices would not. Mainly, the influence over each level of quantile displays how an input distribution entirely propagates through the output. We establish further results for the quantile-oriented indices properties in order to justify their relevancy. The main contribution of this paper comes when a statistical estimator for this index is introduced.},
	urldate = {2021-09-05},
	author = {Browne, Thomas and Fort, Jean-Claude and Iooss, Bertrand and Le Gratiet, Loïc},
	month = jan,
	year = {2017},
	keywords = {Sensitivity analysis, Sobol indices, goal-oriented sensitivity analysis, kernel-based estimators, output quantiles},
	file = {Browne et al_2017_Estimate of quantile-oriented sensitivity indices.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Browne et al_2017_Estimate of quantile-oriented sensitivity indices.pdf:application/pdf},
}

@article{maume-deschamps2018EstimationQuantileOriented,
	title = {Estimation of quantile oriented sensitivity indices},
	volume = {134},
	issn = {0167-7152},
	url = {https://www.sciencedirect.com/science/article/pii/S0167715217303413},
	doi = {10.1016/j.spl.2017.10.019},
	abstract = {This paper concerns quantile oriented sensitivity analysis (qosa). We rewrite the corresponding indices using the Conditional Tail Expectation risk measure. Then, we use this new expression to built estimators of qosa indices.},
	language = {en},
	urldate = {2021-09-05},
	journal = {Statistics \& Probability Letters},
	author = {Maume-Deschamps, Véronique and Niang, Ibrahima},
	month = mar,
	year = {2018},
	keywords = {Sensitivity analysis, Quantile oriented indices, Risk measures},
	pages = {122--127},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/F9NUSMYK/S0167715217303413.html:text/html;Maume-Deschamps_Niang_2018_Estimation of quantile oriented sensitivity indices.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Maume-Deschamps_Niang_2018_Estimation of quantile oriented sensitivity indices.pdf:application/pdf},
}

@article{cui2010MomentindependentImportanceMeasure,
	title = {Moment-independent importance measure of basic random variable and its probability density evolution solution},
	volume = {53},
	issn = {1862-281X},
	url = {https://doi.org/10.1007/s11431-009-0386-8},
	doi = {10.1007/s11431-009-0386-8},
	abstract = {To analyze the effect of basic variable on failure probability in reliability analysis, a moment-independent importance measure of the basic random variable is proposed, and its properties are analyzed and verified. Based on this work, the importance measure of the basic variable on the failure probability is compared with that on the distribution density of the response. By use of the probability density evolution method, a solution is established to solve two importance measures, which can efficiently avoid the difficulty in solving the importance measures. Some numerical examples and engineering examples are used to demonstrate the proposed importance measure on the failure probability and that on the distribution density of the response. The results show that the proposed importance measure can effectively describe the effect of the basic variable on the failure probability from the distribution density of the basic variable. Additionally, the results show that the established solution on the probability density evolution is efficient for the importance measures.},
	language = {en},
	number = {4},
	urldate = {2021-09-05},
	journal = {Science China Technological Sciences},
	author = {Cui, LiJie and Lü, ZhenZhou and Zhao, XinPan},
	month = apr,
	year = {2010},
	pages = {1138--1145},
	file = {Cui et al_2010_Moment-independent importance measure of basic random variable and its.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Cui et al_2010_Moment-independent importance measure of basic random variable and its.pdf:application/pdf},
}

@article{kucherenko2009MonteCarloEvaluation,
	series = {Special {Issue} on {Sensitivity} {Analysis}},
	title = {Monte {Carlo} evaluation of derivative-based global sensitivity measures},
	volume = {94},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S095183200800152X},
	doi = {10.1016/j.ress.2008.05.006},
	abstract = {A novel approach for evaluation of derivative-based global sensitivity measures (DGSM) is presented. It is compared with the Morris and the Sobol’ sensitivity indices methods. It is shown that there is a link between DGSM and Sobol’ sensitivity indices. DGSM are very easy to implement and evaluate numerically. The computational time required for numerical evaluation of DGSM is many orders of magnitude lower than that for estimation of the Sobol’ sensitivity indices. It is also lower than that for the Morris method. Efficiencies of Monte Carlo (MC) and quasi-Monte Carlo (QMC) sampling methods for calculation of DGSM are compared. It is shown that the superiority of QMC over MC depends on the problem's effective dimension, which can also be estimated using DGSM.},
	language = {en},
	number = {7},
	urldate = {2021-09-06},
	journal = {Reliability Engineering \& System Safety},
	author = {Kucherenko, S. and Rodriguez-Fernandez, M. and Pantelides, C. and Shah, N.},
	month = jul,
	year = {2009},
	keywords = {Global sensitivity analysis, Sobol’ sensitivity indices, Derivative-based global measures, Monte Carlo methods, Morris method, Quasi-Monte Carlo methods, Sobol’ sequences},
	pages = {1135--1148},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/AVT8CZG5/S095183200800152X.html:text/html;Kucherenko et al_2009_Monte Carlo evaluation of derivative-based global sensitivity measures.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kucherenko et al_2009_Monte Carlo evaluation of derivative-based global sensitivity measures.pdf:application/pdf},
}

@article{sobol2009DerivativeBasedGlobal,
	title = {Derivative based global sensitivity measures and their link with global sensitivity indices},
	volume = {79},
	issn = {0378-4754},
	url = {https://www.sciencedirect.com/science/article/pii/S0378475409000354},
	doi = {10.1016/j.matcom.2009.01.023},
	abstract = {A model function f(x1,…,xn) defined in the unit hypercube Hn with Lebesque measure dx=dx1…dxn is considered. If the function is square integrable, global sensitivity indices provide adequate estimates for the influence of individual factors xi or groups of such factors. Alternative estimators that require less computer time can also be used. If the function f is differentiable, functionals depending on ∂f/∂xi have been suggested as estimators for the influence of xi. The Morris importance measure modified by Campolongo, Cariboni and Saltelli μ* is an approximation of the functional μi=∫Hn∂f/∂xidx. In this paper a similar functional is studiedνi=∫Hn∂f∂xi2dxEvidently, μi≤νi, and νi≤Cμi if ∂f/∂xi≤C. A link between νi and the sensitivity index Sitot is established:Sitot≤νiπ2Dwhere D is the total variance of f(x1,…,xn). Thus small νi imply small Sitot, and unessential factors xi (that is xi corresponding to a very small Sitot) can be detected analyzing computed values ν1,…,νn. However, ranking influential factors xi using these values can give false conclusions. Generalized Sitot and νi can be applied in situations where the factors x1,…,xn are independent random variables. If xi is a normal random variable with variance σi2, then Sitot≤νiσi2/D.},
	language = {en},
	number = {10},
	urldate = {2021-09-06},
	journal = {Mathematics and Computers in Simulation},
	author = {Sobol’, I. M. and Kucherenko, S.},
	month = jun,
	year = {2009},
	keywords = {Morris method, Derivative based global sensitivity measure, Global sensitivity index, Quasi Monte Carlo method},
	pages = {3009--3017},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/NHEFQPAN/S0378475409000354.html:text/html;Sobol’_Kucherenko_2009_Derivative based global sensitivity measures and their link with global.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Sobol’_Kucherenko_2009_Derivative based global sensitivity measures and their link with global.pdf:application/pdf},
}

@techreport{glynn1996ImportanceSamplingMonte,
	title = {Importance {Sampling} {For} {Monte} {Carlo} {Estimation} {Of} {Quantiles}},
	abstract = {This paper is concerned with applying importance sampling as a variance reduction tool for computing extreme quantiles. A central limit theorem is derived for each of four proposed importance sampling quantile estimators. Efficiency comparisons are provided in a certain asymptotic setting, using ideas from large deviation theory.  Keywords: quantiles, importance sampling, large deviations. 1 Introduction  Let X be a real-valued random variable with distribution function F ({\textbackslash}Delta). For 0 ! p ! 1, the quantity  F  {\textbackslash}Gamma1  (p)  4  = inffx : F (x)  pg  is called the p'th quantile of X. The quantile estimation problem is concerned with computing such quantities efficiently. Quantile estimation is of interest in many applications settings. For example, in computing tables of critical values associated with complicated hypothesis tests in which F cannot be computed analytically in closed form, it may be necessary to resort to Monte Carlo methodology as a means of calculating such critical v...},
	institution = {Publishing House of Saint Petersburg University},
	author = {Glynn, Peter W.},
	year = {1996},
	file = {Citeseer - Snapshot:/Users/yulei/Zotero/storage/SEJUVV2U/summary.html:text/html;Glynn_1996_Importance Sampling For Monte Carlo Estimation Of Quantiles.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Glynn_1996_Importance Sampling For Monte Carlo Estimation Of Quantiles.pdf:application/pdf},
}

@article{egloff2010QuantileEstimationAdaptive,
	title = {Quantile estimation with adaptive importance sampling},
	volume = {38},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-38/issue-2/Quantile-estimation-with-adaptive-importance-sampling/10.1214/09-AOS745.full},
	doi = {10.1214/09-AOS745},
	abstract = {We introduce new quantile estimators with adaptive importance sampling. The adaptive estimators are based on weighted samples that are neither independent nor identically distributed. Using a new law of iterated logarithm for martingales, we prove the convergence of the adaptive quantile estimators for general distributions with nonunique quantiles thereby extending the work of Feldman and Tucker [Ann. Math. Statist. 37 (1996) 451–457]. We illustrate the algorithm with an example from credit portfolio risk analysis.},
	number = {2},
	urldate = {2021-09-06},
	journal = {The Annals of Statistics},
	author = {Egloff, Daniel and Leippold, Markus},
	month = apr,
	year = {2010},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {65C60, 62L20, 65C05, Adaptive importance sampling, Law of iterated logarithm, Quantile estimation, Robbins–Monro, stochastic approximation},
	pages = {1244--1278},
	file = {Snapshot:/Users/yulei/Zotero/storage/Q5BU2V85/09-AOS745.html:text/html;Egloff_Leippold_2010_Quantile estimation with adaptive importance sampling.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Egloff_Leippold_2010_Quantile estimation with adaptive importance sampling.pdf:application/pdf},
}

@article{oakley2004EstimatingPercentilesUncertain,
	title = {Estimating percentiles of uncertain computer code outputs},
	volume = {53},
	issn = {1467-9876},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1046/j.0035-9254.2003.05044.x},
	doi = {10.1046/j.0035-9254.2003.05044.x},
	abstract = {Summary. A deterministic computer model is to be used in a situation where there is uncertainty about the values of some or all of the input parameters. This uncertainty induces uncertainty in the output of the model. We consider the problem of estimating a specific percentile of the distribution of this uncertain output. We also suppose that the computer code is computationally expensive, so we can run the model only at a small number of distinct inputs. This means that we must consider our uncertainty about the computer code itself at all untested inputs. We model the output, as a function of its inputs, as a Gaussian process, and after a few initial runs of the code use a simulation approach to choose further suitable design points and to make inferences about the percentile of interest itself. An example is given involving a model that is used in sewer design.},
	language = {en},
	number = {1},
	urldate = {2021-09-06},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Oakley, Jeremy},
	year = {2004},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1046/j.0035-9254.2003.05044.x},
	keywords = {Deterministic computer code, Gaussian process, Uncertainty distribution},
	pages = {83--93},
	file = {Snapshot:/Users/yulei/Zotero/storage/XWINWG5A/j.0035-9254.2003.05044.html:text/html;Oakley_2004_Estimating percentiles of uncertain computer code outputs.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Oakley_2004_Estimating percentiles of uncertain computer code outputs.pdf:application/pdf},
}

@article{saltelli2002SensitivityAnalysisImportance,
	title = {Sensitivity {Analysis} for {Importance} {Assessment}},
	volume = {22},
	issn = {1539-6924},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/0272-4332.00040},
	doi = {10.1111/0272-4332.00040},
	abstract = {We review briefly some examples that would support an extended role for quantitative sensitivity analysis in the context of model-based analysis (Section 1). We then review what features a quantitative sensitivity analysis needs to have to play such a role (Section 2). The methods that meet these requirements are described in Section 3; an example is provided in Section 4. Some pointers to further research are set out in Section 5.},
	language = {en},
	number = {3},
	urldate = {2021-09-06},
	journal = {Risk Analysis},
	author = {Saltelli, Andrea},
	year = {2002},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/0272-4332.00040},
	keywords = {risk analysis, Uncertainty analysis, assessment of importance, computational models, quantitative sensitivity analysis},
	pages = {579--590},
	file = {Snapshot:/Users/yulei/Zotero/storage/K3T6C87F/0272-4332.html:text/html;Saltelli_2002_Sensitivity Analysis for Importance Assessment.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Saltelli_2002_Sensitivity Analysis for Importance Assessment.pdf:application/pdf},
}

@article{marino2008MethodologyPerformingGlobal,
	title = {A {Methodology} {For} {Performing} {Global} {Uncertainty} {And} {Sensitivity} {Analysis} {In} {Systems} {Biology}},
	volume = {254},
	issn = {0022-5193},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2570191/},
	doi = {10.1016/j.jtbi.2008.04.011},
	abstract = {Accuracy of results from mathematical and computer models of biological systems is often complicated by the presence of uncertainties in experimental data that are used to estimate parameter values. Current mathematical modeling approaches typically use either single-parameter or local sensitivity analyses. However, these methods do not accurately assess uncertainty and sensitivity in the system as, by default they hold all other parameters fixed at baseline values. Using techniques described within we demonstrate how a multi-dimensional parameter space can be studied globally so all uncertainties can be identified. Further, uncertainty and sensitivity analysis techniques can help to identify and ultimately control uncertainties. In this work we develop methods for applying existing analytical tools to perform analyses on a variety of mathematical and computer models. We compare two specific types of global sensitivity analysis indexes that have proven to be among the most robust and efficient. Through familiar and new examples of mathematical and computer models, we provide a complete methodology for performing these analyses, both in deterministic and stochastic settings, and propose novel techniques to handle problems encountered during this type of analyses.},
	number = {1},
	urldate = {2021-09-06},
	journal = {Journal of theoretical biology},
	author = {Marino, Simeone and Hogue, Ian B. and Ray, Christian J. and Kirschner, Denise E.},
	month = sep,
	year = {2008},
	pmid = {18572196},
	pmcid = {PMC2570191},
	pages = {178--196},
	file = {Marino et al_2008_A Methodology For Performing Global Uncertainty And Sensitivity Analysis In.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Marino et al_2008_A Methodology For Performing Global Uncertainty And Sensitivity Analysis In.pdf:application/pdf},
}

@article{zi2011SensitivityAnalysisApproaches,
	title = {Sensitivity analysis approaches applied to systems biology models},
	volume = {5},
	issn = {1751-8849},
	doi = {10.1049/iet-syb.2011.0015},
	abstract = {With the rising application of systems biology, sensitivity analysis methods have been widely applied to study the biological systems, including metabolic networks, signalling pathways and genetic circuits. Sensitivity analysis can provide valuable insights about how robust the biological responses are with respect to the changes of biological parameters and which model inputs are the key factors that affect the model outputs. In addition, sensitivity analysis is valuable for guiding experimental analysis, model reduction and parameter estimation. Local and global sensitivity analysis approaches are the two types of sensitivity analysis that are commonly applied in systems biology. Local sensitivity analysis is a classic method that studies the impact of small perturbations on the model outputs. On the other hand, global sensitivity analysis approaches have been applied to understand how the model outputs are affected by large variations of the model input parameters. In this review, the author introduces the basic concepts of sensitivity analysis approaches applied to systems biology models. Moreover, the author discusses the advantages and disadvantages of different sensitivity analysis methods, how to choose a proper sensitivity analysis approach, the available sensitivity analysis tools for systems biology models and the caveats in the interpretation of sensitivity analysis results.},
	language = {eng},
	number = {6},
	journal = {IET systems biology},
	author = {Zi, Z.},
	month = nov,
	year = {2011},
	pmid = {22129029},
	keywords = {Software, Models, Biological, Systems Biology},
	pages = {336--336},
	file = {Zi_2011_Sensitivity analysis approaches applied to systems biology models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Zi_2011_Sensitivity analysis approaches applied to systems biology models.pdf:application/pdf},
}

@article{pannell1997SensitivityAnalysisNormative,
	title = {Sensitivity analysis of normative economic models: theoretical framework and practical strategies},
	volume = {16},
	issn = {0169-5150},
	shorttitle = {Sensitivity analysis of normative economic models},
	url = {https://www.sciencedirect.com/science/article/pii/S0169515096012170},
	doi = {10.1016/S0169-5150(96)01217-0},
	abstract = {The parameter values and assumptions of any economic model are subject to change and error. Sensitivity analysis (SA), broadly defined, is the investigation of these potential changes and errors and their impacts on conclusions to be drawn from the model. There is a very large literature on procedures and techniques for SA, but it includes almost nothing from economists. This paper is a selective review and overview of theoretical and methodological issues in SA. There are many possible uses of SA, described here within the categories of decision support, communication, increased understanding or quantification of the system, and model development. The paper focuses somewhat on decision support. It is argued that even the simplest approaches to SA can be theoretically respectable in decision support if they are applied and interpreted in a way consistent with Bayesian decision theory. This is not to say that SA results should be formally subjected to a Bayesian decision analysis, but that an understanding of Bayesian probability revision will help the modeller plan and interpret an SA. Many different approaches to SA are described, varying in the experimental design used and in the way results are processed. Possible overall strategies for conducting SA are suggested. It is proposed that when using SA for decision support, it can be very helpful to attempt to identify which of the following forms of recommendation is most appropriate: (a) do X, (b) do either X or Y depending on the circumstances, (c) do either X or Y, whichever you like, (d) if in doubt, do X. A system for reporting and discussing SA results is recommended.},
	language = {en},
	number = {2},
	urldate = {2021-09-06},
	journal = {Agricultural Economics},
	author = {Pannell, David J.},
	month = may,
	year = {1997},
	pages = {139--152},
	file = {Pannell_1997_Sensitivity analysis of normative economic models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Pannell_1997_Sensitivity analysis of normative economic models.pdf:application/pdf},
}

@article{degenring2004SensitivityAnalysisReduction,
	series = {Dynamics, {Monitoring}, {Control} and {Optimization} of {Biological} {Systems}},
	title = {Sensitivity analysis for the reduction of complex metabolism models},
	volume = {14},
	issn = {0959-1524},
	url = {https://www.sciencedirect.com/science/article/pii/S0959152403001379},
	doi = {10.1016/j.jprocont.2003.12.008},
	abstract = {Two different model reduction strategies are studied in order to test their applicability to reduce complex metabolism models. Using a model of one pre-identified model set describing complex metabolic dynamics after glucose pulse stimulation, a model reduction method based on the parameter tuning importance is compared with a pca based approach. Up to 49 of 122 parameters are rejected without significant changes of the simulated trajectories and of the flux distribution. Applying the reduction procedure to 12 other dynamic models reveals a general model structure inconsistency within the description of the pentose phosphate pathway. That points out the need of additional experiments to reproduce metabolite courses especially of this metabolic pathway. Thus the sensitivity based model reduction procedure is qualified as a promising tool for the model structure check and can be very useful for the entire model validation process which also includes the critical analysis of the data sets underlying the models.},
	language = {en},
	number = {7},
	urldate = {2021-09-06},
	journal = {Journal of Process Control},
	author = {Degenring, D. and Froemel, C. and Dikta, G. and Takors, R.},
	month = oct,
	year = {2004},
	keywords = {Metabolism, Model reduction, Model validation, Parameter sensitivity, Pca},
	pages = {729--745},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/8UX736BH/S0959152403001379.html:text/html;Degenring et al_2004_Sensitivity analysis for the reduction of complex metabolism models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Degenring et al_2004_Sensitivity analysis for the reduction of complex metabolism models.pdf:application/pdf},
}

@article{feinberg2015ChaospyOpenSource,
	title = {Chaospy: {An} open source tool for designing methods of uncertainty quantification},
	volume = {11},
	issn = {1877-7503},
	shorttitle = {Chaospy},
	url = {https://www.sciencedirect.com/science/article/pii/S1877750315300119},
	doi = {10.1016/j.jocs.2015.08.008},
	abstract = {The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables.},
	language = {en},
	urldate = {2021-09-06},
	journal = {Journal of Computational Science},
	author = {Feinberg, Jonathan and Langtangen, Hans Petter},
	month = nov,
	year = {2015},
	keywords = {Monte Carlo simulation, Uncertainty quantification, Polynomial chaos expansions, Python package, Rosenblatt transformations},
	pages = {46--57},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/4UZJP8TC/S1877750315300119.html:text/html;Feinberg_Langtangen_2015_Chaospy.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Feinberg_Langtangen_2015_Chaospy.pdf:application/pdf},
}

@article{herman2017SALibOpensourcePython,
	title = {{SALib}: {An} open-source {Python} library for {Sensitivity} {Analysis}},
	volume = {2},
	issn = {2475-9066},
	shorttitle = {{SALib}},
	url = {https://joss.theoj.org/papers/10.21105/joss.00097},
	doi = {10.21105/joss.00097},
	abstract = {Herman et al, (2017), SALib: An open-source Python library for Sensitivity Analysis, Journal of Open Source Software, 2(9), 97, doi:10.21105/joss.00097},
	language = {en},
	number = {9},
	urldate = {2021-09-06},
	journal = {Journal of Open Source Software},
	author = {Herman, Jon and Usher, Will},
	month = jan,
	year = {2017},
	pages = {97},
	file = {Snapshot:/Users/yulei/Zotero/storage/6Z7CKAWX/joss.html:text/html;Herman_Usher_2017_SALib.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Herman_Usher_2017_SALib.pdf:application/pdf},
}

@book{wolpin2013LimitsInferenceTheory,
	address = {Cambridge, MA, USA},
	series = {Tjalling {C}. {Koopmans} {Memorial} {Lectures}},
	title = {The {Limits} of {Inference} without {Theory}},
	isbn = {978-0-262-01908-8},
	abstract = {The role of theory in ex ante policy evaluations and the limits that eschewing theory places on inference},
	language = {en},
	publisher = {MIT Press},
	author = {Wolpin, Kenneth I.},
	editor = {Samuelson, Larry},
	month = apr,
	year = {2013},
}

@article{saltelli2020FiveWaysEnsure,
	title = {Five ways to ensure that models serve society: a manifesto},
	volume = {582},
	copyright = {2021 Nature},
	shorttitle = {Five ways to ensure that models serve society},
	url = {https://www.nature.com/articles/d41586-020-01812-9},
	doi = {10.1038/d41586-020-01812-9},
	abstract = {Pandemic politics highlight how predictions need to be transparent and humble to invite insight, not blame.},
	language = {en},
	number = {7813},
	urldate = {2021-09-06},
	journal = {Nature},
	author = {Saltelli, Andrea and Bammer, Gabriele and Bruno, Isabelle and Charters, Erica and Di Fiore, Monica and Didier, Emmanuel and Nelson Espeland, Wendy and Kay, John and Lo Piano, Samuele and Mayo, Deborah and Pielke Jr, Roger and Portaluri, Tommaso and Porter, Theodore M. and Puy, Arnald and Rafols, Ismael and Ravetz, Jerome R. and Reinert, Erik and Sarewitz, Daniel and Stark, Philip B. and Stirling, Andrew and van der Sluijs, Jeroen and Vineis, Paolo},
	month = jun,
	year = {2020},
	note = {Bandiera\_abtest: a
Cg\_type: Comment
Number: 7813
Publisher: Nature Publishing Group
Subject\_term: Communication, Policy, Epidemiology},
	pages = {482--484},
	file = {Snapshot:/Users/yulei/Zotero/storage/QMZ8GA8Q/d41586-020-01812-9.html:text/html;Saltelli et al_2020_Five ways to ensure that models serve society.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Saltelli et al_2020_Five ways to ensure that models serve society.pdf:application/pdf},
}

@book{nationalresearchcouncil2012AssessingReliabilityComplex,
	address = {Washington, DC},
	title = {Assessing the {Reliability} of {Complex} {Models}: {Mathematical} and {Statistical} {Foundations} of {Verification}, {Validation}, and {Uncertainty} {Quantification}},
	isbn = {978-0-309-25634-6},
	shorttitle = {Assessing the {Reliability} of {Complex} {Models}},
	url = {https://www.nap.edu/catalog/13395/assessing-the-reliability-of-complex-models-mathematical-and-statistical-foundations},
	abstract = {Advances in computing hardware and algorithms have dramatically improved the ability to simulate complex processes computationally. Today's simulation capabilities offer the prospect of addressing questions that in the past could be addressed only by resource-intensive experimentation, if at all. Assessing the Reliability of Complex Models recognizes the ubiquity of uncertainty in computational estimates of reality and the necessity for its quantification.
As computational science and engineering have matured, the process of quantifying or bounding uncertainties in a computational estimate of a physical quality of interest has evolved into a small set of interdependent tasks: verification, validation, and uncertainty of quantification (VVUQ). In recognition of the increasing importance of computational simulation and the increasing need to assess uncertainties in computational results, the National Research Council was asked to study the mathematical foundations of VVUQ and to recommend steps that will ultimately lead to improved processes.
Assessing the Reliability of Complex Models discusses changes in education of professionals and dissemination of information that should enhance the ability of future VVUQ practitioners to improve and properly apply VVUQ methodologies to difficult problems, enhance the ability of VVUQ customers to understand VVUQ results and use them to make informed decisions, and enhance the ability of all VVUQ stakeholders to communicate with each other. This report is an essential resource for all decision and policy makers in the field, students, stakeholders, UQ experts, and VVUQ educators and practitioners.},
	language = {English},
	urldate = {2021-09-06},
	publisher = {The National Academies Press},
	author = {National Research Council},
	year = {2012},
	doi = {10.17226/13395},
	keywords = {Math, Chemistry, and Physics, Surveys and Statistics},
}

@book{scienceadviceforpolicybyeuropeanacademies2019MakingSenseScience,
	address = {DE},
	title = {Making sense of science for policy under conditions of complexity and uncertainty},
	url = {https://doi.org/10.26356/masos},
	language = {en},
	urldate = {2021-09-06},
	publisher = {Science Advice for Policy by European Academies},
	author = {{Science Advice for Policy by European Academies}},
	year = {2019},
	file = {Science Advice for Policy by European Academies_2019_Making sense of science for policy under conditions of complexity and.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Science Advice for Policy by European Academies_2019_Making sense of science for policy under conditions of complexity and.pdf:application/pdf},
}

@incollection{keane2011ChapterStructuralEstimation,
	title = {Chapter 4 - {The} {Structural} {Estimation} of {Behavioral} {Models}: {Discrete} {Choice} {Dynamic} {Programming} {Methods} and {Applications}},
	volume = {4},
	shorttitle = {Chapter 4 - {The} {Structural} {Estimation} of {Behavioral} {Models}},
	url = {https://www.sciencedirect.com/science/article/pii/S0169721811004102},
	abstract = {The purpose of this chapter is twofold: (1) to provide an accessible introduction to the methods of structural estimation of discrete choice dynamic programming (DCDP) models and (2) to survey the contributions of applications of these methods to substantive and policy issues in labor economics. The first part of the chapter describes solution and estimation methods for DCDP models using, for expository purposes, a prototypical female labor force participation model. The next part reviews the contribution of the DCDP approach to three leading areas in labor economics: labor supply, job search and human capital. The final section discusses approaches to validating DCDP models.},
	language = {en},
	urldate = {2021-09-06},
	booktitle = {Handbook of {Labor} {Economics}},
	publisher = {Elsevier},
	author = {Keane, Michael P. and Todd, Petra E. and Wolpin, Kenneth I.},
	editor = {Ashenfelter, Orley and Card, David},
	month = jan,
	year = {2011},
	doi = {10.1016/S0169-7218(11)00410-2},
	keywords = {Discrete choice, Dynamic programming, Human capital, Job search, Labor supply, Structural estimation},
	pages = {331--461},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/NV3GRPW5/S0169721811004102.html:text/html;Keane et al_2011_Chapter 4 - The Structural Estimation of Behavioral Models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Keane et al_2011_Chapter 4 - The Structural Estimation of Behavioral Models.pdf:application/pdf},
}

@article{adda2017CareerCostsChildren,
	title = {The {Career} {Costs} of {Children}},
	volume = {125},
	issn = {0022-3808},
	url = {https://www.journals.uchicago.edu/doi/abs/10.1086/690952},
	doi = {10.1086/690952},
	abstract = {We estimate a dynamic life cycle model of labor supply, fertility, and savings, incorporating occupational choices, with specific wage paths and skill atrophy that vary over the career. This allows us to understand the trade-off between occupational choice and desired fertility, as well as sorting both into the labor market and across occupations. We quantify the life cycle career costs associated with children, how they decompose into loss of skills during interruptions, lost earnings opportunities, and selection into more child-friendly occupations. We analyze the long-run effects of policies that encourage fertility and show that they are considerably smaller than short-run effects.},
	number = {2},
	urldate = {2021-09-06},
	journal = {Journal of Political Economy},
	author = {Adda, Jérôme and Dustmann, Christian and Stevens, Katrien},
	month = apr,
	year = {2017},
	note = {Publisher: The University of Chicago Press},
	pages = {293--337},
	file = {Adda et al_2017_The Career Costs of Children.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Adda et al_2017_The Career Costs of Children.pdf:application/pdf},
}

@article{blundell2016FemaleLaborSupply,
	title = {Female {Labor} {Supply}, {Human} {Capital}, and {Welfare} {Reform}},
	volume = {84},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA11576},
	doi = {10.3982/ECTA11576},
	abstract = {We estimate a dynamic model of employment, human capital accumulation—including education, and savings for women in the United Kingdom, exploiting tax and benefit reforms, and use it to analyze the effects of welfare policy. We find substantial elasticities for labor supply and particularly for lone mothers. Returns to experience, which are important in determining the longer-term effects of policy, increase with education, but experience mainly accumulates when in full-time employment. Tax credits are welfare improving in the U.K., increase lone-mother labor supply and marginally reduce educational attainment, but the employment effects do not extend beyond the period of eligibility. Marginal increases in tax credits improve welfare more than equally costly increases in income support or tax cuts.},
	language = {en},
	number = {5},
	urldate = {2021-09-06},
	journal = {Econometrica},
	author = {Blundell, Richard and Dias, Monica Costa and Meghir, Costas and Shaw, Jonathan},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA11576},
	keywords = {education, female labor supply, human capital, income tax, learning by doing, Life-cycle model, negative income tax, part time work, subsidies},
	pages = {1705--1753},
	file = {Snapshot:/Users/yulei/Zotero/storage/2NIX2V7F/ECTA11576.html:text/html;Blundell et al_2016_Female Labor Supply, Human Capital, and Welfare Reform.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Blundell et al_2016_Female Labor Supply, Human Capital, and Welfare Reform.pdf:application/pdf},
}

@article{eckstein2019CareerFamilyDecisions,
	title = {Career and {Family} {Decisions}: {Cohorts} {Born} 1935-1975},
	volume = {87},
	issn = {0012-9682},
	shorttitle = {Career and {Family} {Decisions}},
	url = {https://www.econometricsociety.org/doi/10.3982/ECTA14474},
	doi = {10.3982/ECTA14474},
	language = {en},
	number = {1},
	urldate = {2021-09-06},
	journal = {Econometrica},
	author = {Eckstein, Zvi and Keane, Michael and Lifshitz, Osnat},
	year = {2019},
	pages = {217--253},
	file = {Eckstein et al_2019_Career and Family Decisions.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Eckstein et al_2019_Career and Family Decisions.pdf:application/pdf},
}

@article{manski2021EconometricsDecisionMaking,
	title = {Econometrics {For} {Decision} {Making}: {Building} {Foundations} {Sketched} {By} {Haavelmo} {And} {Wald}},
	shorttitle = {Econometrics {For} {Decision} {Making}},
	url = {http://arxiv.org/abs/1912.08726},
	abstract = {Haavelmo (1944) proposed a probabilistic structure for econometric modeling, aiming to make econometrics useful for decision making. His fundamental contribution has become thoroughly embedded in subsequent econometric research, yet it could not answer all the deep issues that the author raised. Notably, Haavelmo struggled to formalize the implications for decision making of the fact that models can at most approximate actuality. In the same period, Wald (1939, 1945) initiated his own seminal development of statistical decision theory. Haavelmo favorably cited Wald, but econometrics did not embrace statistical decision theory. Instead, it focused on study of identification, estimation, and statistical inference. This paper proposes statistical decision theory as a framework for evaluation of the performance of models in decision making. I particularly consider the common practice of as-if optimization: specification of a model, point estimation of its parameters, and use of the point estimate to make a decision that would be optimal if the estimate were accurate. A central theme is that one should evaluate as-if optimization or any other model-based decision rule by its performance across the state space, listing all states of nature that one believes feasible, not across the model space. I apply the theme to prediction and treatment choice. Statistical decision theory is conceptually simple, but application is often challenging. Advancement of computation is the primary task to continue building the foundations sketched by Haavelmo and Wald.},
	urldate = {2021-09-06},
	journal = {arXiv:1912.08726 [econ]},
	author = {Manski, Charles F.},
	month = feb,
	year = {2021},
	note = {arXiv: 1912.08726},
	keywords = {⛔ No DOI found, Economics - Econometrics},
	file = {arXiv.org Snapshot:/Users/yulei/Zotero/storage/YSMRK5JZ/1912.html:text/html;Manski_2021_Econometrics For Decision Making.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Manski_2021_Econometrics For Decision Making.pdf:application/pdf},
}

@techreport{janosgabler2020RespyFrameworkSimulation,
	title = {respy - {A} {Framework} for the {Simulation} and {Estimation} of {Eckstein}-{Keane}-{Wolpin} {Models}},
	url = {https://github.com/OpenSourceEconomics/respy},
	author = {{Janos Gabler} and {Tobias Raabe}},
	year = {2020},
}

@misc{EngineeringDesignSurrogate,
	title = {Engineering {Design} via {Surrogate} {Modelling}: {A} {Practical} {Guide} {\textbar} {Wiley}},
	shorttitle = {Engineering {Design} via {Surrogate} {Modelling}},
	url = {https://www.wiley.com/en-ao/Engineering+Design+via+Surrogate+Modelling%3A+A+Practical+Guide-p-9780470060681},
	abstract = {Surrogate models expedite the search for promising designs by standing in for expensive design evaluations or simulations. They provide a global model of some metric of a design (such as weight, aerodynamic drag, cost, etc.), which can then be optimized efficiently. Engineering Design via Surrogate Modelling is a self-contained guide to surrogate models and their use in engineering design. The fundamentals of building, selecting, validating, searching and refining a surrogate are presented in a manner accessible to novices in the field. Figures are used liberally to explain the key concepts and clearly show the differences between the various techniques, as well as to emphasize the intuitive nature of the conceptual and mathematical reasoning behind them. More advanced and recent concepts are each presented in stand-alone chapters, allowing the reader to concentrate on material pertinent to their current design problem, and concepts are clearly demonstrated using simple design problems. This collection of advanced concepts (visualization, constraint handling, coping with noisy data, gradient-enhanced modelling, multi-fidelity analysis and multiple objectives) represents an invaluable reference manual for engineers and researchers active in the area. Engineering Design via Surrogate Modelling is complemented by a suite of Matlab codes, allowing the reader to apply all the techniques presented to their own design problems. By applying statistical modelling to engineering design, this book bridges the wide gap between the engineering and statistics communities. It will appeal to postgraduates and researchers across the academic engineering design community as well as practising design engineers. Provides an inclusive and practical guide to using surrogates in engineering design. Presents the fundamentals of building, selecting, validating, searching and refining a surrogate model. Guides the reader through the practical implementation of a surrogate-based design process using a set of case studies from real engineering design challenges. Accompanied by a companion website featuring Matlab software at http://www.wiley.com/go/forrester},
	language = {en-ao},
	urldate = {2021-09-06},
	journal = {Wiley.com},
	file = {Snapshot:/Users/yulei/Zotero/storage/7EPLYPH7/Engineering+Design+via+Surrogate+Modelling+A+Practical+Guide-p-9780470060681.html:text/html},
}

@article{leamer1985SensitivityAnalysesWould,
	title = {Sensitivity {Analyses} {Would} {Help}},
	volume = {75},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/1814801},
	number = {3},
	urldate = {2021-09-06},
	journal = {The American Economic Review},
	author = {Leamer, Edward E.},
	year = {1985},
	note = {Publisher: American Economic Association},
	keywords = {⛔ No DOI found},
	pages = {308--313},
}

@article{sudret2007UncertaintyPropagationSensitivity,
	title = {Uncertainty propagation and sensitivity analysis in mechanical models},
	language = {en},
	author = {Sudret, Bruno},
	year = {2007},
	keywords = {⛔ No DOI found},
	pages = {252},
	file = {Sudret_2007_Uncertainty propagation and sensitivity analysis in mechanical models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Sudret_2007_Uncertainty propagation and sensitivity analysis in mechanical models.pdf:application/pdf},
}

@article{sobol1993SensitivityEstimatesNonlinear,
	title = {Sensitivity {Estimates} for {Nonlinear} {Mathematical} {Models}},
	url = {https://www.semanticscholar.org/paper/Sensitivity-Estimates-for-Nonlinear-Mathematical-Sobol/3e0b415213a580254226fdbcbfc9980b70dd0468},
	abstract = {Semantic Scholar extracted view of \&quot;Sensitivity Estimates for Nonlinear Mathematical Models\&quot; by I. Sobol},
	language = {en},
	number = {4},
	urldate = {2021-09-06},
	journal = {Mathematical Modelling and Computational Experiments},
	author = {Sobol, I.},
	year = {1993},
	keywords = {⛔ No DOI found},
	pages = {407--414},
	file = {Snapshot:/Users/yulei/Zotero/storage/CS8NAECT/3e0b415213a580254226fdbcbfc9980b70dd0468.html:text/html;Sobol_1993_Sensitivity Estimates for Nonlinear Mathematical Models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Sobol_1993_Sensitivity Estimates for Nonlinear Mathematical Models.pdf:application/pdf},
}

@article{harrison1992SensitivityAnalysisApplied,
	title = {The {Sensitivity} {Analysis} of {Applied} {General} {Equilibrium} {Models}: {Completely} {Randomized} {Factorial} {Sampling} {Designs}},
	volume = {74},
	issn = {0034-6535},
	shorttitle = {The {Sensitivity} {Analysis} of {Applied} {General} {Equilibrium} {Models}},
	url = {https://www.jstor.org/stable/2109672},
	doi = {10.2307/2109672},
	abstract = {We propose a method for estimating the population mean of a distribution of solution values from applied general equilibrium models subject to parameter uncertainty. The method is illustrated by demonstrating that the "marginal excess burden" of the U.S. taxation system may be robustly bounded with a high confidence.},
	number = {2},
	urldate = {2021-09-06},
	journal = {The Review of Economics and Statistics},
	author = {Harrison, Glenn W. and Vinod, H. D.},
	year = {1992},
	note = {Publisher: The MIT Press},
	pages = {357--362},
	file = {Harrison_Vinod_1992_The Sensitivity Analysis of Applied General Equilibrium Models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Harrison_Vinod_1992_The Sensitivity Analysis of Applied General Equilibrium Models.pdf:application/pdf},
}

@article{anderson2014UncertaintyClimateChange,
	title = {Uncertainty in climate change modeling: can global sensitivity analysis be of help?},
	volume = {34},
	issn = {1539-6924},
	shorttitle = {Uncertainty in climate change modeling},
	doi = {10.1111/risa.12117},
	abstract = {Integrated assessment models offer a crucial support to decisionmakers in climate policy making. For a full understanding and corroboration of model results, analysts ought to identify the exogenous variables that influence the model results the most (key drivers), appraise the relevance of interactions, and the direction of change associated with the simultaneous variation of uncertain variables. We show that such information can be directly extracted from the data set produced by Monte Carlo simulations. Our discussion is guided by the application to the well-known DICE model of William Nordhaus. The proposed methodology allows analysts to draw robust insights into the dependence of future atmospheric temperature, global emissions, and carbon costs and taxes on the model's exogenous variables.},
	language = {eng},
	number = {2},
	journal = {Risk Analysis: An Official Publication of the Society for Risk Analysis},
	author = {Anderson, Barry and Borgonovo, Emanuele and Galeotti, Marzio and Roson, Roberto},
	month = feb,
	year = {2014},
	pmid = {24111855},
	keywords = {risk analysis, Climate change, Climate Change, Computer Simulation, global sensitivity analysis, integrated assessment modeling, Models, Theoretical, Monte Carlo Method, Risk Assessment, Uncertainty},
	pages = {271--293},
	file = {Anderson et al_2014_Uncertainty in climate change modeling.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Anderson et al_2014_Uncertainty in climate change modeling.pdf:application/pdf},
}

@article{saltelli2010VarianceBasedSensitivity,
	title = {Variance based sensitivity analysis of model output. {Design} and estimator for the total sensitivity index},
	volume = {181},
	issn = {0010-4655},
	url = {https://www.sciencedirect.com/science/article/pii/S0010465509003087},
	doi = {10.1016/j.cpc.2009.09.018},
	abstract = {Variance based methods have assessed themselves as versatile and effective among the various available techniques for sensitivity analysis of model output. Practitioners can in principle describe the sensitivity pattern of a model Y=f(X1,X2,…,Xk) with k uncertain input factors via a full decomposition of the variance V of Y into terms depending on the factors and their interactions. More often practitioners are satisfied with computing just k first order effects and k total effects, the latter describing synthetically interactions among input factors. In sensitivity analysis a key concern is the computational cost of the analysis, defined in terms of number of evaluations of f(X1,X2,…,Xk) needed to complete the analysis, as f(X1,X2,…,Xk) is often in the form of a numerical model which may take long processing time. While the computational cost is relatively cheap and weakly dependent on k for estimating first order effects, it remains expensive and strictly k-dependent for total effect indices. In the present note we compare existing and new practices for this index and offer recommendations on which to use.},
	language = {en},
	number = {2},
	urldate = {2021-09-06},
	journal = {Computer Physics Communications},
	author = {Saltelli, Andrea and Annoni, Paola and Azzini, Ivano and Campolongo, Francesca and Ratto, Marco and Tarantola, Stefano},
	month = feb,
	year = {2010},
	pages = {259--270},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/ZDFH28ZJ/S0010465509003087.html:text/html;Saltelli et al_2010_Variance based sensitivity analysis of model output.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Saltelli et al_2010_Variance based sensitivity analysis of model output.pdf:application/pdf},
}

@article{ratto2008AnalysingDSGEModels,
	title = {Analysing {DSGE} {Models} with {Global} {Sensitivity} {Analysis}},
	volume = {31},
	issn = {1572-9974},
	url = {https://doi.org/10.1007/s10614-007-9110-6},
	doi = {10.1007/s10614-007-9110-6},
	abstract = {We present computational tools to analyse some key properties of DSGE models and address the following questions: (i) Which is the domain of structural coefficients assuring the stability and determinacy of a DSGE model? (ii) Which parameters mostly drive the fit of, e.g., GDP and which the fit of inflation? Is there any conflict between the optimal fit of one observed series versus another one? (iii) How to represent in a direct, albeit approximated, form the relationship between structural parameters and the reduced form of a rational expectations model? Global sensitivity analysis (GSA) techniques are used to answer these questions. We will discuss two classes of methods: Monte Carlo filtering (MCF) techniques and functional/variance decomposition techniques. These tools can make the model properties more transparent; helping the analyst to identify critical elements in the specification and, if necessary, guiding her to revise the model; supporting calibration and estimation procedures and interpreting estimation results. Applications to small DSGE models will complete the description of the methodologies.},
	language = {en},
	number = {2},
	urldate = {2021-09-06},
	journal = {Computational Economics},
	author = {Ratto, Marco},
	month = mar,
	year = {2008},
	pages = {115--139},
	file = {Ratto_2008_Analysing DSGE Models with Global Sensitivity Analysis.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Ratto_2008_Analysing DSGE Models with Global Sensitivity Analysis.pdf:application/pdf},
}

@techreport{scheidegger2017MachineLearningHighDimensional,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Machine {Learning} for {High}-{Dimensional} {Dynamic} {Stochastic} {Economies}},
	url = {https://papers.ssrn.com/abstract=2927400},
	abstract = {We present the first computational framework that can compute global solutions to very-high-dimensional dynamic stochastic economic models on arbitrary state space geometries.},
	language = {en},
	number = {ID 2927400},
	urldate = {2021-09-06},
	institution = {Social Science Research Network},
	author = {Scheidegger, Simon and Bilionis, Ilias},
	month = jun,
	year = {2017},
	doi = {10.2139/ssrn.2927400},
	keywords = {Active Subspaces, Dynamic Programming, Gaussian Process, High-Performance Computing, Machine Learning, Neoclassical Growth},
	file = {Snapshot:/Users/yulei/Zotero/storage/V7WNKSLI/papers.html:text/html;Scheidegger_Bilionis_2017_Machine Learning for High-Dimensional Dynamic Stochastic Economies.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Scheidegger_Bilionis_2017_Machine Learning for High-Dimensional Dynamic Stochastic Economies.pdf:application/pdf},
}

@book{saltelli2008SensitivityAnalysis,
	address = {Chichester Weinheim},
	edition = {Paperback ed},
	series = {Wiley paperback series},
	title = {Sensitivity analysis},
	isbn = {978-0-470-74382-9},
	language = {eng},
	publisher = {Wiley},
	editor = {Saltelli, Andrea},
	year = {2008},
	file = {Saltelli_2008_Sensitivity analysis.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Saltelli_2008_Sensitivity analysis.pdf:application/pdf},
}

@book{hansen2008Robustness,
	edition = {STU - Student edition},
	title = {Robustness},
	url = {https://www.jstor.org/stable/j.ctt1dr35gx},
	abstract = {The standard theory of decision making under uncertainty advises the decision maker to form a statistical model linking outcomes to decisions and then to choose the optimal distribution of outcomes. This assumes that the decision maker trusts the model completely. But what should a decision maker do if the model cannot be trusted?  Lars Hansen and Thomas Sargent, two leading macroeconomists, push the field forward as they set about answering this question. They adapt robust control techniques and apply them to economics. By using this theory to let decision makers acknowledge misspecification in economic modeling, the authors develop applications to a variety of problems in dynamic macroeconomics.  Technical, rigorous, and self-contained, this book will be useful for macroeconomists who seek to improve the robustness of decision-making processes.},
	urldate = {2021-09-07},
	publisher = {Princeton University Press},
	author = {Hansen, Lars Peter and Sargent, Thomas J.},
	year = {2008},
}

@article{brock2003PolicyEvaluationUncertain,
	title = {Policy {Evaluation} in {Uncertain} {Economic} {Environments}},
	volume = {2003},
	issn = {0007-2303},
	url = {https://www.jstor.org/stable/1209150},
	doi = {10.1353/eca.2003.0013},
	number = {1},
	urldate = {2021-09-07},
	journal = {Brookings Papers on Economic Activity},
	author = {Brock, William A. and Durlauf, Steven N. and West, Kenneth D.},
	year = {2003},
	note = {Publisher: Brookings Institution Press},
	pages = {235--301},
}

@article{gupta2018RevisitingBasisSensitivity,
	title = {Revisiting the {Basis} of {Sensitivity} {Analysis} for {Dynamical} {Earth} {System} {Models}},
	volume = {54},
	issn = {1944-7973},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2018WR022668},
	doi = {10.1029/2018WR022668},
	abstract = {This paper investigates the problem of global sensitivity analysis (GSA) of Dynamical Earth System Models and proposes a basis for how such analyses should be performed. We argue that (a) performance metric-based approaches to parameter GSA are actually identifiability analyses, (b) the use of a performance metric to assess sensitivity unavoidably distorts the information provided by the model about relative parameter importance, and (c) it is a serious conceptual flaw to interpret the results of such an analysis as being consistent and accurate indications of the sensitivity of the model response to parameter perturbations. Further, because such approaches depend on availability of system state/output observational data, the analysis they provide is necessarily incomplete. Here we frame the GSA problem from first principles, using trajectories of the partial derivatives of model outputs with respect to controlling factors as the theoretical basis for sensitivity, and construct a global sensitivity matrix from which statistical indices of total period time-aggregate parameter importance, and time series of time-varying parameter importance, can be inferred. We demonstrate this framework using the HBV-SASK conceptual hydrologic model applied to the Oldman basin in Canada and show that it disagrees with performance metric-based methods regarding which parameters exert the strongest controls on model behavior. Further, it is highly efficient, requiring less than 1,000 base samples to obtain stable and robust parameter importance assessments for our 10-parameter example.},
	language = {en},
	number = {11},
	urldate = {2021-09-07},
	journal = {Water Resources Research},
	author = {Gupta, Hoshin V. and Razavi, Saman},
	year = {2018},
	note = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2018WR022668},
	keywords = {global sensitivity analysis, dynamical systems, efficiency and robustness, global sensitivity matrix, Parameter importance analysis, time-varying sensitivity},
	pages = {8692--8717},
	file = {Snapshot:/Users/yulei/Zotero/storage/2K7T7NHE/2018WR022668.html:text/html;Gupta_Razavi_2018_Revisiting the Basis of Sensitivity Analysis for Dynamical Earth System Models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Gupta_Razavi_2018_Revisiting the Basis of Sensitivity Analysis for Dynamical Earth System Models.pdf:application/pdf},
}

@article{sobol2007EstimatingApproximationError,
	title = {Estimating the approximation error when fixing unessential factors in global sensitivity analysis},
	volume = {92},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832006001499},
	doi = {10.1016/j.ress.2006.07.001},
	abstract = {One of the major settings of global sensitivity analysis is that of fixing non-influential factors, in order to reduce the dimensionality of a model. However, this is often done without knowing the magnitude of the approximation error being produced. This paper presents a new theorem for the estimation of the average approximation error generated when fixing a group of non-influential factors. A simple function where analytical solutions are available is used to illustrate the theorem. The numerical estimation of small sensitivity indices is discussed.},
	language = {en},
	number = {7},
	urldate = {2021-09-07},
	journal = {Reliability Engineering \& System Safety},
	author = {Sobol’, I. M. and Tarantola, S. and Gatelli, D. and Kucherenko, S. S. and Mauntz, W.},
	month = jul,
	year = {2007},
	keywords = {Approximation error, Sensitivity analysis by groups, Sensitivity indices, Sobol’ method},
	pages = {957--960},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/7DKEMVHW/S0951832006001499.html:text/html;Sobol’ et al_2007_Estimating the approximation error when fixing unessential factors in global.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Sobol’ et al_2007_Estimating the approximation error when fixing unessential factors in global.pdf:application/pdf},
}

@article{guillaume2019IntroductoryOverviewIdentifiability,
	title = {Introductory overview of identifiability analysis: {A} guide to evaluating whether you have the right type of data for your modeling purpose},
	volume = {119},
	issn = {1364-8152},
	shorttitle = {Introductory overview of identifiability analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815218307278},
	doi = {10.1016/j.envsoft.2019.07.007},
	abstract = {Identifiability is a fundamental concept in parameter estimation, and therefore key to the large majority of environmental modeling applications. Parameter identifiability analysis assesses whether it is theoretically possible to estimate unique parameter values from data, given the quantities measured, conditions present in the forcing data, model structure (and objective function), and properties of errors in the model and observations. In other words, it tackles the problem of whether the right type of data is available to estimate the desired parameter values. Identifiability analysis is therefore an essential technique that should be adopted more routinely in practice, alongside complementary methods such as uncertainty analysis and evaluation of model performance. This article provides an introductory overview to the topic. We recommend that any modeling study should document whether a model is non-identifiable, the source of potential non-identifiability, and how this affects intended project outcomes.},
	language = {en},
	urldate = {2021-09-07},
	journal = {Environmental Modelling \& Software},
	author = {Guillaume, Joseph H. A. and Jakeman, John D. and Marsili-Libelli, Stefano and Asher, Michael and Brunner, Philip and Croke, Barry and Hill, Mary C. and Jakeman, Anthony J. and Keesman, Karel J. and Razavi, Saman and Stigter, Johannes D.},
	month = sep,
	year = {2019},
	keywords = {Uncertainty, Derivative based methods, Emulation, Hessian, Identifiability, Non-uniqueness, Response surface},
	pages = {418--432},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/KPPDDZ7G/S1364815218307278.html:text/html;Guillaume et al_2019_Introductory overview of identifiability analysis.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Guillaume et al_2019_Introductory overview of identifiability analysis.pdf:application/pdf},
}

@article{tarantola2002CanGlobalSensitivity,
	title = {Can global sensitivity analysis steer the implementation of models for environmental assessments and decision-making?},
	volume = {16},
	issn = {1436-3259},
	url = {https://doi.org/10.1007/s00477-001-0085-x},
	doi = {10.1007/s00477-001-0085-x},
	abstract = {We illustrate a method of global sensitivity analysis and we test it on a preliminary case study in the field of environmental assessment to quantify uncertainty importance in poorly-known model parameters and spatially referenced input data. The focus of the paper is to show how the methodology provides guidance to improve the quality of environmental assessment practices and decision support systems employed in environmental policy. Global sensitivity analysis, coupled with uncertainty analysis, is a tool to assess the robustness of decisions, to understand whether the current state of knowledge on input data and parametric uncertainties is sufficient to enable a decision to be taken. The methodology is applied to a preliminary case study, which is based on a numerical model that employs GIS-based soil data and expert consultation to evaluate an index that joins environmental and economic aspects of land depletion. The index is used as a yardstick by decision-makers involved in the planning of highways to identify the route that minimises the overall impact.},
	language = {en},
	number = {1},
	urldate = {2021-09-07},
	journal = {Stochastic Environmental Research and Risk Assessment},
	author = {Tarantola, S. and Giglioli, N. and Jesinghaus, J. and Saltelli, A.},
	month = feb,
	year = {2002},
	pages = {63--76},
	file = {Tarantola et al_2002_Can global sensitivity analysis steer the implementation of models for.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Tarantola et al_2002_Can global sensitivity analysis steer the implementation of models for.pdf:application/pdf},
}

@misc{BetterRegulationToolbox,
	type = {Text},
	title = {Better regulation toolbox},
	url = {https://ec.europa.eu/info/law/law-making-process/planning-and-proposing-law/better-regulation-why-and-how/better-regulation-guidelines-and-toolbox/better-regulation-toolbox_en},
	abstract = {Better regulation toolbox},
	language = {en},
	urldate = {2021-09-07},
	journal = {European Commission - European Commission},
	file = {Snapshot:/Users/yulei/Zotero/storage/L94CX48Q/better-regulation-toolbox_en.html:text/html},
}

@article{box1986AnalysisUnreplicatedFractional,
	title = {An {Analysis} for {Unreplicated} {Fractional} {Factorials}},
	volume = {28},
	issn = {0040-1706},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1986.10488093},
	doi = {10.1080/00401706.1986.10488093},
	abstract = {Loss of markets to Japan has recently caused attention to return to the enormous potential that experimental design possesses for the improvement of product design, for the improvement of the manufacturing process, and hence for improvement of overall product quality. In the screening stage of industrial experimentation it is frequently true that the “Pareto Principle” applies; that is, a large proportion of process variation is associated with a small proportion of the process variables. In such circumstances of “factor sparsity,” unreplicated fractional designs and other orthogonal arrays have frequently been effective when used as a screen for isolating preponderant factors. A useful graphical analysis due to Daniel (1959) employs normal probability plotting. A more formal analysis is presented here, which may be used to supplement such plots and hence to facilitate the use of these unreplicated experimental arrangements.},
	number = {1},
	urldate = {2021-09-07},
	journal = {Technometrics},
	author = {Box, George   E.P. and Meyer, R.   Daniel},
	month = feb,
	year = {1986},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1986.10488093},
	pages = {11--18},
	file = {Snapshot:/Users/yulei/Zotero/storage/32HD9HQJ/00401706.1986.html:text/html;Box_Meyer_1986_An Analysis for Unreplicated Fractional Factorials.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Box_Meyer_1986_An Analysis for Unreplicated Fractional Factorials.pdf:application/pdf},
}

@article{fisher1936DesignExperiments,
	title = {Design of {Experiments}},
	volume = {1},
	issn = {0007-1447, 1468-5833},
	url = {https://www.bmj.com/content/1/3923/554.2},
	doi = {10.1136/bmj.1.3923.554-a},
	language = {en},
	number = {3923},
	urldate = {2021-09-07},
	journal = {Br Med J},
	author = {Fisher, R. A.},
	month = mar,
	year = {1936},
	note = {Publisher: British Medical Journal Publishing Group
Section: Correspondence},
	pages = {554--554},
	file = {Snapshot:/Users/yulei/Zotero/storage/RNXEL7XT/554.2.html:text/html;Fisher_1936_Design of Experiments.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Fisher_1936_Design of Experiments.pdf:application/pdf},
}

@article{saltelli2010HowAvoidPerfunctory,
	title = {How to avoid a perfunctory sensitivity analysis},
	volume = {25},
	issn = {1364-8152},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815210001180},
	doi = {10.1016/j.envsoft.2010.04.012},
	abstract = {Mathematical modelers from different disciplines and regulatory agencies worldwide agree on the importance of a careful sensitivity analysis (SA) of model-based inference. The most popular SA practice seen in the literature is that of ’one-factor-at-a-time’ (OAT). This consists of analyzing the effect of varying one model input factor at a time while keeping all other fixed. While the shortcomings of OAT are known from the statistical literature, its widespread use among modelers raises concern on the quality of the associated sensitivity analyses. The present paper introduces a novel geometric proof of the inefficiency of OAT, with the purpose of providing the modeling community with a convincing and possibly definitive argument against OAT. Alternatives to OAT are indicated which are based on statistical theory, drawing from experimental design, regression analysis and sensitivity analysis proper.},
	language = {en},
	number = {12},
	urldate = {2021-09-07},
	journal = {Environmental Modelling \& Software},
	author = {Saltelli, Andrea and Annoni, Paola},
	month = dec,
	year = {2010},
	keywords = {Mathematical modeling, Sensitivity analysis, Uncertainty analysis, One-at-a-time, Robustness},
	pages = {1508--1517},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/KZZTNTNR/S1364815210001180.html:text/html;Saltelli_Annoni_2010_How to avoid a perfunctory sensitivity analysis.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Saltelli_Annoni_2010_How to avoid a perfunctory sensitivity analysis.pdf:application/pdf},
}

@article{campolongo2000SensitivityAnaysisIngredient,
	title = {Sensitivity {Anaysis} as an {Ingredient} of {Modeling}},
	volume = {15},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-15/issue-4/Sensitivity-Anaysis-as-an-Ingredient-of-Modeling/10.1214/ss/1009213004.full},
	doi = {10.1214/ss/1009213004},
	abstract = {We explore the tasks where sensitivity analysis (SA) can be useful and try to assess the relevance of SA within the modeling process. We suggest that SA could considerably assist in the use of models, by providing objective criteria of judgement for different phases of the model­building process: model identification and discrimination; model calibration; model corroboration. We review some new global quantitative SA methods and suggest that these might enlarge the scope for sensitivity analysis in computational and statistical modeling practice. Among the advantages of the new methods are their robustness, model independence and computational convenience. The discussion is based on worked examples.},
	number = {4},
	urldate = {2021-09-07},
	journal = {Statistical Science},
	author = {Campolongo, F. and Saltelli, A. and Tarantola, S.},
	month = nov,
	year = {2000},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Global sensitivity analysis, model transparency, numerical experiments, predictive uncertainty, quantitative sensitivity measure, reliability and dependability of models, Screening},
	pages = {377--395},
	file = {Snapshot:/Users/yulei/Zotero/storage/K2AXXWHJ/1009213004.html:text/html;Campolongo et al_2000_Sensitivity Anaysis as an Ingredient of Modeling.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Campolongo et al_2000_Sensitivity Anaysis as an Ingredient of Modeling.pdf:application/pdf},
}

@article{homma1996ImportanceMeasuresGlobal,
	title = {Importance measures in global sensitivity analysis of nonlinear models},
	volume = {52},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/0951832096000026},
	doi = {10.1016/0951-8320(96)00002-6},
	abstract = {The present paper deals with a new method of global sensitivity analysis of nonlinear models. This is based on a measure of importance to calculate the fractional contribution of the input parameters to the variance of the model prediction. Measures of importance in sensitivity analysis have been suggested by several authors, whose work is reviewed in this article. More emphasis is given to the developments of sensitivity indices by the Russian mathematician I.M. Sobol'. Given that Sobol' treatment of the measure of importance is the most general, his formalism is employed throughout this paper where conceptual and computational improvements of the method are presented. The computational novelty of this study is the introduction of the ‘total effect’ parameter index. This index provides a measure of the total effect of a given parameter, including all the possible synergetic terms between that parameter and all the others. Rank transformation of the data is also introduced in order to increase the reproducibility of the method. These methods are tested on a few analytical and computer models. The main conclusion of this work is the identification of a sensitivity analysis methodology which is both flexible, accurate and informative, and which can be achieved at reasonable computational cost.},
	language = {en},
	number = {1},
	urldate = {2021-09-07},
	journal = {Reliability Engineering \& System Safety},
	author = {Homma, Toshimitsu and Saltelli, Andrea},
	month = apr,
	year = {1996},
	pages = {1--17},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/DPAT923N/0951832096000026.html:text/html;Homma_Saltelli_1996_Importance measures in global sensitivity analysis of nonlinear models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Homma_Saltelli_1996_Importance measures in global sensitivity analysis of nonlinear models.pdf:application/pdf},
}

@article{kleijnen1995SensitivityAnalysisOptimization,
	title = {Sensitivity analysis and optimization of system dynamics models: {Regression} analysis and statistical design of experiments},
	volume = {11},
	issn = {1099-1727},
	shorttitle = {Sensitivity analysis and optimization of system dynamics models},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sdr.4260110403},
	doi = {10.1002/sdr.4260110403},
	abstract = {This paper discusses what-if analysis and optimization of system dynamics models. These problems are solved, using the statistical techniques of regression analysis and Design of Experiments (DOE). These issues are illustrated by applying the statistical techniques to a system dynamics model for coal transportation, taken from Wolstenholme's book System Enquiry: a System Dynamics Approach (1990). The regression analysis uses the least-squares algorithm. DOE uses classic designs, namely, factorials and central composite designs. Compared with intuitive approaches, DOE is more efficient: DOE gives more accurate estimators of input effects. Moreover DOE is more effective: interactions among inputs are estimable too. The system dynamics model is also optimized, using a heuristic that is inspired by Response Surface Methodology (RSM) but that also accounts for constraints. Conclusions are presented for the case study, and general principles are derived. References are given for further study.},
	language = {en},
	number = {4},
	urldate = {2021-09-07},
	journal = {System Dynamics Review},
	author = {Kleijnen, Jack P. C.},
	year = {1995},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sdr.4260110403},
	pages = {275--288},
	file = {Snapshot:/Users/yulei/Zotero/storage/FRF4WUP7/sdr.html:text/html;Kleijnen_1995_Sensitivity analysis and optimization of system dynamics models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kleijnen_1995_Sensitivity analysis and optimization of system dynamics models.pdf:application/pdf},
}

@article{razavi2015WhatWeMean,
	title = {What do we mean by sensitivity analysis? {The} need for comprehensive characterization of “global” sensitivity in {Earth} and {Environmental} systems models},
	volume = {51},
	issn = {1944-7973},
	shorttitle = {What do we mean by sensitivity analysis?},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2014WR016527},
	doi = {10.1002/2014WR016527},
	abstract = {Sensitivity analysis is an essential paradigm in Earth and Environmental Systems modeling. However, the term “sensitivity” has a clear definition, based in partial derivatives, only when specified locally around a particular point (e.g., optimal solution) in the problem space. Accordingly, no unique definition exists for “global sensitivity” across the problem space, when considering one or more model responses to different factors such as model parameters or forcings. A variety of approaches have been proposed for global sensitivity analysis, based on different philosophies and theories, and each of these formally characterizes a different “intuitive” understanding of sensitivity. These approaches focus on different properties of the model response at a fundamental level and may therefore lead to different (even conflicting) conclusions about the underlying sensitivities. Here we revisit the theoretical basis for sensitivity analysis, summarize and critically evaluate existing approaches in the literature, and demonstrate their flaws and shortcomings through conceptual examples. We also demonstrate the difficulty involved in interpreting “global” interaction effects, which may undermine the value of existing interpretive approaches. With this background, we identify several important properties of response surfaces that are associated with the understanding and interpretation of sensitivities in the context of Earth and Environmental System models. Finally, we highlight the need for a new, comprehensive framework for sensitivity analysis that effectively characterizes all of the important sensitivity-related properties of model response surfaces.},
	language = {en},
	number = {5},
	urldate = {2021-09-07},
	journal = {Water Resources Research},
	author = {Razavi, Saman and Gupta, Hoshin V.},
	year = {2015},
	note = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/2014WR016527},
	keywords = {sensitivity analysis, Earth and Environmental system models, interaction effect, Morris, response surface, Sobol},
	pages = {3070--3092},
	file = {Snapshot:/Users/yulei/Zotero/storage/3MUBCG7M/2014WR016527.html:text/html;Razavi_Gupta_2015_What do we mean by sensitivity analysis.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Razavi_Gupta_2015_What do we mean by sensitivity analysis.pdf:application/pdf},
}

@article{do2020CorrelationEffectsMajor,
	title = {Correlation {Effects}? {A} {Major} but {Often} {Neglected} {Component} in {Sensitivity} and {Uncertainty} {Analysis}},
	volume = {56},
	issn = {1944-7973},
	shorttitle = {Correlation {Effects}?},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019WR025436},
	doi = {10.1029/2019WR025436},
	abstract = {Global sensitivity analysis (GSA) provides essential insights into the behavior of Earth and environmental systems models and identifies dominant controls of output uncertainty. Previous work on GSA, however, has typically been under the assumption that the controlling factors such as model inputs and parameters are independent, whereas, in many cases, they are correlated and their joint distribution follows a variety of forms. Although this assumption can limit the credibility of GSA and its results, very few studies in the field of water and environmental modeling address this issue. In this paper, we first discuss the significance of correlation effects in GSA and then propose a new GSA framework for properly accounting for correlations in input/parameter spaces. To this end, we extend the “variogram-based” theory of GSA, called variogram analysis of response surfaces (VARS), and develop a new generalized star sampling technique (called gSTAR) to accommodate correlated multivariate distributions. We test the new gSTAR-VARS method on two test functions, against a state-of-the-art GSA method that handles correlation effects. We then apply gSTAR-VARS to the HBV-SASK model, calibrated via a Bayesian, Markov chain Monte Carlo approach, for design flood estimation in the Oldman River Basin in Canada. Results demonstrate that accounting for correlation effects can be critically important in GSA, especially in the presence of nonlinearity and interaction effects in the underlying response surfaces. The proposed method can efficiently handle correlations and different distribution types and simultaneously generate a range of sensitivity indices, such as total-variogram effects, variance-based total-order effects, and derivative-based elementary effects.},
	language = {en},
	number = {3},
	urldate = {2021-09-07},
	journal = {Water Resources Research},
	author = {Do, Nhu Cuong and Razavi, Saman},
	year = {2020},
	note = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019WR025436},
	keywords = {Climate change, Global sensitivity analysis, Uncertainty, Correlation, Variogram analysis of response surfaces (VARS)},
	pages = {e2019WR025436},
	file = {Snapshot:/Users/yulei/Zotero/storage/RW5XTAZK/2019WR025436.html:text/html;Do_Razavi_2020_Correlation Effects.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Do_Razavi_2020_Correlation Effects.pdf:application/pdf},
}

@article{chastaing2012GeneralizedHoeffdingSobolDecomposition,
	title = {Generalized {Hoeffding}-{Sobol} decomposition for dependent variables - application to sensitivity analysis},
	volume = {6},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-6/issue-none/Generalized-Hoeffding-Sobol-decomposition-for-dependent-variables---application/10.1214/12-EJS749.full},
	doi = {10.1214/12-EJS749},
	abstract = {In this paper, we consider a regression model built on dependent variables. This regression modelizes an input output relationship. Under boundedness type assumptions on the joint density function of the input variables, we show that a generalized Hoeffding-Sobol decomposition is available. This leads to new indices measuring the sensitivity of the output with respect to the input variables. We also study and discuss the estimation of these new indices.},
	number = {none},
	urldate = {2021-09-07},
	journal = {Electronic Journal of Statistics},
	author = {Chastaing, Gaelle and Gamboa, Fabrice and Prieur, Clémentine},
	month = jan,
	year = {2012},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {dependent variables, Hoeffding decomposition, Sensitivity index, Sobol decomposition},
	pages = {2420--2448},
	file = {Snapshot:/Users/yulei/Zotero/storage/FNGPHPFB/12-EJS749.html:text/html;Chastaing et al_2012_Generalized Hoeffding-Sobol decomposition for dependent variables - application.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Chastaing et al_2012_Generalized Hoeffding-Sobol decomposition for dependent variables - application.pdf:application/pdf},
}

@article{xu2008UncertaintySensitivityAnalysis,
	title = {Uncertainty and sensitivity analysis for models with correlated parameters},
	volume = {93},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832007001652},
	doi = {10.1016/j.ress.2007.06.003},
	abstract = {When conducting sensitivity and uncertainty analysis, most of the global sensitivity techniques assume parameter independence. However, it is common that the parameters are correlated with each other. For models with correlated inputs, we propose that the contribution of uncertainty to model output by an individual parameter be divided into two parts: the correlated contribution (by the correlated variations, i.e. variations of a parameter which are correlated with other parameters) and the uncorrelated contribution (by the uncorrelated variations, i.e. the unique variations of a parameter which cannot be explained by any other parameters). So far, only a few studies have been conducted to obtain the sensitivity index for a model with correlated input. But these studies do not distinguish between the correlated and uncorrelated contribution of a parameter. In this study, we propose a regression-based method to quantitatively decompose the total uncertainty in model output into partial variances contributed by the correlated variations and partial variances contributed by the uncorrelated variations. The proposed regression-based method is then applied in three test cases. Results show that the regression-based method can successfully measure the uncertainty contribution in the case where the relationship between response and parameters is approximately linear.},
	language = {en},
	number = {10},
	urldate = {2021-09-07},
	journal = {Reliability Engineering \& System Safety},
	author = {Xu, Chonggang and Gertner, George Zdzislaw},
	month = oct,
	year = {2008},
	keywords = {Sensitivity analysis, Uncertainty analysis, Correlated parameters, Latin hypercube sampling, Linear regression},
	pages = {1563--1573},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/UEYA2V89/S0951832007001652.html:text/html;Xu_Gertner_2008_Uncertainty and sensitivity analysis for models with correlated parameters.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Xu_Gertner_2008_Uncertainty and sensitivity analysis for models with correlated parameters.pdf:application/pdf},
}

@misc{sheikholeslami2020VISCOUSVarianceBasedSensitivity,
	type = {preprint},
	title = {{VISCOUS}: {A} {Variance}-{Based} {Sensitivity} {Analysis} {Using} {Copulas} for {Efficient} {Identification} of {Dominant} {Hydrological} {Processes}},
	copyright = {Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)},
	shorttitle = {{VISCOUS}},
	url = {http://www.essoar.org/doi/10.1002/essoar.10505333.1},
	abstract = {Global Sensitivity Analysis (GSA) has long been recognized as an indispensable tool for model analysis. GSA has been extensively used for model simplification, identifiability analysis, and diagnosti},
	language = {EN},
	urldate = {2021-09-07},
	journal = {Earth and Space Science Open Archive},
	author = {Sheikholeslami, Razi and Gharari, Shervan and Papalexiou, Simon M. and Clark, Martyn P.},
	month = dec,
	year = {2020},
	doi = {10.1002/essoar.10505333.1},
	note = {Archive Location: world
Publisher: Earth and Space Science Open Archive
Section: Hydrology},
	file = {Snapshot:/Users/yulei/Zotero/storage/PIFPASQG/essoar.10505333.html:text/html},
}

@article{owen2014SobolIndicesShapley,
	title = {Sobol' {Indices} and {Shapley} {Value}},
	volume = {2},
	url = {https://epubs.siam.org/doi/10.1137/130936233},
	doi = {10.1137/130936233},
	abstract = {Global sensitivity analysis measures the importance of some input variables to a function \$f\$ by looking at the impact on \$f\$ of making large random perturbations to subsets of those variables. Using measures like those of Sobol' we can attribute importance to input variables based on the extent to which they help predict the target function \$f\$. There is a longstanding literature in economics and game theory that considers how to attribute the value of a team effort to individual members of that team. The primary result, known as the Shapley value, is the unique method satisfying some intuitively necessary criteria. In this paper we find the Shapley value of individual variables when we take “variance explained” as their combined value. The result does not match either of the usual Sobol' indices. It is instead bracketed between them for variance explained or indeed any totally monotone game. Because those indices are comparatively easy to compute, Sobol' indices provide effectively computable bounds for the Shapley value.},
	number = {1},
	urldate = {2021-09-07},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Owen, Art B.},
	month = jan,
	year = {2014},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {97M50, 91B82, functional ANOVA, global sensitivity, value attribution},
	pages = {245--251},
	file = {Owen_2014_Sobol' Indices and Shapley Value.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Owen_2014_Sobol' Indices and Shapley Value.pdf:application/pdf},
}

@article{iooss2019SHAPLEYEFFECTSSENSITIVITY,
	title = {{SHAPLEY} {EFFECTS} {FOR} {SENSITIVITY} {ANALYSIS} {WITH} {CORRELATED} {INPUTS}: {COMPARISONS} {WITH} {SOBOL}' {INDICES}, {NUMERICAL} {ESTIMATION} {AND} {APPLICATIONS}},
	volume = {9},
	issn = {2152-5080, 2152-5099},
	shorttitle = {{SHAPLEY} {EFFECTS} {FOR} {SENSITIVITY} {ANALYSIS} {WITH} {CORRELATED} {INPUTS}},
	url = {https://www.dl.begellhouse.com/journals/52034eb04b657aea,23ab8f375b210514,706e4963504bc249.html},
	doi = {10.1615/Int.J.UncertaintyQuantification.2019028372},
	language = {English},
	number = {5},
	urldate = {2021-09-07},
	journal = {International Journal for Uncertainty Quantification},
	author = {Iooss, Bertrand and Prieur, Clementine},
	year = {2019},
	note = {Publisher: Begel House Inc.},
	file = {Snapshot:/Users/yulei/Zotero/storage/58S4JJWM/52034eb04b657aea,23ab8f375b210514,706e4963504bc249.html:text/html;Iooss_Prieur_2019_SHAPLEY EFFECTS FOR SENSITIVITY ANALYSIS WITH CORRELATED INPUTS.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Iooss_Prieur_2019_SHAPLEY EFFECTS FOR SENSITIVITY ANALYSIS WITH CORRELATED INPUTS.pdf:application/pdf},
}

@article{prieur2019GlobalSensitivityAnalysis,
	title = {A global sensitivity analysis approach for marine biogeochemical modeling},
	volume = {139},
	issn = {1463-5003},
	url = {https://www.sciencedirect.com/science/article/pii/S1463500318303688},
	doi = {10.1016/j.ocemod.2019.101402},
	abstract = {This paper introduces the Sobol’ indices approach for global sensitivity analysis (SA), in the context of marine biogeochemistry. Such an approach is particularly well suited for ocean biogeochemical models, which make use of numerous parameters within large sets of differential equations with complex dependencies. This SA allows for a detailed study of the relative influence of a large number of input parameters on output quantities of interest to be chosen. It is able to distinguish between direct effects of these parameters and effects due to interaction between two or more parameters. Although demanding in terms of computation, such a tool is now becoming affordable, thanks to the development of distributed computing environments. An applicative example is presented with the MODECOGeL biogeochemical model, and illustrates the advantages of this approach over standard local SA.},
	language = {en},
	urldate = {2021-09-07},
	journal = {Ocean Modelling},
	author = {Prieur, C. and Viry, L. and Blayo, E. and Brankart, J. -M},
	month = jul,
	year = {2019},
	keywords = {Sensitivity analysis, Marine biogeochemistry, Sobol’ indices},
	pages = {101402},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/4R7VXK9I/S1463500318303688.html:text/html;Prieur et al_2019_A global sensitivity analysis approach for marine biogeochemical modeling.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Prieur et al_2019_A global sensitivity analysis approach for marine biogeochemical modeling.pdf:application/pdf},
}

@article{efron1987BetterBootstrapConfidence,
	title = {Better {Bootstrap} {Confidence} {Intervals}},
	volume = {82},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478410},
	doi = {10.1080/01621459.1987.10478410},
	abstract = {We consider the problem of setting approximate confidence intervals for a single parameter θ in a multiparameter family. The standard approximate intervals based on maximum likelihood theory, , can be quite misleading. In practice, tricks based on transformations, bias corrections, and so forth, are often used to improve their accuracy. The bootstrap confidence intervals discussed in this article automatically incorporate such tricks without requiring the statistician to think them through for each new application, at the price of a considerable increase in computational effort. The new intervals incorporate an improvement over previously suggested methods, which results in second-order correctness in a wide variety of problems. In addition to parametric families, bootstrap intervals are also developed for nonparametric situations.},
	number = {397},
	urldate = {2021-09-07},
	journal = {Journal of the American Statistical Association},
	author = {Efron, Bradley},
	month = mar,
	year = {1987},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1987.10478410},
	keywords = {Approximate confidence intervals, Nonparametric intervals, Resampling methods, Second-order theory, Skewness corrections, Transformations},
	pages = {171--185},
	file = {Snapshot:/Users/yulei/Zotero/storage/RDTZM2J2/01621459.1987.html:text/html;Efron_1987_Better Bootstrap Confidence Intervals.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Efron_1987_Better Bootstrap Confidence Intervals.pdf:application/pdf},
}

@article{saltelli2002MakingBestUse,
	title = {Making best use of model evaluations to compute sensitivity indices},
	volume = {145},
	issn = {0010-4655},
	url = {https://www.sciencedirect.com/science/article/pii/S0010465502002801},
	doi = {10.1016/S0010-4655(02)00280-1},
	abstract = {This paper deals with computations of sensitivity indices in sensitivity analysis. Given a mathematical or computational model y=f(x1,x2,…,xk), where the input factors xi's are uncorrelated with one another, one can see y as the realization of a stochastic process obtained by sampling each of the xi from its marginal distribution. The sensitivity indices are related to the decomposition of the variance of y into terms either due to each xi taken singularly (first order indices), as well as into terms due to the cooperative effects of more than one xi. In this paper we assume that one has computed the full set of first order sensitivity indices as well as the full set of total-order sensitivity indices (a fairly common strategy in sensitivity analysis), and show that in this case the same set of model evaluations can be used to compute double estimates of: \&\#x02022;the total effect of two factors taken together, for all such k2 couples, where k is the dimensionality of the model;\&\#x02022;the total effect of k−2 factors taken together, for all k2 such (k−2) ples. We further introduce a new strategy for the computation of the full sets of first plus total order sensitivity indices that is about 50\% cheaper in terms of model evaluations with respect to previously published works. We discuss separately the case where the input factors xi's are not independent from each other.},
	language = {en},
	number = {2},
	urldate = {2021-09-07},
	journal = {Computer Physics Communications},
	author = {Saltelli, Andrea},
	month = may,
	year = {2002},
	keywords = {Sensitivity analysis, Importance measures, Sensitivity indices, Sensitivity measures},
	pages = {280--297},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/DTHESDYS/S0010465502002801.html:text/html;Saltelli_2002_Making best use of model evaluations to compute sensitivity indices.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Saltelli_2002_Making best use of model evaluations to compute sensitivity indices.pdf:application/pdf},
}

@article{sobol1967DistributionPointsCube,
	title = {On the distribution of points in a cube and the approximate evaluation of integrals},
	volume = {7},
	issn = {0041-5553},
	url = {https://www.sciencedirect.com/science/article/pii/0041555367901449},
	doi = {10.1016/0041-5553(67)90144-9},
	language = {en},
	number = {4},
	urldate = {2021-09-07},
	journal = {USSR Computational Mathematics and Mathematical Physics},
	author = {Sobol', I. M},
	month = jan,
	year = {1967},
	pages = {86--112},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/MHFCHQUM/0041555367901449.html:text/html;Sobol'_1967_On the distribution of points in a cube and the approximate evaluation of.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Sobol'_1967_On the distribution of points in a cube and the approximate evaluation of.pdf:application/pdf},
}

@article{sobol2011ConstructionComparisonHighDimensional,
	title = {Construction and {Comparison} of {High}-{Dimensional} {Sobol}' {Generators}},
	volume = {2011},
	issn = {1541-8286},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wilm.10056},
	doi = {10.1002/wilm.10056},
	abstract = {Sobol' sequence generators are actively used in financial applications. In this paper, we explore the effect of the uniformity properties A and A' on the generator performance in high dimensional problems. It is shown that these properties provide an additional guarantee of uniformity for high-dimensional problems even at a small number of sampled points. By imposing additional uniformity properties on low dimensional projections of the sequence in addition to the uniformity properties of the d-dimensional sequence itself the efficiency of the Sobol' sequence can be increased. The SobolSeq16384 generator which satisfies additional uniformity properties (Property A for all 16384 dimensions and Property A' for adjacent dimensions) is constructed. A comparison of known Sobol' sequence generators for a set of tests shows that for majority of tests the SobolSeq16384 generator performs better than other generators. Copyright © 2012 Wilmott Magazine Ltd.},
	language = {en},
	number = {56},
	urldate = {2021-09-08},
	journal = {Wilmott},
	author = {Sobol', Ilya M. and Asotsky, Danil and Kreinin, Alexander and Kucherenko, Sergei},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wilm.10056},
	keywords = {effective dimensions, quasi-Monte Carlo, Sobol' sequences},
	pages = {64--79},
	file = {Snapshot:/Users/yulei/Zotero/storage/55ZJZUAW/wilm.html:text/html;Sobol' et al_2011_Construction and Comparison of High-Dimensional Sobol' Generators.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Sobol' et al_2011_Construction and Comparison of High-Dimensional Sobol' Generators.pdf:application/pdf},
}

@misc{gilquin2016RecursiveEstimationProcedure,
	title = {Recursive estimation procedure of {Sobol}' indices based on replicated designs},
	url = {https://hal.inria.fr/hal-01291769},
	abstract = {In the context of global sensitivity analysis, the replication procedure allows to estimate Sobol' indices at an efficient cost. However this method still requires a large number of model evaluations. In this paper, we consider the ability of increasing the number of evaluation points, thus the accuracy of estimates, by rendering the replication procedure recursive. The key feature of this approach is the construction of structured space-filling designs. For the estimation of first-order indices, we exploit a nested Latin Hypercube already introduced in the literature. For the estimation of closed second-order indices, two methods are proposed to construct iteratively an orthogonal array. One of the two leads to a partition of the coordinate space over a Galois field. Various space-filling criteria are used to evaluate our designs.},
	language = {en},
	urldate = {2021-09-08},
	author = {Gilquin, Laurent and Arnaud, Elise and Prieur, Clémentine and Monod, Hervé},
	month = jan,
	year = {2016},
	file = {Snapshot:/Users/yulei/Zotero/storage/2CZM7Z8Z/hal-01291769v1.html:text/html;Gilquin et al_2016_Recursive estimation procedure of Sobol' indices based on replicated designs.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Gilquin et al_2016_Recursive estimation procedure of Sobol' indices based on replicated designs.pdf:application/pdf},
}

@article{sheikholeslami2017ProgressiveLatinHypercube,
	title = {Progressive {Latin} {Hypercube} {Sampling}: {An} efficient approach for robust sampling-based analysis of environmental models},
	volume = {93},
	issn = {1364-8152},
	shorttitle = {Progressive {Latin} {Hypercube} {Sampling}},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815216305096},
	doi = {10.1016/j.envsoft.2017.03.010},
	abstract = {Efficient sampling strategies that scale with the size of the problem, computational budget, and users’ needs are essential for various sampling-based analyses, such as sensitivity and uncertainty analysis. In this study, we propose a new strategy, called Progressive Latin Hypercube Sampling (PLHS), which sequentially generates sample points while progressively preserving the distributional properties of interest (Latin hypercube properties, space-filling, etc.), as the sample size grows. Unlike Latin hypercube sampling, PLHS generates a series of smaller sub-sets (slices) such that (1) the first slice is Latin hypercube, (2) the progressive union of slices remains Latin hypercube and achieves maximum stratification in any one-dimensional projection, and as such (3) the entire sample set is Latin hypercube. The performance of PLHS is compared with benchmark sampling strategies across multiple case studies for Monte Carlo simulation, sensitivity and uncertainty analysis. Our results indicate that PLHS leads to improved efficiency, convergence, and robustness of sampling-based analyses.},
	language = {en},
	urldate = {2021-09-08},
	journal = {Environmental Modelling \& Software},
	author = {Sheikholeslami, Razi and Razavi, Saman},
	month = jul,
	year = {2017},
	keywords = {Monte Carlo simulation, Sensitivity analysis, Uncertainty analysis, Design of computer experiments, Optimal Latin hypercube sampling, Sequential sampling},
	pages = {109--126},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/W6DKDEG8/S1364815216305096.html:text/html;Sheikholeslami_Razavi_2017_Progressive Latin Hypercube Sampling.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Sheikholeslami_Razavi_2017_Progressive Latin Hypercube Sampling.pdf:application/pdf},
}

@article{kucherenko2015ExploringMultidimensionalSpaces,
	title = {Exploring multi-dimensional spaces: a {Comparison} of {Latin} {Hypercube} and {Quasi} {Monte} {Carlo} {Sampling} {Techniques}},
	shorttitle = {Exploring multi-dimensional spaces},
	url = {http://arxiv.org/abs/1505.02350},
	abstract = {Three sampling methods are compared for efficiency on a number of test problems of various complexity for which analytic quadratures are available. The methods compared are Monte Carlo with pseudo-random numbers, Latin Hypercube Sampling, and Quasi Monte Carlo with sampling based on Sobol sequences. Generally results show superior performance of the Quasi Monte Carlo approach based on Sobol sequences in line with theoretical predictions. Latin Hypercube Sampling can be more efficient than both Monte Carlo method and Quasi Monte Carlo method but the latter inequality holds for a reduced set of function typology and at small number of sampled points. In conclusion Quasi Monte Carlo method would appear the safest bet when integrating functions of unknown typology.},
	urldate = {2021-09-08},
	journal = {arXiv:1505.02350 [stat]},
	author = {Kucherenko, Sergei and Albrecht, Daniel and Saltelli, Andrea},
	month = may,
	year = {2015},
	note = {arXiv: 1505.02350},
	keywords = {⛔ No DOI found, 65C05, G.3, Statistics - Applications, Statistics - Computation},
	file = {arXiv.org Snapshot:/Users/yulei/Zotero/storage/A9B9INZ9/1505.html:text/html;Kucherenko et al_2015_Exploring multi-dimensional spaces.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kucherenko et al_2015_Exploring multi-dimensional spaces.pdf:application/pdf},
}

@article{kucherenko2011IdentificationModelEffective,
	title = {The identification of model effective dimensions using global sensitivity analysis},
	volume = {96},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832010002437},
	doi = {10.1016/j.ress.2010.11.003},
	abstract = {It is shown that the effective dimensions can be estimated at reasonable computational costs using variance based global sensitivity analysis. Namely, the effective dimension in the truncation sense can be found by using the Sobol' sensitivity indices for subsets of variables. The effective dimension in the superposition sense can be estimated by using the first order effects and the total Sobol' sensitivity indices. The classification of some important classes of integrable functions based on their effective dimension is proposed. It is shown that it can be used for the prediction of the QMC efficiency. Results of numerical tests verify the prediction of the developed techniques.},
	language = {en},
	number = {4},
	urldate = {2021-09-08},
	journal = {Reliability Engineering \& System Safety},
	author = {Kucherenko, Sergei and Feil, Balazs and Shah, Nilay and Mauntz, Wolfgang},
	month = apr,
	year = {2011},
	keywords = {Global sensitivity analysis, Effective dimensions, Monte Carlo integration, Quasi-Monte Carlo, Sobol' sensitivity indices},
	pages = {440--449},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/QW5BJI5F/S0951832010002437.html:text/html;Kucherenko et al_2011_The identification of model effective dimensions using global sensitivity.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kucherenko et al_2011_The identification of model effective dimensions using global sensitivity.pdf:application/pdf},
}

@article{razavi2012ReviewSurrogateModeling,
	title = {Review of surrogate modeling in water resources: {REVIEW}},
	volume = {48},
	issn = {00431397},
	shorttitle = {Review of surrogate modeling in water resources},
	url = {http://doi.wiley.com/10.1029/2011WR011527},
	doi = {10.1029/2011WR011527},
	language = {en},
	number = {7},
	urldate = {2021-09-08},
	journal = {Water Resources Research},
	author = {Razavi, Saman and Tolson, Bryan A. and Burn, Donald H.},
	month = jul,
	year = {2012},
	file = {Razavi et al_2012_Review of surrogate modeling in water resources.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Razavi et al_2012_Review of surrogate modeling in water resources.pdf:application/pdf},
}

@article{xiu2002WienerAskeyPolynomial,
	title = {The {Wiener}--{Askey} {Polynomial} {Chaos} for {Stochastic} {Differential} {Equations}},
	volume = {24},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/S1064827501387826},
	doi = {10.1137/S1064827501387826},
	abstract = {We present a new method for solving stochastic differential equations based on Galerkin projections and extensions of Wiener's polynomial chaos. Specifically, we represent the stochastic processes with an optimum trial basis from the Askey family of orthogonal polynomials that reduces the dimensionality of the system and leads to exponential convergence of the error. Several continuous and discrete processes are treated, and numerical examples show substantial speed-up compared to Monte Carlo simulations for low dimensional stochastic inputs.},
	number = {2},
	urldate = {2021-09-08},
	journal = {SIAM Journal on Scientific Computing},
	author = {Xiu, Dongbin and Karniadakis, George Em},
	month = jan,
	year = {2002},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65C20, 65C30, Askey scheme, Galerkin projection, orthogonal polynomials, polynomial chaos, spectral methods, stochastic differential equations},
	pages = {619--644},
	file = {Xiu_Karniadakis_2002_The Wiener--Askey Polynomial Chaos for Stochastic Differential Equations.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Xiu_Karniadakis_2002_The Wiener--Askey Polynomial Chaos for Stochastic Differential Equations.pdf:application/pdf},
}

@article{sudret2008GlobalSensitivityAnalysis,
	series = {Bayesian {Networks} in {Dependability}},
	title = {Global sensitivity analysis using polynomial chaos expansions},
	volume = {93},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832007001329},
	doi = {10.1016/j.ress.2007.04.002},
	abstract = {Global sensitivity analysis (SA) aims at quantifying the respective effects of input random variables (or combinations thereof) onto the variance of the response of a physical or mathematical model. Among the abundant literature on sensitivity measures, the Sobol’ indices have received much attention since they provide accurate information for most models. The paper introduces generalized polynomial chaos expansions (PCE) to build surrogate models that allow one to compute the Sobol’ indices analytically as a post-processing of the PCE coefficients. Thus the computational cost of the sensitivity indices practically reduces to that of estimating the PCE coefficients. An original non intrusive regression-based approach is proposed, together with an experimental design of minimal size. Various application examples illustrate the approach, both from the field of global SA (i.e. well-known benchmark problems) and from the field of stochastic mechanics. The proposed method gives accurate results for various examples that involve up to eight input random variables, at a computational cost which is 2–3 orders of magnitude smaller than the traditional Monte Carlo-based evaluation of the Sobol’ indices.},
	language = {en},
	number = {7},
	urldate = {2021-09-08},
	journal = {Reliability Engineering \& System Safety},
	author = {Sudret, Bruno},
	month = jul,
	year = {2008},
	keywords = {Global sensitivity analysis, Sobol’ indices, Analysis of variance, Generalized chaos, Polynomial chaos, Regression, Stochastic finite elements},
	pages = {964--979},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/XJRM3EWI/S0951832007001329.html:text/html;Sudret_2008_Global sensitivity analysis using polynomial chaos expansions.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Sudret_2008_Global sensitivity analysis using polynomial chaos expansions.pdf:application/pdf},
}

@phdthesis{bier1982MeasureUncertaintyImportance,
	type = {Thesis},
	title = {A measure of uncertainty importance for components in fault trees},
	copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
	url = {https://dspace.mit.edu/handle/1721.1/15687},
	abstract = {Thesis (Ph.D.)--Massachusetts Institute of Technology, Alfred P. Sloan School of Management, 1983.},
	language = {eng},
	urldate = {2021-09-08},
	school = {Massachusetts Institute of Technology},
	author = {Bier, Vicki M.},
	year = {1982},
	note = {Accepted: 2005-08-04T22:08:51Z},
	file = {Snapshot:/Users/yulei/Zotero/storage/7GTDYG2D/15687.html:text/html;Bier_1982_A measure of uncertainty importance for components in fault trees.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Bier_1982_A measure of uncertainty importance for components in fault trees.pdf:application/pdf},
}

@article{butler2020OptimalExperimentalDesign,
	title = {Optimal experimental design for prediction based on push-forward probability measures},
	volume = {416},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999120302928},
	doi = {10.1016/j.jcp.2020.109518},
	abstract = {Incorporating experimental data is essential for increasing the credibility of simulation-aided decision making and design. This paper presents a method which uses a computational model to guide the optimal acquisition of experimental data to produce data-informed predictions of quantities of interest (QoI). Many strategies for optimal experimental design (OED) select data that maximize some utility that measures the reduction in uncertainty of uncertain model parameters, for example the expected information gain between prior and posterior distributions of these parameters. In this paper, we seek to maximize the expected information gained from the push-forward of an initial (prior) density to the push-forward of the updated (posterior) density through the parameter-to-prediction map. The formulation presented is based upon the solution of a specific class of stochastic inverse problems which seeks a probability density that is consistent with the model and the data in the sense that the push-forward of this density through the parameter-to-observable map matches a target density on the observable data. While this stochastic inverse problem forms the mathematical basis for our approach, we develop a one-step algorithm, focused on push-forward probability measures, that leverages inference-for-prediction to bypass constructing the solution to the stochastic inverse problem. A number of numerical results are presented to demonstrate the utility of this optimal experimental design for prediction and facilitate comparison of our approach with traditional OED.},
	language = {en},
	urldate = {2021-09-08},
	journal = {Journal of Computational Physics},
	author = {Butler, T. and Jakeman, J. D. and Wildey, T.},
	month = sep,
	year = {2020},
	keywords = {Uncertainty quantification, Optimal experimental design, Push-forward measures, Stochastic inference},
	pages = {109518},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/EP5REPUX/S0021999120302928.html:text/html;Butler et al_2020_Optimal experimental design for prediction based on push-forward probability.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Butler et al_2020_Optimal experimental design for prediction based on push-forward probability.pdf:application/pdf},
}

@book{artb.owen2013MonteCarloTheory,
	title = {Monte {Carlo} theory, methods and examples},
	author = {{Art B. Owen}},
	year = {2013},
}

@article{ravalico2009SensitivityAnalysisDecisionmaking,
	series = {Special {Issue} on {Sensitivity} {Analysis}},
	title = {Sensitivity analysis for decision-making using the {MORE} method—{A} {Pareto} approach},
	volume = {94},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832009000234},
	doi = {10.1016/j.ress.2009.01.009},
	abstract = {Integrated Assessment Modelling (IAM) incorporates knowledge from different disciplines to provide an overarching assessment of the impact of different management decisions. The complex nature of these models, which often include non-linearities and feedback loops, requires special attention for sensitivity analysis. This is especially true when the models are used to form the basis of management decisions, where it is important to assess how sensitive the decisions being made are to changes in model parameters. This research proposes an extension to the Management Option Rank Equivalence (MORE) method of sensitivity analysis; a new method of sensitivity analysis developed specifically for use in IAM and decision-making. The extension proposes using a multi-objective Pareto optimal search to locate minimum combined parameter changes that result in a change in the preferred management option. It is demonstrated through a case study of the Namoi River, where results show that the extension to MORE is able to provide sensitivity information for individual parameters that takes into account simultaneous variations in all parameters. Furthermore, the increased sensitivities to individual parameters that are discovered when joint parameter variation is taken into account shows the importance of ensuring that any sensitivity analysis accounts for these changes.},
	language = {en},
	number = {7},
	urldate = {2021-09-08},
	journal = {Reliability Engineering \& System Safety},
	author = {Ravalico, Jakin K. and Maier, Holger R. and Dandy, Graeme C.},
	month = jul,
	year = {2009},
	keywords = {Sensitivity analysis, Decision-making, Integrated assessment modelling, Pareto optimization},
	pages = {1229--1237},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/9EX4V4IA/S0951832009000234.html:text/html;Ravalico et al_2009_Sensitivity analysis for decision-making using the MORE method—A Pareto approach.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Ravalico et al_2009_Sensitivity analysis for decision-making using the MORE method—A Pareto approach.pdf:application/pdf},
}

@book{dorfman1987LinearProgrammingEconomic,
	title = {Linear {Programming} and {Economic} {Analysis}},
	isbn = {978-0-486-65491-1},
	abstract = {Designed primarily for economists and those interested in management economics who are not necessarily accomplished mathematicians, this text offers a clear, concise exposition of the relationship of linear programming to standard economic analysis. The research and writing were supported by The RAND Corporation in the late 1950s. Linear programming has been one of the most important postwar developments in economic theory, but until publication of the present volume, no text offered a comprehensive treatment of the many facets of the relationship of linear programming to traditional economic theory. This book was the first to provide a wide-ranging survey of such important aspects of the topic as the interrelations between the celebrated von Neumann theory of games and linear programming, and the relationship between game theory and the traditional economic theories of duopoly and bilateral monopoly. Modern economists will especially appreciate the treatment of the connection between linear programming and modern welfare economics and the insights that linear programming gives into the determinateness of Walrasian equilibrium. The book also offers an excellent introduction to the important Leontief theory of input-output as well as extensive treatment of the problems of dynamic linear programming. Successfully used for three decades in graduate economics courses, this book stresses practical problems and specifies important concrete applications.},
	language = {en},
	publisher = {Courier Corporation},
	author = {Dorfman, Robert and Samuelson, Paul Anthony and Solow, Robert M.},
	month = jan,
	year = {1987},
	note = {Google-Books-ID: k5\_vzaCNQP4C},
	keywords = {Business \& Economics / Economics / General, Mathematics / General, Mathematics / Linear \& Nonlinear Programming},
}

@article{duinker2007ScenarioAnalysisEnvironmental,
	title = {Scenario analysis in environmental impact assessment: {Improving} explorations of the future},
	volume = {27},
	issn = {0195-9255},
	shorttitle = {Scenario analysis in environmental impact assessment},
	url = {https://www.sciencedirect.com/science/article/pii/S0195925506001302},
	doi = {10.1016/j.eiar.2006.11.001},
	abstract = {Scenarios and scenario analysis have become popular approaches in organizational planning and participatory exercises in pursuit of sustainable development. However, they are little used, at least in any formal way, in environmental impact assessment (EIA). This is puzzling because EIA is a process specifically dedicated to exploring options for more-sustainable (i.e., less environmentally damaging) futures. In this paper, we review the state of the art associated with scenarios and scenario analysis, and describe two areas where scenario analysis could be particularly helpful in EIA: (a) in defining future developments for cumulative effects assessment; and (b) in considering the influence of contextual change – e.g. climate change – on impact forecasts for specific projects. We conclude by encouraging EIA practitioners to learn about the promise of scenario-based analysis and implement scenario-based methods so that EIA can become more effective in fostering sustainable development.},
	language = {en},
	number = {3},
	urldate = {2021-09-08},
	journal = {Environmental Impact Assessment Review},
	author = {Duinker, Peter N. and Greig, Lorne A.},
	month = apr,
	year = {2007},
	keywords = {Climate change, Canada, Cumulative effects assessment, Environmental impact assessment, Futures studies, Scenario},
	pages = {206--219},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/6GWQHIJX/S0195925506001302.html:text/html;Duinker_Greig_2007_Scenario analysis in environmental impact assessment.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Duinker_Greig_2007_Scenario analysis in environmental impact assessment.pdf:application/pdf},
}

@article{maier2016UncertainFutureDeep,
	title = {An uncertain future, deep uncertainty, scenarios, robustness and adaptation: {How} do they fit together?},
	volume = {81},
	issn = {1364-8152},
	shorttitle = {An uncertain future, deep uncertainty, scenarios, robustness and adaptation},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815216300780},
	doi = {10.1016/j.envsoft.2016.03.014},
	abstract = {A highly uncertain future due to changes in climate, technology and socio-economics has led to the realisation that identification of “best-guess” future conditions might no longer be appropriate. Instead, multiple plausible futures need to be considered, which requires (i) uncertainties to be described with the aid of scenarios that represent coherent future pathways based on different sets of assumptions, (ii) system performance to be represented by metrics that measure insensitivity (i.e. robustness) to changes in future conditions, and (iii) adaptive strategies to be considered alongside their more commonly used static counterparts. However, while these factors have been considered in isolation previously, there has been a lack of discussion of the way they are connected. In order to address this shortcoming, this paper presents a multidisciplinary perspective on how the above factors fit together to facilitate the development of strategies that are best suited to dealing with a deeply uncertain future.},
	language = {en},
	urldate = {2021-09-08},
	journal = {Environmental Modelling \& Software},
	author = {Maier, H. R. and Guillaume, J. H. A. and van Delden, H. and Riddell, G. A. and Haasnoot, M. and Kwakkel, J. H.},
	month = jul,
	year = {2016},
	keywords = {Robustness, Adaptability, Decision support, Deep uncertainty, Scenarios, Uncertain future},
	pages = {154--164},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/N399T4QW/S1364815216300780.html:text/html;Maier et al_2016_An uncertain future, deep uncertainty, scenarios, robustness and adaptation.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Maier et al_2016_An uncertain future, deep uncertainty, scenarios, robustness and adaptation.pdf:application/pdf},
}

@article{lamontagne2019RobustAbatementPathways,
	title = {Robust abatement pathways to tolerable climate futures require immediate global action},
	volume = {9},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1758-6798},
	url = {https://www.nature.com/articles/s41558-019-0426-8},
	doi = {10.1038/s41558-019-0426-8},
	abstract = {Disentangling the relative importance of climate change abatement policies from the human–Earth system (HES) uncertainties that determine their performance is challenging because the two are inexorably linked, and the nature of this linkage is dynamic, interactive and metric specific1. Here, we demonstrate an approach to quantify the individual and joint roles that diverse HES uncertainties and our choices in abatement policy play in determining future climate and economic conditions, as simulated by an improved version of the Dynamic Integrated model of Climate and the Economy2,3. Despite wide-ranging HES uncertainties, the growth rate of global abatement (a societal choice) is the primary driver of long-term warming. It is not a question of whether we can limit warming but whether we choose to do so. Our results elucidate important long-term HES dynamics that are often masked by common time-aggregated metrics. Aggressive near-term abatement will be very costly and do little to impact near-term warming. Conversely, the warming that will be experienced by future generations will mostly be driven by earlier abatement actions. We quantify probabilistic abatement pathways to tolerable climate/economic outcomes4,5, conditional on the climate sensitivity to the atmospheric CO2 concentration. Even under optimistic assumptions about the climate sensitivity, pathways to a tolerable climate/economic future are rapidly narrowing.},
	language = {en},
	number = {4},
	urldate = {2021-09-08},
	journal = {Nature Climate Change},
	author = {Lamontagne, J. R. and Reed, P. M. and Marangoni, G. and Keller, K. and Garner, G. G.},
	month = apr,
	year = {2019},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 4
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Climate-change mitigation;Climate-change policy;Governance;Socioeconomic scenarios
Subject\_term\_id: climate-change-mitigation;climate-change-policy;governance;socioeconomic-scenarios},
	pages = {290--294},
	file = {Snapshot:/Users/yulei/Zotero/storage/K74PG7N9/s41558-019-0426-8.html:text/html;Lamontagne et al_2019_Robust abatement pathways to tolerable climate futures require immediate global.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Lamontagne et al_2019_Robust abatement pathways to tolerable climate futures require immediate global.pdf:application/pdf},
}

@article{herman2015HowShouldRobustness,
	title = {How {Should} {Robustness} {Be} {Defined} for {Water} {Systems} {Planning} under {Change}?},
	volume = {141},
	copyright = {© 2015 American Society of Civil Engineers},
	issn = {1943-5452},
	url = {https://ascelibrary.org/doi/abs/10.1061/%28ASCE%29WR.1943-5452.0000509},
	doi = {10.1061/(ASCE)WR.1943-5452.0000509},
	abstract = {Water systems planners have long recognized the need for robust solutions capable of withstanding deviations from the conditions for which they were designed. Robustness analyses have shifted from expected utility to exploratory bottom-up approaches which identify vulnerable scenarios prior to assigning likelihoods. Examples include Robust Decision Making (RDM), Decision Scaling, Info-Gap, and Many-Objective Robust Decision Making (MORDM). We propose a taxonomy of robustness frameworks to compare and contrast these approaches based on their methods of (1) alternative generation, (2) sampling of states of the world, (3) quantification of robustness measures, and (4) sensitivity analysis to identify important uncertainties. Building from the proposed taxonomy, we use a regional urban water supply case study in the Research Triangle region of North Carolina to illustrate the decision-relevant consequences that emerge from each of these choices. Results indicate that the methodological choices in the taxonomy lead to the selection of substantially different planning alternatives, underscoring the importance of an informed definition of robustness. Moreover, the results show that some commonly employed methodological choices and definitions of robustness can have undesired consequences when ranking decision alternatives. For the demonstrated test case, recommendations for overcoming these issues include: (1) decision alternatives should be searched rather than prespecified, (2) dominant uncertainties should be discovered through sensitivity analysis rather than assumed, and (3) a carefully elicited multivariate satisficing measure of robustness allows stakeholders to achieve their problem-specific performance requirements. This work emphasizes the importance of an informed problem formulation for systems facing challenging performance tradeoffs and provides a common vocabulary to link the robustness frameworks widely used in the field of water systems planning.},
	language = {EN},
	number = {10},
	urldate = {2021-09-08},
	journal = {Journal of Water Resources Planning and Management},
	author = {Herman, Jonathan D. and Reed, Patrick M. and Zeff, Harrison B. and Characklis, Gregory W.},
	month = oct,
	year = {2015},
	note = {Publisher: American Society of Civil Engineers},
	pages = {04015012},
	file = {Snapshot:/Users/yulei/Zotero/storage/WC2CZ2PI/(ASCE)WR.1943-5452.html:text/html;Herman et al_2015_How Should Robustness Be Defined for Water Systems Planning under Change.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Herman et al_2015_How Should Robustness Be Defined for Water Systems Planning under Change.pdf:application/pdf},
}

@article{marangoni2017SensitivityProjectedLongterm,
	title = {Sensitivity of projected long-term {CO2} emissions across the {Shared} {Socioeconomic} {Pathways}},
	volume = {7},
	copyright = {2017 Nature Publishing Group},
	issn = {1758-6798},
	url = {https://www.nature.com/articles/nclimate3199},
	doi = {10.1038/nclimate3199},
	abstract = {Socioeconomic scenarios of climate change contain a number of assumptions, which lead to uncertainty in projections. Emission estimates in the scenarios are found to be most sensitive for assumptions about energy intensity and economic growth.},
	language = {en},
	number = {2},
	urldate = {2021-09-08},
	journal = {Nature Climate Change},
	author = {Marangoni, G. and Tavoni, M. and Bosetti, V. and Borgonovo, E. and Capros, P. and Fricko, O. and Gernaat, D. E. H. J. and Guivarch, C. and Havlik, P. and Huppmann, D. and Johnson, N. and Karkatsoulis, P. and Keppo, I. and Krey, V. and Ó Broin, E. and Price, J. and van Vuuren, D. P.},
	month = feb,
	year = {2017},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 2
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Climate-change mitigation;Socioeconomic scenarios
Subject\_term\_id: climate-change-mitigation;socioeconomic-scenarios},
	pages = {113--117},
	file = {Snapshot:/Users/yulei/Zotero/storage/J2XXU2SR/nclimate3199.html:text/html;Marangoni et al_2017_Sensitivity of projected long-term CO2 emissions across the Shared.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Marangoni et al_2017_Sensitivity of projected long-term CO2 emissions across the Shared.pdf:application/pdf},
}

@article{workman2020DecisionMakingContexts,
	title = {Decision making in contexts of deep uncertainty - {An} alternative approach for long-term climate policy},
	volume = {103},
	issn = {1462-9011},
	url = {https://www.sciencedirect.com/science/article/pii/S1462901119304319},
	doi = {10.1016/j.envsci.2019.10.002},
	abstract = {The majority of global emissions scenarios compatible with holding global warming to less than 2 °C depend on the large-scale use of bioenergy with carbon capture and storage (BECCS) to compensate for an overshoot of atmospheric CO2 budgets. Recent critiques have highlighted the ethical and environmental risks of this strategy and the danger of building long-term climate policy on such speculative technological scenarios emerging from integrated assessment models. Here, we critically examine both the use of BECCS in mitigation scenarios and the decision making philosophy underlying the use of integrated assessment modelling to inform climate policy. We identify a number of features of integrated assessment models that favour selection of BECCS over alternative strategies. However, we argue that the deeper issue lies in the tendency to view model outputs as objective science, capable of defining “optimal” goals and strategies for which climate policy should strive, rather than as exploratory tools within a broader policy development process. This model-centric decision making philosophy is highly sensitive to uncertainties in model assumptions and future trends, and tends to favour solutions that perform well within the model framework at the expense of a wider mix of strategies and values. Drawing on the principles of Robust Decision Making, we articulate the need for an alternative approach that explicitly embraces uncertainty, multiple values and diversity among stakeholders and viewpoints, and in which modelling exists in an iterative exchange with policy development rather than separate from it. Such an approach would provide more relevant and robust information to near-term policymaking, and enable an inclusive societal dialogue about the appropriate role for carbon dioxide removal within climate policy.},
	language = {en},
	urldate = {2021-09-08},
	journal = {Environmental Science \& Policy},
	author = {Workman, Mark and Dooley, Kate and Lomax, Guy and Maltby, James and Darch, Geoff},
	month = jan,
	year = {2020},
	keywords = {Integrated assessment modelling, Carbon dioxide removal technologies, Diversity in value-sets, International climate policy, Predict then act, Robust decision making},
	pages = {77--84},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/6D8BQJJ8/S1462901119304319.html:text/html;Workman et al_2020_Decision making in contexts of deep uncertainty - An alternative approach for.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Workman et al_2020_Decision making in contexts of deep uncertainty - An alternative approach for.pdf:application/pdf},
}

@article{saltelli2002RelativeImportanceInput,
	title = {On the {Relative} {Importance} of {Input} {Factors} in {Mathematical} {Models}},
	volume = {97},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214502388618447},
	doi = {10.1198/016214502388618447},
	abstract = {This article deals with global quantitative sensitivity analysis of the Level E model, a computer code used in safety assessment for nuclear waste disposal. The Level E code has been the subject of two international benchmarks of risk assessment codes and Monte Carlo methods and is well known in the literature. We discuss the Level E model with reference to two different settings. In the first setting, the objective is to find the input factor that drives most of the output variance. In the second setting, we strive to achieve a preestablished reduction in the variance of the model output by fixing the smallest number of factors. The emphasis of this work is on how to define the concept of importance in an unambiguous way and how to assess it in the simultaneous occurrence of correlated input factors and non-additive models.},
	number = {459},
	urldate = {2021-09-08},
	journal = {Journal of the American Statistical Association},
	author = {Saltelli, Andrea and Tarantola, Stefano},
	month = sep,
	year = {2002},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214502388618447},
	keywords = {Sensitivity analysis, Analysis of variance, Correlated input, Nonadditive model},
	pages = {702--709},
	file = {Snapshot:/Users/yulei/Zotero/storage/8L2C4376/016214502388618447.html:text/html;Saltelli_Tarantola_2002_On the Relative Importance of Input Factors in Mathematical Models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Saltelli_Tarantola_2002_On the Relative Importance of Input Factors in Mathematical Models.pdf:application/pdf},
}

@article{eisenhower2012MethodologyMetamodelBased,
	title = {A methodology for meta-model based optimization in building energy models},
	volume = {47},
	issn = {0378-7788},
	url = {https://www.sciencedirect.com/science/article/pii/S0378778811005962},
	doi = {10.1016/j.enbuild.2011.12.001},
	abstract = {As building energy models become more accurate and numerically efficient, model-based optimization of building design and operation is becoming more practical. The state-of-the-art typically couples an optimizer with a building energy model which tends to be time consuming and often leads to suboptimal results because of the mathematical properties of the energy model. To mitigate this issue, we present an approach that begins by sampling the parameter space of the building model around its baseline. An analytical meta-model is then fit to this data and optimization can be performed using different optimization cost functions or optimization algorithms with very little computational effort. Uncertainty and sensitivity analysis is also performed to identify the most influential parameters for the optimization. A case study is explored using an EnergyPlus model of an existing building which contains over 1000 parameters. When using a cost function that penalizes thermal comfort and energy, 45\% annual energy reduction is achieved while simultaneously increasing thermal comfort by a factor of two. We compare the optimization using the meta-model approach with an approach using the EnergyPlus model integrated with the optimizer on a smaller problem using only seven optimization parameters illustrating good performance.},
	language = {en},
	urldate = {2021-09-08},
	journal = {Energy and Buildings},
	author = {Eisenhower, Bryan and O’Neill, Zheng and Narayanan, Satish and Fonoberov, Vladimir A. and Mezić, Igor},
	month = apr,
	year = {2012},
	keywords = {Machine learning, Sensitivity analysis, Comfort and energy optimization, EnergyPlus},
	pages = {292--301},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/VWMXTWRW/S0378778811005962.html:text/html;Eisenhower et al_2012_A methodology for meta-model based optimization in building energy models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Eisenhower et al_2012_A methodology for meta-model based optimization in building energy models.pdf:application/pdf},
}

@article{becker2012BayesianSensitivityAnalysis,
	series = {Uncertainties in {Structural} {Dynamics}},
	title = {Bayesian sensitivity analysis of a nonlinear finite element model},
	volume = {32},
	issn = {0888-3270},
	url = {https://www.sciencedirect.com/science/article/pii/S0888327012000866},
	doi = {10.1016/j.ymssp.2012.03.009},
	abstract = {A major problem in uncertainty and sensitivity analysis is that the computational cost of propagating probabilistic uncertainty through large nonlinear models can be prohibitive when using conventional methods (such as Monte Carlo methods). A powerful solution to this problem is to use an emulator, which is a mathematical representation of the model built from a small set of model runs at specified points in input space. Such emulators are massively cheaper to run and can be used to mimic the “true” model, with the result that uncertainty analysis and sensitivity analysis can be performed for a greatly reduced computational cost. The work here investigates the use of an emulator known as a Gaussian process (GP), which is an advanced probabilistic form of regression. The GP is particularly suited to uncertainty analysis since it is able to emulate a wide class of models, and accounts for its own emulation uncertainty. Additionally, uncertainty and sensitivity measures can be estimated analytically, given certain assumptions. The GP approach is explained in detail here, and a case study of a finite element model of an airship is used to demonstrate the method. It is concluded that the GP is a very attractive way of performing uncertainty and sensitivity analysis on large models, provided that the dimensionality is not too high.},
	language = {en},
	urldate = {2021-09-08},
	journal = {Mechanical Systems and Signal Processing},
	author = {Becker, W. and Oakley, J. E. and Surace, C. and Gili, P. and Rowson, J. and Worden, K.},
	month = oct,
	year = {2012},
	keywords = {Gaussian process, Uncertainty, Bayesian, Emulator, Finite element, Sensitivity},
	pages = {18--31},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/WHC3T3KJ/S0888327012000866.html:text/html;Becker et al_2012_Bayesian sensitivity analysis of a nonlinear finite element model.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Becker et al_2012_Bayesian sensitivity analysis of a nonlinear finite element model.pdf:application/pdf},
}

@book{box2005StatisticsExperimentersDesign,
	address = {Hoboken, N.J},
	edition = {2nd ed},
	series = {Wiley series in probability and statistics},
	title = {Statistics for experimenters: design, innovation, and discovery},
	isbn = {978-0-471-71813-0},
	shorttitle = {Statistics for experimenters},
	publisher = {Wiley-Interscience},
	author = {Box, George E. P. and Hunter, J. Stuart and Hunter, William Gordon},
	year = {2005},
	keywords = {Analysis of variance, Experimental design, Mathematical statistics},
}

@article{morris1991FactorialSamplingPlans,
	title = {Factorial {Sampling} {Plans} for {Preliminary} {Computational} {Experiments}},
	volume = {33},
	issn = {0040-1706},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1991.10484804},
	doi = {10.1080/00401706.1991.10484804},
	abstract = {A computational model is a representation of some physical or other system of interest, first expressed mathematically and then implemented in the form of a computer program; it may be viewed as a function of inputs that, when evaluated, produces outputs. Motivation for this article comes from computational models that are deterministic, complicated enough to make classical mathematical analysis impractical and that have a moderate-to-large number of inputs. The problem of designing computational experiments to determine which inputs have important effects on an output is considered. The proposed experimental plans are composed of individually randomized one-factor-at-a-time designs, and data analysis is based on the resulting random sample of observed elementary effects, those changes in an output due solely to changes in a particular input. Advantages of this approach include a lack of reliance on assumptions of relative sparsity of important inputs, monotonicity of outputs with respect to inputs, or adequacy of a low-order polynomial as an approximation to the computational model.},
	number = {2},
	urldate = {2021-09-08},
	journal = {Technometrics},
	author = {Morris, Max D.},
	month = may,
	year = {1991},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1991.10484804},
	keywords = {Sensitivity analysis, Latin hypercube sampling, Computational model, Factor screening},
	pages = {161--174},
	file = {Snapshot:/Users/yulei/Zotero/storage/8SP3LASI/00401706.1991.html:text/html;Morris_1991_Factorial Sampling Plans for Preliminary Computational Experiments.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Morris_1991_Factorial Sampling Plans for Preliminary Computational Experiments.pdf:application/pdf},
}

@article{daveiga2015GlobalSensitivityAnalysis,
	title = {Global sensitivity analysis with dependence measures},
	volume = {85},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949655.2014.945932},
	doi = {10.1080/00949655.2014.945932},
	abstract = {Global sensitivity analysis with variance-based measures suffers from several theoretical and practical limitations, since they focus only on the variance of the output and handle multivariate variables in a limited way. In this paper, we introduce a new class of sensitivity indices based on dependence measures which overcomes these insufficiencies. Our approach originates from the idea to compare the output distribution with its conditional counterpart when one of the input variables is fixed. We establish that this comparison yields previously proposed indices when it is performed with Csiszár f-divergences, as well as sensitivity indices which are well-known dependence measures between random variables. This leads us to investigate completely new sensitivity indices based on recent state-of-the-art dependence measures, such as distance correlation and the Hilbert–Schmidt independence criterion. We also emphasize the potential of feature selection techniques relying on such dependence measures as alternatives to screening in high dimension.},
	number = {7},
	urldate = {2021-09-08},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Da Veiga, Sebastien},
	month = may,
	year = {2015},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00949655.2014.945932},
	keywords = {global sensitivity analysis, independence, kernel, mutual information},
	pages = {1283--1305},
	file = {Snapshot:/Users/yulei/Zotero/storage/9SMFPUPX/00949655.2014.html:text/html;Da Veiga_2015_Global sensitivity analysis with dependence measures.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Da Veiga_2015_Global sensitivity analysis with dependence measures.pdf:application/pdf},
}

@article{razavi2019VARSTOOLToolboxComprehensive,
	title = {{VARS}-{TOOL}: {A} toolbox for comprehensive, efficient, and robust sensitivity and uncertainty analysis},
	volume = {112},
	issn = {1364-8152},
	shorttitle = {{VARS}-{TOOL}},
	url = {https://www.sciencedirect.com/science/article/pii/S1364815218304766},
	doi = {10.1016/j.envsoft.2018.10.005},
	abstract = {VARS-TOOL is a software toolbox for sensitivity and uncertainty analysis. Developed primarily around the “Variogram Analysis of Response Surfaces” framework, VARS-TOOL adopts a multi-method approach that enables simultaneous generation of a range of sensitivity indices, including ones based on derivative, variance, and variogram concepts, from a single sample. Other special features of VARS-TOOL include (1) novel tools for time-varying and time-aggregate sensitivity analysis of dynamical systems models, (2) highly efficient sampling techniques, such as Progressive Latin Hypercube Sampling (PLHS), that maximize robustness and rapid convergence to stable sensitivity estimates, (3) factor grouping for dealing with high-dimensional problems, (4) visualization for monitoring stability and convergence, (5) model emulation for handling model crashes, and (6) an interface that allows working with any model in any programming language and operating system. As a test bed for training and research, VARS-TOOL provides a set of mathematical test functions and the (dynamical) HBV-SASK hydrologic model.},
	language = {en},
	urldate = {2021-09-08},
	journal = {Environmental Modelling \& Software},
	author = {Razavi, Saman and Sheikholeslami, Razi and Gupta, Hoshin V. and Haghnegahdar, Amin},
	month = feb,
	year = {2019},
	keywords = {Global sensitivity analysis, Uncertainty analysis, Sensitivity indices, Morris, Dynamical systems models, Performance metrics, Progressive Latin hypercube sampling (PLHS), Sobol’, Variogram analysis of response surface (VARS)},
	pages = {95--107},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/N95SGY9D/S1364815218304766.html:text/html;Razavi et al_2019_VARS-TOOL.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Razavi et al_2019_VARS-TOOL.pdf:application/pdf},
}

@article{saltelli2013WhatMakeYour,
	title = {What do {I} make of your latinorumc {Sensitivity} auditing of mathematical modelling},
	volume = {9},
	issn = {1740-2816, 1740-2824},
	url = {http://www.inderscience.com/link.php?id=58610},
	doi = {10.1504/IJFIP.2013.058610},
	language = {en},
	number = {2/3/4},
	urldate = {2021-09-08},
	journal = {International Journal of Foresight and Innovation Policy},
	author = {Saltelli, Andrea and Guimaraes Pereira, Ângela and Sluijs, Jeroen P. Van der and Funtowicz, Silvio},
	year = {2013},
	pages = {213},
	file = {Saltelli et al_2013_What do I make of your latinorumc Sensitivity auditing of mathematical modelling.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Saltelli et al_2013_What do I make of your latinorumc Sensitivity auditing of mathematical modelling.pdf:application/pdf},
}

@article{gunawan2005SensitivityAnalysisDiscrete,
	title = {Sensitivity {Analysis} of {Discrete} {Stochastic} {Systems}},
	volume = {88},
	issn = {0006-3495},
	url = {https://www.sciencedirect.com/science/article/pii/S0006349505733087},
	doi = {10.1529/biophysj.104.053405},
	abstract = {Sensitivity analysis quantifies the dependence of system behavior on the parameters that affect the process dynamics. Classical sensitivity analysis, however, does not directly apply to discrete stochastic dynamical systems, which have recently gained popularity because of its relevance in the simulation of biological processes. In this work, sensitivity analysis for discrete stochastic processes is developed based on density function (distribution) sensitivity, using an analog of the classical sensitivity and the Fisher Information Matrix. There exist many circumstances, such as in systems with multistability, in which the stochastic effects become nontrivial and classical sensitivity analysis on the deterministic representation of a system cannot adequately capture the true system behavior. The proposed analysis is applied to a bistable chemical system—the Schlögl model, and to a synthetic genetic toggle-switch model. Comparisons between the stochastic and deterministic analyses show the significance of explicit consideration of the probabilistic nature in the sensitivity analysis for this class of processes.},
	language = {en},
	number = {4},
	urldate = {2021-09-11},
	journal = {Biophysical Journal},
	author = {Gunawan, Rudiyanto and Cao, Yang and Petzold, Linda and Doyle, Francis J.},
	month = apr,
	year = {2005},
	pages = {2530--2540},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/KF8VVQA7/S0006349505733087.html:text/html;Gunawan et al_2005_Sensitivity Analysis of Discrete Stochastic Systems.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Gunawan et al_2005_Sensitivity Analysis of Discrete Stochastic Systems.pdf:application/pdf},
}

@article{adelman1986SensitivityAnalysisDiscrete,
	title = {Sensitivity {Analysis} of {Discrete} {Structural} {Systems}},
	volume = {24},
	issn = {0001-1452},
	url = {https://doi.org/10.2514/3.48671},
	doi = {10.2514/3.48671},
	number = {5},
	urldate = {2021-09-12},
	journal = {AIAA Journal},
	author = {Adelman, Howard M. and Haftka, Raphael T.},
	year = {1986},
	note = {Publisher: American Institute of Aeronautics and Astronautics
\_eprint: https://doi.org/10.2514/3.48671},
	pages = {823--832},
	file = {AIAA Snapshot:/Users/yulei/Zotero/storage/922K8HW9/3.html:text/html;Adelman_Haftka_1986_Sensitivity Analysis of Discrete Structural Systems.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Adelman_Haftka_1986_Sensitivity Analysis of Discrete Structural Systems.pdf:application/pdf},
}

@techreport{dellavigna2018StructuralBehavioralEconomics,
	type = {Working {Paper}},
	title = {Structural {Behavioral} {Economics}},
	url = {https://www.nber.org/papers/w24797},
	abstract = {What is the role of structural estimation in behavioral economics? I discuss advantages, and limitations, of the work in Structural Behavioral Economics. I also cover common modeling choices and how to get started. Among the advantages, I argue that structural estimation builds on, and expands, a classical behavioral tool, simple calibrations, and that it benefits from the presence of a few parsimonious behavioral models which can be taken to the data. Estimation is also well suited for experimental work, common in behavioral economics, as it can lead to improvements in the experimental design. In addition, at a time where policy implications of behavioral work are increasingly discussed, it is important to ground these policy implications in (estimated) models. Structural work, however, has important limitations, which are relevant to its behavioral applications. Estimation takes much longer and the extra degree of complexity can make it difficult to know which of a series of assumptions is driving the results. For related reasons, it is also easy to over-reach with the welfare implications. Taking this into account, I provide a partial how-to guide to structural behavioral economics, covering: (i) the choice of estimation method; (ii) the modeling of heterogeneity; (iii) identification and sensitivity. Finally, I discuss common issues for the estimation of leading behavioral models. I illustrate this discussion with selected coverage of existing work in the literature.},
	number = {24797},
	urldate = {2021-09-12},
	institution = {National Bureau of Economic Research},
	author = {DellaVigna, Stefano},
	month = jul,
	year = {2018},
	doi = {10.3386/w24797},
	note = {Series: Working Paper Series},
	file = {DellaVigna_2018_Structural Behavioral Economics.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/DellaVigna_2018_Structural Behavioral Economics.pdf:application/pdf},
}

@incollection{reiss2007Chapter64Structural,
	title = {Chapter 64 {Structural} {Econometric} {Modeling}: {Rationales} and {Examples} from {Industrial} {Organization}},
	volume = {6},
	shorttitle = {Chapter 64 {Structural} {Econometric} {Modeling}},
	url = {https://www.sciencedirect.com/science/article/pii/S1573441207060643},
	abstract = {This chapter explains the logic of structural econometric models and compares them to other types of econometric models. We provide a framework researchers can use to develop and evaluate structural econometric models. This framework pays particular attention to describing different sources of unobservables in structural models. We use our framework to evaluate several literatures in industrial organization economics, including the literatures dealing with market power, product differentiation, auctions, regulation and entry.},
	language = {en},
	urldate = {2021-09-12},
	booktitle = {Handbook of {Econometrics}},
	publisher = {Elsevier},
	author = {Reiss, Peter C. and Wolak, Frank A.},
	editor = {Heckman, James J. and Leamer, Edward E.},
	month = jan,
	year = {2007},
	doi = {10.1016/S1573-4412(07)06064-3},
	keywords = {auctions, entry, market power, regulation, structural econometric model},
	pages = {4277--4415},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/LFPEF43M/S1573441207060643.html:text/html;Reiss_Wolak_2007_Chapter 64 Structural Econometric Modeling.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Reiss_Wolak_2007_Chapter 64 Structural Econometric Modeling.pdf:application/pdf},
}

@techreport{orazioattansio2019StructuralModellingPolicymaking,
	title = {Structural {Modelling} in {Policymaking}},
	url = {https://cedilprogramme.org/wp-content/uploads/2018/11/Inception-Paper-9-Orazio-Attanasio-Structural-Modelling-in-Policymaking.pdf},
	urldate = {2021-09-12},
	institution = {The Centre of Excellence for Development Impact and Learning (CEDIL) is an academic consortium supported by UKAID through DFID},
	author = {{Orazio Attansio} and {Debbie Blair}},
	year = {2019},
	file = {Orazio Attansio_Debbie Blair_2019_Structural Modelling in Policymaking.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Orazio Attansio_Debbie Blair_2019_Structural Modelling in Policymaking.pdf:application/pdf},
}

@techreport{browningTwoExamplesStructural,
	title = {Two examples of structural modelling. {Notes} for "{Structural} modelling".},
	language = {en},
	author = {Browning, Martin},
	pages = {9},
	file = {Browning_Two examples of structural modelling.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Browning_Two examples of structural modelling.pdf:application/pdf},
}

@article{attanasio2012EducationChoicesMexico,
	title = {Education {Choices} in {Mexico}: {Using} a {Structural} {Model} and a {Randomized} {Experiment} to {Evaluate} {PROGRESA}},
	volume = {79},
	issn = {0034-6527},
	shorttitle = {Education {Choices} in {Mexico}},
	url = {https://doi.org/10.1093/restud/rdr015},
	doi = {10.1093/restud/rdr015},
	abstract = {In this paper, we use an economic model to analyse data from a major randomized social experiment, namely PROGRESA in Mexico, and to evaluate its impact on school participation. We show the usefulness of using experimental data to estimate a structural economic model as well as the importance of a structural model in interpreting experimental results. The availability of the experiment also allows us to estimate the program's general equilibrium effects, which we then incorporate into our simulations. Our main findings are (i) the program's grant has a much stronger impact on school enrolment than an equivalent reduction in child wages; (ii) the program has a positive effect on the enrollment of children, especially after primary school; this result is well replicated by the parsimonious structural model; (iii) there are sizeable effects of the program on child wages, which, however, reduce the effectiveness of the program only marginally; and (iv) a revenue neutral change in the program that would increase the grant for secondary school children while eliminating for the primary school children would have a substantially larger effect on enrollment of the latter, while having minor effects on the former.},
	number = {1},
	urldate = {2021-09-12},
	journal = {The Review of Economic Studies},
	author = {Attanasio, Orazio P. and Meghir, Costas and Santiago, Ana},
	month = jan,
	year = {2012},
	pages = {37--66},
	file = {Snapshot:/Users/yulei/Zotero/storage/MD8GDC4G/1562110.html:text/html;Attanasio et al_2012_Education Choices in Mexico.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Attanasio et al_2012_Education Choices in Mexico.pdf:application/pdf},
}

@article{low2017UseStructuralModels,
	title = {The {Use} of {Structural} {Models} in {Econometrics}},
	volume = {31},
	issn = {0895-3309},
	url = {https://www.aeaweb.org/articles?id=10.1257/jep.31.2.33},
	doi = {10.1257/jep.31.2.33},
	abstract = {This paper discusses the role of structural economic models in empirical analysis and policy design. The central payoff of a structural econometric model is that it allows an empirical researcher to go beyond the conclusions of a more conventional empirical study that provides reduced-form causal relationships. Structural models identify mechanisms that determine outcomes and are designed to analyze counterfactual policies, quantifying impacts on specific outcomes as well as effects in the short and longer run. We start by defining structural models, distinguishing between those that are fully specified and those that are partially specified. We contrast the treatment effects approach with structural models, and present an example of how a structural model is specified and the particular choices that were made. We cover combining structural estimation with randomized experiments. We then turn to numerical techniques for solving dynamic stochastic models that are often used in structural estimation, again with an example. The penultimate section focuses on issues of estimation using the method of moments.},
	language = {en},
	number = {2},
	urldate = {2021-09-12},
	journal = {Journal of Economic Perspectives},
	author = {Low, Hamish and Meghir, Costas},
	month = may,
	year = {2017},
	keywords = {Single Equation Models, Single Variables: General, Model Construction and Estimation},
	pages = {33--58},
	file = {Snapshot:/Users/yulei/Zotero/storage/55VR24GW/articles.html:text/html;Low_Meghir_2017_The Use of Structural Models in Econometrics.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Low_Meghir_2017_The Use of Structural Models in Econometrics.pdf:application/pdf},
}

@book{train2009DiscreteChoiceMethods,
	address = {Cambridge},
	edition = {2},
	title = {Discrete {Choice} {Methods} with {Simulation}},
	isbn = {978-0-521-76655-5},
	url = {https://www.cambridge.org/core/books/discrete-choice-methods-with-simulation/49CABD00F3DDDA088A8FBFAAAD7E9546},
	abstract = {This book describes the new generation of discrete choice methods, focusing on the many advances that are made possible by simulation. Researchers use these statistical methods to examine the choices that consumers, households, firms, and other agents make. Each of the major models is covered: logit, generalized extreme value, or GEV (including nested and cross-nested logits), probit, and mixed logit, plus a variety of specifications that build on these basics. Recent advances in Bayesian procedures are explored, including the use of the Metropolis-Hastings algorithm and its variant Gibbs sampling. This second edition adds chapters on endogeneity and expectation-maximization (EM) algorithms. No other book incorporates all these fields, which have arisen in the past 25 years. The procedures are applicable in many fields, including energy, transportation, environmental studies, health, labor, and marketing.},
	urldate = {2021-09-12},
	publisher = {Cambridge University Press},
	author = {Train, Kenneth E.},
	year = {2009},
	doi = {10.1017/CBO9780511805271},
	file = {Snapshot:/Users/yulei/Zotero/storage/FC35MEJC/49CABD00F3DDDA088A8FBFAAAD7E9546.html:text/html;Train_2009_Discrete Choice Methods with Simulation.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Train_2009_Discrete Choice Methods with Simulation.pdf:application/pdf},
}

@article{keane2009EmpiricalApplicationsDiscrete,
	title = {Empirical applications of discrete choice dynamic programming models},
	volume = {12},
	issn = {1094-2025},
	url = {https://www.sciencedirect.com/science/article/pii/S1094202508000318},
	doi = {10.1016/j.red.2008.07.001},
	abstract = {The development over the past 25 years of methods for the estimation of discrete choice dynamic programming (DCDP) models opened up new frontiers for empirical research in a host of areas, including labor economics, industrial organization, economic demography, health economics, development economics, political economy and marketing. In this paper, we first describe the development of the DCDP framework, showing how it was a natural extension of static discrete choice modeling. We then summarize six papers that adopt the DCDP paradigm that address substantively important social and economic questions. Finally, we consider the issue of the credibility of empirical findings based on the structural estimation of DCDP models.},
	language = {en},
	number = {1},
	urldate = {2021-09-12},
	journal = {Review of Economic Dynamics},
	author = {Keane, Michael P. and Wolpin, Kenneth I.},
	month = jan,
	year = {2009},
	keywords = {Structural estimation, Discrete choice dynamic programming models, Policy evaluation},
	pages = {1--22},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/C4WKUWUE/S1094202508000318.html:text/html;Keane_Wolpin_2009_Empirical applications of discrete choice dynamic programming models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Keane_Wolpin_2009_Empirical applications of discrete choice dynamic programming models.pdf:application/pdf},
}

@article{todd2006AssessingImpactSchool,
	title = {Assessing the {Impact} of a {School} {Subsidy} {Program} in {Mexico}: {Using} a {Social} {Experiment} to {Validate} a {Dynamic} {Behavioral} {Model} of {Child} {Schooling} and {Fertility}},
	volume = {96},
	issn = {0002-8282},
	shorttitle = {Assessing the {Impact} of a {School} {Subsidy} {Program} in {Mexico}},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.96.5.1384},
	doi = {10.1257/aer.96.5.1384},
	abstract = {This paper uses data from a randomized social experiment in Mexico to estimate
and validate a dynamic behavioral model of parental decisions about fertility and
child schooling, to evaluate the effects of the PROGRESA school subsidy program,
and to perform a variety of counterfactual experiments of policy alternatives. Our
method of validation estimates the model without using post-program data and then
compares the models predictions about program impacts to the experimental
impact estimates. The results show that the models predicted program impacts
track the experimental results. Our analysis of counterfactual policies reveals an
alternative subsidy schedule that would induce a greater impact on average school
attainment at similar cost to the existing program.},
	language = {en},
	number = {5},
	urldate = {2021-09-12},
	journal = {American Economic Review},
	author = {Todd, Petra E. and Wolpin, Kenneth I.},
	month = dec,
	year = {2006},
	keywords = {Analysis of Education, Education: Government Policy, Fertility, Child Care, Children, Family Planning, Human Development, Income Distribution, Migration, Youth, Economic Development: Human Resources},
	pages = {1384--1417},
	file = {Snapshot:/Users/yulei/Zotero/storage/NVWXYVX6/articles.html:text/html;Todd_Wolpin_2006_Assessing the Impact of a School Subsidy Program in Mexico.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Todd_Wolpin_2006_Assessing the Impact of a School Subsidy Program in Mexico.pdf:application/pdf},
}

@article{blundell2017WhatHaveWe,
	title = {What {Have} {We} {Learned} from {Structural} {Models}?},
	volume = {107},
	issn = {0002-8282},
	url = {https://pubs.aeaweb.org/doi/10.1257/aer.p20171116},
	doi = {10.1257/aer.p20171116},
	abstract = {A structural economic model is one where the structure of decision making is incorporated in the model specification. Structural models aim to identify three distinct, but related, objects: (i) structural “deep” parameters; (ii) underlying mechanisms; (iii) policy counterfactuals. The ability to provide counterfactual predictions sets structural models apart from reduced-form models. The focus is on studies that allow a better understanding of the mechanisms underlying observed behavior and that provide reliable insights about policy counterfactuals. Emphasis is given to models that minimize assumptions on the structural function and on unobserved heterogeneity and approaches that align structural and “reduced form” moments.},
	language = {en},
	number = {5},
	urldate = {2021-09-13},
	journal = {American Economic Review},
	author = {Blundell, Richard},
	month = may,
	year = {2017},
	pages = {287--292},
	file = {Blundell_2017_What Have We Learned from Structural Models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Blundell_2017_What Have We Learned from Structural Models.pdf:application/pdf},
}

@misc{2021DynamicDiscreteChoice,
	title = {Dynamic discrete choice},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Dynamic_discrete_choice&oldid=1016923675},
	abstract = {Dynamic discrete choice (DDC) models, also known as discrete choice models of dynamic programming, model an agent's choices over discrete options that have future implications. Rather than assuming observed choices are the result of static utility maximization, observed choices in DDC models are assumed to result from an agent's maximization of the present value of utility, generalizing the utility theory upon which discrete choice models are based.The goal of DDC methods is to estimate the structural parameters of the agent's decision process. Once these parameters are known, the researcher can then use the estimates to simulate how the agent would behave in a counterfactual state of the world. (For example, how a prospective college student's enrollment decision would change in response to a tuition increase.)},
	language = {en},
	urldate = {2021-09-13},
	journal = {Wikipedia},
	month = apr,
	year = {2021},
	note = {Page Version ID: 1016923675},
	file = {Snapshot:/Users/yulei/Zotero/storage/49JBJGD9/index.html:text/html},
}

@article{aguirregabiria2010DynamicDiscreteChoice,
	series = {Structural {Models} of {Optimization} {Behavior} in {Labor}, {Aging}, and {Health}},
	title = {Dynamic discrete choice structural models: {A} survey},
	volume = {156},
	issn = {0304-4076},
	shorttitle = {Dynamic discrete choice structural models},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407609001985},
	doi = {10.1016/j.jeconom.2009.09.007},
	abstract = {This paper reviews methods for the estimation of dynamic discrete choice structural models and discusses related econometric issues. We consider single-agent models, competitive equilibrium models and dynamic games. The methods are illustrated with descriptions of empirical studies which have applied these techniques to problems in different areas of economics. Programming codes for some of the estimation methods are available in a companion web page.},
	language = {en},
	number = {1},
	urldate = {2021-09-13},
	journal = {Journal of Econometrics},
	author = {Aguirregabiria, Victor and Mira, Pedro},
	month = may,
	year = {2010},
	keywords = {Discrete choice, Dynamic structural models, Estimation methods},
	pages = {38--67},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/AQYJIU82/S0304407609001985.html:text/html;Aguirregabiria_Mira_2010_Dynamic discrete choice structural models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Aguirregabiria_Mira_2010_Dynamic discrete choice structural models.pdf:application/pdf},
}

@book{saltelli2004SensitivityAnalysisPractice,
	address = {Hoboken, NJ},
	title = {Sensitivity analysis in practice: a guide to assessing scientific models},
	isbn = {978-0-470-87093-8},
	shorttitle = {Sensitivity analysis in practice},
	language = {en},
	publisher = {Wiley},
	editor = {Saltelli, A.},
	year = {2004},
	keywords = {Sensitivity theory (Mathematics), SIMLAB, Simulation methods},
	file = {Saltelli_2004_Sensitivity analysis in practice.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Saltelli_2004_Sensitivity analysis in practice.pdf:application/pdf},
}

@techreport{andreylaunovStructuralEstimation,
	type = {Seminar announcement},
	title = {Structural {Estimation}},
	url = {https://www.blogs.uni-mainz.de/fb03-economics-macro/files/2018/11/se.pdf},
	urldate = {2021-09-14},
	author = {{Andrey Launov} and {Klaus Wälde}},
	file = {Andrey Launov_Klaus Wälde_Structural Estimation.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Andrey Launov_Klaus Wälde_Structural Estimation.pdf:application/pdf},
}

@misc{PhD524MinicourseStructural,
	title = {{PhD524} - {Mini}-course on {Structural} {Microeconometrics}},
	url = {https://pcw.hhs.se/courses/PhD524},
	abstract = {Description of course/module PhD524 at the Stockholm School of Economics},
	urldate = {2021-09-14},
	file = {Snapshot:/Users/yulei/Zotero/storage/PM4HHBBK/PhD524.html:text/html},
}

@techreport{LectureDynamicDiscrete,
	title = {Lecture: {Dynamic} {Discrete} {Choice}},
	url = {http://www.econ2.jhu.edu/people/hu/teaching/Lecture-Dynamic-Discrete-Choice.pdf},
	urldate = {2021-09-14},
	file = {Lecture.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Lecture.pdf:application/pdf},
}

@techreport{juannaschroterjoensen2018RESEARCHSTATEMENT,
	title = {{RESEARCH} {STATEMENT}},
	author = {{Juanna Schrøter Joensen}},
	year = {2018},
	file = {Juanna Schrøter Joensen - 2018 - RESEARCH STATEMENT.pdf:/Users/yulei/Zotero/storage/78F8EINE/Juanna Schrøter Joensen - 2018 - RESEARCH STATEMENT.pdf:application/pdf},
}

@article{ching2012PractitionerGuideBayesian,
	title = {A practitioner’s guide to {Bayesian} estimation of discrete choice dynamic programming models},
	volume = {10},
	issn = {1573-711X},
	url = {https://doi.org/10.1007/s11129-012-9119-6},
	doi = {10.1007/s11129-012-9119-6},
	abstract = {This paper provides a step-by-step guide to estimating infinite horizon discrete choice dynamic programming (DDP) models using a new Bayesian estimation algorithm (Imai et al., Econometrica 77:1865–1899, 2009a) (IJC). In the conventional nested fixed point algorithm, most of the information obtained in the past iterations remains unused in the current iteration. In contrast, the IJC algorithm extensively uses the computational results obtained from the past iterations to help solve the DDP model at the current iterated parameter values. Consequently, it has the potential to significantly alleviate the computational burden of estimating DDP models. To illustrate this new estimation method, we use a simple dynamic store choice model where stores offer “frequent-buyer” type rewards programs. Our Monte Carlo results demonstrate that the IJC method is able to recover the true parameter values of this model quite precisely. We also show that the IJC method could reduce the estimation time significantly when estimating DDP models with unobserved heterogeneity, especially when the discount factor is close to 1.},
	language = {en},
	number = {2},
	urldate = {2021-09-15},
	journal = {Quantitative Marketing and Economics},
	author = {Ching, Andrew T. and Imai, Susumu and Ishihara, Masakazu and Jain, Neelam},
	month = jun,
	year = {2012},
	pages = {151--196},
	file = {Springer Full Text PDF:/Users/yulei/Zotero/storage/BA6677Y2/Ching 等。 - 2012 - A practitioner’s guide to Bayesian estimation of d.pdf:application/pdf},
}

@article{kevinsummers1993MethodQuantifyingPrediction,
	title = {A method for quantifying the prediction uncertainties associated with water quality models},
	volume = {65},
	issn = {0304-3800},
	url = {https://www.sciencedirect.com/science/article/pii/0304380093900787},
	doi = {10.1016/0304-3800(93)90078-7},
	abstract = {Many environmental regulatory agencies depend, to a large extent, upon the use of models to organize, understand, and utilize the information available for regulatory decision making. In light of the extensive use of environmental models, we developed a general analytical protocol to quantify the prediction error associated with commonly used surface water quality models. The methodology is designed in order to compare water quality models configured to represent different levels of spatial, temporal, and mechanistic complexity. This comparison can be accomplished by fitting the models to a benchmark data set. Once the models are successfully fitted to the benchmark data, the prediction errors associated with each application can be quantified using the Monte Carlo simulation techniques. The application of the protocol using these simulation techniques is described in a companion paper in which comparisons among model uncertainty results are made using the Wilcoxon ranked sum test to determine significant differences.},
	language = {en},
	number = {3},
	urldate = {2021-09-19},
	journal = {Ecological Modelling},
	author = {Kevin Summers, J. and Wilson, Harold T. and Kou, Jingyee},
	month = feb,
	year = {1993},
	pages = {161--176},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/Z8JG6JQS/0304380093900787.html:text/html;Kevin Summers et al_1993_A method for quantifying the prediction uncertainties associated with water.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Kevin Summers et al_1993_A method for quantifying the prediction uncertainties associated with water.pdf:application/pdf},
}

@article{rust2014LimitsInferenceTheory,
	title = {The {Limits} of {Inference} with {Theory}: {A} {Review} of {Wolpin} (2013)},
	volume = {52},
	shorttitle = {The {Limits} of {Inference} with {Theory}},
	doi = {10.1257/jel.52.3.820},
	abstract = {This essay reviews Kenneth I. Wolpin's (2013) monograph The Limits of Inference without Theory, which arose from lectures he presented at the Cowles Foundation in 2010 in honor of Tjalling Koopmans. While I readily agree with Wolpin's basic premise that empirical work that eschews the role of economic theory faces unnecessary self-imposed limits relative to empirical work that embraces and tries to test and improve economic theory, it is important to be aware that the use of economic theory is not a panacea. I point out that there are also serious limits to inference with theory: 1) there may be no truly “structural” (policy invariant) parameters, a key assumption underpinning the structural econometric approach that Wolpin and the Cowles Foundation have championed; 2) there is a curse of dimensionality that makes it very difficult for us to elucidate the detailed implications of economic theo- ries, which is necessary to empirically implement and test these theories; 3) there is an identification problem that makes it impossible to decide between competing theories without imposing ad hoc auxiliary assumptions (such as parametric functional form assumptions); and 4) there is a problem of multiplicity and indeterminacy of equilib- ria that limits the predictive empirical content of many economic theories. I conclude that though these are very challenging problems, I agree with Wolpin and the Cowles Foundation that economists have far more to gain by trying to incorporate economic theory into empirical work and test and improve our theories than by rejecting theory and presuming that all interesting economic issues can be answered by well-designed controlled, randomized experiments and assuming that difficult questions of causality and evaluation of alternative hypothetical policies can be resolved by simply allowing the “data to speak for itself.”},
	journal = {Journal of Economic Literature},
	author = {Rust, John},
	month = sep,
	year = {2014},
	file = {Rust_2014_The Limits of Inference with Theory.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Rust_2014_The Limits of Inference with Theory.pdf:application/pdf},
}

@article{berry1995AutomobilePricesMarket,
	title = {Automobile {Prices} in {Market} {Equilibrium}},
	volume = {63},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/2171802},
	doi = {10.2307/2171802},
	abstract = {This paper develops techniques for empirically analyzing demand and supply in differentiated products markets and then applies these techniques to analyze equilibrium in the U.S. automobile industry. Our primary goal is to present a framework which enables one to obtain estimates of demand and cost parameters for a class of oligopolistic differentiated products markets. These estimates can be obtained using only widely available product-level and aggregate consumer-level data, and they are consistent with a structural model of equilibrium in an oligopolistic industry. When we apply the techniques developed here to the U.S. automobile market, we obtain cost and demand parameters for (essentially) all models marketed over a twenty year period.},
	number = {4},
	urldate = {2021-09-19},
	journal = {Econometrica},
	author = {Berry, Steven and Levinsohn, James and Pakes, Ariel},
	year = {1995},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {841--890},
	file = {Berry et al_1995_Automobile Prices in Market Equilibrium.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Berry et al_1995_Automobile Prices in Market Equilibrium.pdf:application/pdf},
}

@article{bayer2016DynamicModelDemand,
	title = {A {Dynamic} {Model} of {Demand} for {Houses} and {Neighborhoods}},
	volume = {84},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA10170},
	doi = {10.3982/ECTA10170},
	abstract = {This paper develops a dynamic model of neighborhood choice along with a computationally light multi-step estimator. The proposed empirical framework captures observed and unobserved preference heterogeneity across households and locations in a flexible way. We estimate the model using a newly assembled data set that matches demographic information from mortgage applications to the universe of housing transactions in the San Francisco Bay Area from 1994 to 2004. The results provide the first estimates of the marginal willingness to pay for several non-marketed amenities—neighborhood air pollution, violent crime, and racial composition—in a dynamic framework. Comparing these estimates with those from a static version of the model highlights several important biases that arise when dynamic considerations are ignored.},
	language = {en},
	number = {3},
	urldate = {2021-09-19},
	journal = {Econometrica},
	author = {Bayer, Patrick and McMillan, Robert and Murphy, Alvin and Timmins, Christopher},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA10170},
	keywords = {amenities, dynamic discrete choice, hedonic valuation, housing demand, Neighborhood choice, residential sorting, unobserved heterogeneity},
	pages = {893--942},
	file = {Snapshot:/Users/yulei/Zotero/storage/2PNKHTIJ/ECTA10170.html:text/html;Bayer et al_2016_A Dynamic Model of Demand for Houses and Neighborhoods.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Bayer et al_2016_A Dynamic Model of Demand for Houses and Neighborhoods.pdf:application/pdf},
}

@article{rudik2020OptimalClimatePolicy,
	title = {Optimal {Climate} {Policy} {When} {Damages} {Are} {Unknown}},
	volume = {12},
	issn = {1945-7731},
	url = {https://www.aeaweb.org/articles?id=10.1257/pol.20160541},
	doi = {10.1257/pol.20160541},
	abstract = {Integrated assessment models (IAMs) are economists' primary tool for analyzing the optimal carbon tax. Damage functions, which link temperature to economic impacts, have come under fire because of their assumptions that may be incorrect in significant but a priori unknowable ways. Here I develop recursive IAM frameworks to model uncertainty, learning, and concern for misspecification about damages. I decompose the carbon tax into channels capturing state uncertainty, insurance motives, and precautionary saving. Damage learning improves ex ante welfare by 750 billion USD. If damage functions are misspecified and omit the potential for catastrophic damages, robust control may be beneficial ex post.},
	language = {en},
	number = {2},
	urldate = {2021-09-19},
	journal = {American Economic Journal: Economic Policy},
	author = {Rudik, Ivan},
	month = may,
	year = {2020},
	keywords = {Environmental Taxes and Subsidies, Climate, Global Warming, Environmental Economics: Government Policy, Natural Disasters and Their Management, Redistributive Effects, Taxation and Subsidies: Externalities},
	pages = {340--373},
	file = {Snapshot:/Users/yulei/Zotero/storage/5LPXAB4E/articles.html:text/html;Rudik_2020_Optimal Climate Policy When Damages Are Unknown.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Rudik_2020_Optimal Climate Policy When Damages Are Unknown.pdf:application/pdf},
}

@article{eckstein1989DynamicLabourForce,
	title = {Dynamic {Labour} {Force} {Participation} of {Married} {Women} and {Endogenous} {Work} {Experience}},
	volume = {56},
	issn = {0034-6527},
	url = {https://www.jstor.org/stable/2297553},
	doi = {10.2307/2297553},
	abstract = {This paper presents and estimates a dynamic model of married women's labour force participation and fertility in which the effect of work experience on wages is explicitly taken into account. Because current participation alters future potential earnings, the investment return to work will be an important factor in the current work decision in any forward-looking behavioural model. The model is estimated using the National Longitudinal Surveys mature women's cohort. We use the estimates of our model to predict changes in the lifecycle patterns of employment due to changes in schooling, fertility, husband's income, and the magnitude of the experience effect on wages. We find that although work experience increases the disutility of further work, the effect is overwhelmed by the positive effect of experience on wages, leading to persistence in the employment patterns of these women. In addition we find that an increase in young children and in husband's income substantially reduces participation while increased schooling has a powerful positive impact on participation. \_pg 375-390},
	number = {3},
	urldate = {2021-09-20},
	journal = {The Review of Economic Studies},
	author = {Eckstein, Zvi and Wolpin, Kenneth I.},
	year = {1989},
	note = {Publisher: [Oxford University Press, Review of Economic Studies, Ltd.]},
	pages = {375--390},
	file = {Eckstein_Wolpin_1989_Dynamic Labour Force Participation of Married Women and Endogenous Work.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Eckstein_Wolpin_1989_Dynamic Labour Force Participation of Married Women and Endogenous Work.pdf:application/pdf},
}

@article{leamer1985SensitivityAnalysesWoulda,
	title = {Sensitivity {Analyses} {Would} {Help}},
	volume = {75},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/1814801},
	number = {3},
	urldate = {2021-09-20},
	journal = {The American Economic Review},
	author = {Leamer, Edward E.},
	year = {1985},
	note = {Publisher: American Economic Association},
	pages = {308--313},
}

@article{backus1992InternationalRealBusiness,
	title = {International {Real} {Business} {Cycles}},
	volume = {100},
	url = {https://ideas.repec.org/a/ucp/jpolec/v100y1992i4p745-75.html},
	abstract = {The authors ask whether a two-country business cycle model can account simultaneously for domestic and international aspects of business cycles. With this question in mind, the authors document a number of discrepancies between theory and data. The most striking discrepancy concerns the correlations of consumption and output across countries. In this data, outputs are generally more highly correlated across countries than consumptions. In the model they see the opposite. Copyright 1992 by University of Chicago Press.},
	language = {en},
	number = {4},
	urldate = {2021-09-20},
	journal = {Journal of Political Economy},
	author = {Backus, David K. and Kehoe, Patrick J. and Kydland, Finn E.},
	year = {1992},
	note = {Publisher: University of Chicago Press},
	pages = {745--775},
	file = {Snapshot:/Users/yulei/Zotero/storage/DIF5LHFA/v100y1992i4p745-75.html:text/html},
}

@article{canova1994StatisticalInferenceCalibrated,
	title = {Statistical inference in calibrated models},
	volume = {9},
	issn = {1099-1255},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.3950090508},
	doi = {10.1002/jae.3950090508},
	abstract = {This paper describes a Monte Carlo procedure to assess the performance of calibrated dynamic general equilibrium models. The procedure formalizes the choice of parameters and the evaluation of the model and provides an efficient way to conduct a sensitivity analysis for perturbations of the parameters within a reasonable range. As an illustration the methodology is applied to two problems: the equity premium puzzle and how much of the variance of actual US output is explained by a real business cycle model.},
	language = {en},
	number = {S1},
	urldate = {2021-09-20},
	journal = {Journal of Applied Econometrics},
	author = {Canova, Fabio},
	year = {1994},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.3950090508},
	pages = {S123--S144},
	file = {Snapshot:/Users/yulei/Zotero/storage/HPNWSTCS/jae.html:text/html;Canova_1994_Statistical inference in calibrated models.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Canova_1994_Statistical inference in calibrated models.pdf:application/pdf},
}

@article{ransomModelFitValidation,
	title = {Model {Fit}, {Validation} and {Counterfactual} {Simulations}},
	language = {en},
	author = {Ransom, Tyler},
	pages = {25},
	file = {Ransom - Model Fit, Validation and Counterfactual Simulatio.pdf:/Users/yulei/Zotero/storage/8BVMD5QU/Ransom - Model Fit, Validation and Counterfactual Simulatio.pdf:application/pdf},
}

@techreport{arcidiacono2016CollegeAttritionDynamics,
	type = {Working {Paper}},
	title = {College {Attrition} and the {Dynamics} of {Information} {Revelation}},
	url = {https://www.nber.org/papers/w22325},
	abstract = {This paper investigates the role played by informational frictions in college and the workplace. We estimate a dynamic structural model of schooling and work decisions, where individuals have imperfect information about their schooling ability and labor market productivity. We take into account the heterogeneity in schooling investments by distinguishing between two- and four-year colleges, graduate school, as well as science and non-science majors for four-year colleges. Individuals may also choose whether to work full-time, part-time, or not at all. A key feature of our approach is to account for correlated learning through college grades and wages, whereby individuals may leave or re-enter college as a result of the arrival of new information on their ability and productivity. Our findings indicate that the elimination of informational frictions would increase the college graduation rate by 9 percentage points, and would increase the college wage premium by 32.7 percentage points through increased sorting on ability.},
	number = {22325},
	urldate = {2021-09-21},
	institution = {National Bureau of Economic Research},
	author = {Arcidiacono, Peter and Aucejo, Esteban and Maurel, Arnaud and Ransom, Tyler},
	month = jun,
	year = {2016},
	doi = {10.3386/w22325},
	note = {Series: Working Paper Series},
	file = {Arcidiacono et al_2016_College Attrition and the Dynamics of Information Revelation.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Arcidiacono et al_2016_College Attrition and the Dynamics of Information Revelation.pdf:application/pdf},
}

@article{martin2007AssessingPreferenceChange,
	title = {Assessing {Preference} {Change} on the {US} {Supreme} {Court}},
	volume = {23},
	issn = {8756-6222},
	url = {https://doi.org/10.1093/jleo/ewm028},
	doi = {10.1093/jleo/ewm028},
	abstract = {The foundation upon which accounts of policy-motivated behavior of Supreme Court justices are built consists of assumptions about the policy preferences of the justices. To date, most scholars have assumed that the policy positions of Supreme Court justices remain consistent throughout the course of their careers and most measures of judicial ideology—such as Segal and Cover scores—are time invariant. On its face, this assumption is reasonable; Supreme Court justices serve with life tenure and are typically appointed after serving in other political or judicial roles. However, it is also possible that the worldviews, and thus the policy positions, of justices evolve through the course of their careers. In this article we use a Bayesian dynamic ideal point model to investigate preference change on the US Supreme Court. The model allows for justices' ideal points to change over time in a smooth fashion. We focus our attention on the 16 justices who served for 10 or more terms and completed their service between the 1937 and 2003 terms. The results are striking—14 of these 16 justices exhibit significant preference change. This has profound implications for the use of time-invariant preference measures in applied work.},
	number = {2},
	urldate = {2021-09-22},
	journal = {The Journal of Law, Economics, and Organization},
	author = {Martin, Andrew D. and Quinn, Kevin M.},
	month = jun,
	year = {2007},
	pages = {365--385},
	file = {Snapshot:/Users/yulei/Zotero/storage/YRRVY26Z/893206.html:text/html;Martin_Quinn_2007_Assessing Preference Change on the US Supreme Court.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Martin_Quinn_2007_Assessing Preference Change on the US Supreme Court.pdf:application/pdf},
}

@article{benke2008ParameterUncertaintySensitivity,
	title = {Parameter uncertainty, sensitivity analysis and prediction error in a water-balance hydrological model},
	volume = {47},
	issn = {0895-7177},
	url = {https://www.sciencedirect.com/science/article/pii/S0895717707002373},
	doi = {10.1016/j.mcm.2007.05.017},
	abstract = {Analysis of uncertainty is often neglected in the evaluation of complex systems models, such as computational models used in hydrology or ecology. Prediction uncertainty arises from a variety of sources, such as input error, calibration accuracy, parameter sensitivity and parameter uncertainty. In this study, various computational approaches were investigated for analysing the impact of parameter uncertainty on predictions of streamflow for a water-balance hydrological model used in eastern Australia. The parameters and associated equations which had greatest impact on model output were determined by combining differential error analysis and Monte Carlo simulation with stochastic and deterministic sensitivity analysis. This integrated approach aids in the identification of insignificant or redundant parameters and provides support for further simplifications in the mathematical structure underlying the model. Parameter uncertainty was represented by a probability distribution and simulation experiments revealed that the shape (skewness) of the distribution had a significant effect on model output uncertainty. More specifically, increasing negative skewness of the parameter distribution correlated with decreasing width of the model output confidence interval (i.e. resulting in less uncertainty). For skewed distributions, characterisation of uncertainty is more accurate using the confidence interval from the cumulative distribution rather than using variance. The analytic approach also identified the key parameters and the non-linear flux equation most influential in affecting model output uncertainty.},
	language = {en},
	number = {11},
	urldate = {2021-09-22},
	journal = {Mathematical and Computer Modelling},
	author = {Benke, Kurt K. and Lowell, Kim E. and Hamilton, Andrew J.},
	month = jun,
	year = {2008},
	keywords = {2C, 2CSalt, Complex systems, Error propagation, Hydrological model, Monte Carlo simulation, Risk, Sensitivity analysis, Uncertainty},
	pages = {1134--1149},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/3E35LCK3/S0895717707002373.html:text/html;Benke et al_2008_Parameter uncertainty, sensitivity analysis and prediction error in a.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Benke et al_2008_Parameter uncertainty, sensitivity analysis and prediction error in a.pdf:application/pdf},
}

@article{mcdonald2010StructuralModelsArt,
	title = {Structural {Models} and the {Art} of {Approximation}},
	volume = {5},
	issn = {1745-6916},
	url = {https://www.jstor.org/stable/41613584},
	abstract = {Structural equation models have provided a seemingly rigorous method for investigating causal relations in nonexperimental data in the presence of measurement error or multiple measures of putative causes or effects. Methods have been developed for fitting these very complex models globally and obtaining global fit statistics or global measures of their approximation to sample data. Structural equation models are idealizations that can serve only as approximations to real multivariate data. Further, these models are multidimensional, and the approximation is itself multidimensional. Tests of "significance" and global indices of approximation do not provide an adequate basis for judging the acceptability of the approximation. Standard applications of structural models use a composite of two models—a measurement (path) model and a path (causal) model. Separate analyses of the measurement model and the path model provide an informed judgment, whereas the composite global analysis can easily yield unreasonable conclusions. Separating the component models enables a careful assessment of the actual constraints implied by the path model, using recently developed methods. An empirical example shows how the conventional global treatment yields unacceptable conclusions.},
	number = {6},
	urldate = {2021-09-22},
	journal = {Perspectives on Psychological Science},
	author = {McDonald, Roderick P.},
	year = {2010},
	note = {Publisher: [Association for Psychological Science, Sage Publications, Inc.]},
	pages = {675--686},
}

@article{saltelli2005SensitivityAnalysisChemical,
	title = {Sensitivity {Analysis} for {Chemical} {Models}},
	volume = {105},
	issn = {0009-2665, 1520-6890},
	url = {https://pubs.acs.org/doi/10.1021/cr040659d},
	doi = {10.1021/cr040659d},
	language = {en},
	number = {7},
	urldate = {2021-09-22},
	journal = {Chemical Reviews},
	author = {Saltelli, Andrea and Ratto, Marco and Tarantola, Stefano and Campolongo, Francesca},
	month = jul,
	year = {2005},
	pages = {2811--2828},
	file = {Saltelli 等。 - 2005 - Sensitivity Analysis for Chemical Models.pdf:/Users/yulei/Zotero/storage/Y5X7T6HJ/Saltelli 等。 - 2005 - Sensitivity Analysis for Chemical Models.pdf:application/pdf},
}

@article{boulore2012UncertaintySensitivityAnalysis,
	series = {{SI} : {CFD4NRS}-3},
	title = {Uncertainty and sensitivity analysis of the nuclear fuel thermal behavior},
	volume = {253},
	issn = {0029-5493},
	url = {https://www.sciencedirect.com/science/article/pii/S0029549312004487},
	doi = {10.1016/j.nucengdes.2012.08.017},
	abstract = {In the global framework of nuclear fuel behavior simulation, the response of the models describing the physical phenomena occurring during the irradiation in reactor is mainly conditioned by the confidence in the calculated temperature of the fuel. Amongst all parameters influencing the temperature calculation in our fuel rod simulation code (METEOR V2), several sources of uncertainty have been identified as being the most sensitive: thermal conductivity of UO2, radial distribution of power in the fuel pellet, local linear heat rate in the fuel rod, geometry of the pellet and thermal transfer in the gap. Expert judgment and inverse methods have been used to model the uncertainty of these parameters using theoretical distributions and correlation matrices. Propagation of these uncertainties in the METEOR V2 code using the URANIE framework and a Monte-Carlo technique has been performed in different experimental irradiations of UO2 fuel. At every time step of the simulated experiments, we get a temperature statistical distribution which results from the initial distributions of the uncertain parameters. We then can estimate confidence intervals of the calculated temperature. In order to quantify the sensitivity of the calculated temperature to each of the uncertain input parameters and data, we have also performed a sensitivity analysis using the Sobol’ indices at first order.},
	language = {en},
	urldate = {2021-09-22},
	journal = {Nuclear Engineering and Design},
	author = {Bouloré, A. and Struzik, C. and Gaudier, F.},
	month = dec,
	year = {2012},
	pages = {200--210},
	file = {ScienceDirect Snapshot:/Users/yulei/Zotero/storage/69HW39AM/S0029549312004487.html:text/html;Bouloré et al_2012_Uncertainty and sensitivity analysis of the nuclear fuel thermal behavior.pdf:/Users/yulei/Dropbox/ZoteroAttachments/master thesis/Bouloré et al_2012_Uncertainty and sensitivity analysis of the nuclear fuel thermal behavior.pdf:application/pdf},
}

@article{sauer2004EducationalFinancingLifetime,
	title = {Educational {Financing} and {Lifetime} {Earnings}},
	volume = {71},
	issn = {0034-6527},
	url = {https://doi.org/10.1111/0034-6527.00319},
	doi = {10.1111/0034-6527.00319},
	abstract = {This paper formulates and estimates a dynamic programming model of optimal educational financing decisions. The main purpose of the paper is to measure the effect of short-term parental cash transfers, received during school, on educational borrowing and in-school work decisions, and on post-graduation lifetime earnings. The estimated parameters of the model imply that parental cash transfers do not significantly influence post-graduation lifetime earnings. Long-term factors such as family background and prior human capital investments are more important. Parental cash transfers do, however, significantly determine the decision to borrow or work during school and the level of lifetime consumption.},
	number = {4},
	urldate = {2021-09-23},
	journal = {The Review of Economic Studies},
	author = {Sauer, Robert M.},
	month = oct,
	year = {2004},
	pages = {1189--1216},
	file = {已提交版本:/Users/yulei/Zotero/storage/JSA4RUK9/Sauer - 2004 - Educational Financing and Lifetime Earnings.pdf:application/pdf;Snapshot:/Users/yulei/Zotero/storage/THRRGTKN/1566110.html:text/html},
}
